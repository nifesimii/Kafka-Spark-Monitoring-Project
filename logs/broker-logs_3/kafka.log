[2025-12-24 09:10:28,425] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 09:10:28,610] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 09:10:28,616] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 09:10:28,617] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,424] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 09:10:32,532] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 09:10:32,546] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,734] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,759] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,803] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 09:10:32,817] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 09:10:32,826] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 09:10:32,835] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,998] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:33,004] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:33,009] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:33,036] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 09:10:33,088] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 09:10:33,094] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,101] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,126] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1673) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:33,128] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 09:10:33,128] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 09:10:33,148] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,149] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-24 09:10:33,160] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:33,171] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,172] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,173] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,174] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,180] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1365125390 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,211] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 09:10:33,211] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 09:10:33,221] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:33,231] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 09:10:33,251] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,357] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,450] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1673) (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:33,464] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,516] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,524] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,577] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,584] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,608] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,637] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,638] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,690] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,706] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,711] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,722] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,725] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,758] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 09:10:33,794] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,824] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 09:10:33,840] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 09:10:33,892] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 09:10:33,899] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,908] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,911] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,926] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:33,975] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,002] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,021] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,022] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,027] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,032] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,039] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,126] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,140] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,140] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,212] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:34,215] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:34,230] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,233] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 09:10:34,235] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,242] INFO [BrokerLifecycleManager id=6] Incarnation vtr74vqdTLSxQ5CpiSk4wA of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,266] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,332] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,349] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,348] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 09:10:34,358] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 09:10:34,359] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 09:10:34,451] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,512] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:34,528] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,540] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,554] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,556] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,565] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,569] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:34,575] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,625] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,658] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,662] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 1 (org.apache.kafka.raft.FollowerState)
[2025-12-24 09:10:34,671] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,682] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,689] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 7 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,690] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,691] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,691] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,692] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=6, epoch=1) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 09:10:34,695] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 09:10:34,699] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 09:10:34,712] INFO Loaded 0 logs in 17ms (kafka.log.LogManager)
[2025-12-24 09:10:34,716] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 09:10:34,720] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 09:10:34,730] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 09:10:34,811] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,812] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 09:10:34,814] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 09:10:34,871] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,951] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 09:10:34,964] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 09:10:34,965] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 09:10:34,967] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:34,971] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:34,973] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 09:10:34,975] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 09:10:34,975] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 09:10:34,985] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 09:10:34,985] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,997] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 09:10:35,003] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:35,015] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 09:10:35,077] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:35,078] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 09:10:35,080] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 09:10:35,080] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 09:10:35,081] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 09:10:35,089] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 09:10:35,091] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 09:10:35,099] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 09:10:35,099] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 09:10:35,100] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 09:10:35,100] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 09:10:35,100] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 09:10:35,100] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,101] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,101] INFO Kafka startTimeMs: 1766567435100 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,102] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 09:10:38,172] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-24 09:10:38,178] INFO [Broker id=6] Creating new partition _schemas-0 with topic id Im1NE7r1TMiyWVwhbgXZ_g. (state.change.logger)
[2025-12-24 09:10:38,195] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:38,197] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 09:10:38,200] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 09:10:38,201] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:38,203] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:38,205] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:38,206] INFO [Broker id=6] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 09:10:38,231] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:38,233] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(Im1NE7r1TMiyWVwhbgXZ_g),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:38,233] INFO [Broker id=6] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 09:10:38,235] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:38,236] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:38,244] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 09:10:38,965] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 09:10:39,164] INFO [Broker id=6] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-24 09:10:39,166] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-44, __consumer_offsets-10, __consumer_offsets-23, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-8, __consumer_offsets-40, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,168] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,177] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,180] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,180] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 09:10:39,181] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,182] INFO [Broker id=6] Leader __consumer_offsets-15 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,198] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,210] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,212] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,214] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 09:10:39,214] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,215] INFO [Broker id=6] Leader __consumer_offsets-45 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,223] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,230] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,240] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,241] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 09:10:39,242] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,242] INFO [Broker id=6] Leader __consumer_offsets-14 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,254] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,269] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,270] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,271] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 09:10:39,271] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,272] INFO [Broker id=6] Leader __consumer_offsets-44 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,277] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,279] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,281] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,281] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 09:10:39,281] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,282] INFO [Broker id=6] Leader __consumer_offsets-10 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,288] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,292] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,294] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,295] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 09:10:39,295] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,296] INFO [Broker id=6] Leader __consumer_offsets-23 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,300] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,304] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,305] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,305] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 09:10:39,305] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,306] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,311] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,314] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,315] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,315] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 09:10:39,316] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,316] INFO [Broker id=6] Leader __consumer_offsets-49 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,321] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,324] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,325] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,325] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 09:10:39,325] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,326] INFO [Broker id=6] Leader __consumer_offsets-30 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,330] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,335] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,337] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,340] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 09:10:39,340] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,341] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,347] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,350] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,351] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,352] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 09:10:39,353] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,354] INFO [Broker id=6] Leader __consumer_offsets-26 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,361] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,370] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,373] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,374] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 09:10:39,375] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,376] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,381] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,389] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,390] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,391] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 09:10:39,391] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,392] INFO [Broker id=6] Leader __consumer_offsets-40 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,396] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,400] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,401] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,401] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 09:10:39,402] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,402] INFO [Broker id=6] Leader __consumer_offsets-37 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,405] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,412] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,414] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,414] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 09:10:39,415] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,415] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,420] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,430] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,431] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,432] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 09:10:39,432] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,433] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,437] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,439] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,440] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,440] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 09:10:39,440] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,441] INFO [Broker id=6] Leader __consumer_offsets-2 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,447] INFO [Broker id=6] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-24 09:10:39,448] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,451] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,452] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,452] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 09:10:39,452] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,453] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,453] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,455] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,456] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,456] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 09:10:39,457] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,457] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,457] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,462] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,465] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,465] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 09:10:39,466] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,467] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,467] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,471] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,473] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,474] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 09:10:39,475] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,475] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,475] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,479] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,480] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,480] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 09:10:39,481] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,481] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,482] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,487] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,488] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,488] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 09:10:39,488] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,489] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,489] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,494] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,495] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,495] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 09:10:39,496] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,496] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,496] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,498] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,500] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,500] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 09:10:39,500] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,500] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,501] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,503] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,504] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,504] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 09:10:39,505] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,505] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,505] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,508] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,509] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,509] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 09:10:39,509] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,510] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,510] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,514] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,515] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,516] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 09:10:39,516] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,517] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,517] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,520] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,521] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,522] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 09:10:39,522] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,522] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,523] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,528] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,529] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,529] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 09:10:39,529] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,530] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,530] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,533] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,535] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,536] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 09:10:39,536] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,537] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,537] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,543] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,543] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,544] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 09:10:39,544] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,544] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,545] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,547] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,549] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,550] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 09:10:39,550] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,550] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,550] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,553] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,555] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,555] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 09:10:39,556] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,556] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,557] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,561] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,562] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,563] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 09:10:39,563] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,564] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,565] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,567] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,570] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,571] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 09:10:39,571] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,571] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,572] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,576] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,577] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,578] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 09:10:39,578] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,578] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,579] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,582] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,583] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,584] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 09:10:39,585] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,585] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,586] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,592] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,593] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,593] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 09:10:39,594] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,594] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,594] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,597] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,598] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,598] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 09:10:39,599] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,599] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,600] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,605] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,607] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,609] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 09:10:39,609] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,610] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,610] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,613] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,614] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,616] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 09:10:39,617] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,617] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,617] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,620] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,621] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,622] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 09:10:39,622] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,622] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,623] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,626] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,626] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,627] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,627] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,627] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,628] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,633] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,634] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,635] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 09:10:39,636] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,637] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,637] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,644] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,647] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,649] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 09:10:39,649] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,650] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,650] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,655] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,656] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,656] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 09:10:39,656] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,657] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,657] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,660] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,661] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,661] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 09:10:39,661] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,662] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,662] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,668] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,669] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,669] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 09:10:39,669] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,669] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,669] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,673] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,674] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,674] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 09:10:39,675] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,675] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,676] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-7, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,676] INFO [Broker id=6] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 09:10:39,677] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-48 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,679] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,680] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,680] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,680] INFO [Broker id=6] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 09:10:39,680] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,681] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,681] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,681] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,681] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,682] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,682] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,682] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,682] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,683] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,683] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,684] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,684] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,684] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,684] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,685] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,685] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,685] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,685] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,685] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,686] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,686] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,686] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,687] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,687] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,687] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,688] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,688] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,689] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,689] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,690] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,693] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,693] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,694] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,694] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,694] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,695] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,695] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,695] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,696] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,696] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,696] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,696] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,697] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,697] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,697] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,698] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,698] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,698] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,698] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,699] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,699] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,699] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,699] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,699] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,699] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,700] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,700] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,700] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,701] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,701] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,699] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-15 in 4 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,702] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,702] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,702] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,703] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,703] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,704] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-45 in 10 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,705] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,705] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-14 in 10 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,705] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,705] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,706] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,706] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,707] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,707] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,707] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-44 in 11 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,708] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,708] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,708] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-10 in 12 milliseconds for epoch 0, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,708] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,708] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,709] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,709] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,709] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,709] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,709] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,711] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,708] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-23 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,711] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,712] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,712] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,713] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,712] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,713] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,714] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,714] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-49 in 16 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,715] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-30 in 17 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,715] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,715] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,715] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,715] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,716] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-26 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,716] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,717] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,717] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,718] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,718] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,718] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,718] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,719] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 19 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,719] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,720] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,719] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-40 in 19 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,721] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,721] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-37 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,722] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,722] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,722] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,723] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,723] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,724] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,723] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-2 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,724] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,725] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,726] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,726] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,726] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,727] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,727] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,728] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,728] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,728] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,728] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,731] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,729] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,732] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,731] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,732] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,732] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,732] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,732] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,733] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,732] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,734] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,734] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,734] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,734] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,734] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,734] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,735] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,734] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,735] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,735] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,735] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,736] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,736] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,736] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,737] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,736] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,737] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,738] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,738] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 09:10:39,738] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,738] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,739] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,739] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,740] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,740] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,741] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,741] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,741] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,741] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,742] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,742] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,743] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,743] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,743] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,743] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,744] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,744] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,744] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,932] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,933] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,933] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,933] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,933] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,933] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,934] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,934] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,934] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,934] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,935] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,935] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,935] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,935] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,935] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,935] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,936] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,936] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,937] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,937] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,938] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,938] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,938] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,938] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,939] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,939] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,939] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,939] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,939] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,939] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,940] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,940] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,940] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,940] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:20:34,330] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:20:39,154] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:07:11,941] INFO [RaftManager id=6] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6779, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:07:11,968] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6779, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:07:13,392] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:07:13,394] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:15:24,171] INFO [RaftManager id=6] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6898, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:15:24,183] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6898, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:15:25,956] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:15:25,956] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:18:15,407] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000007240-0000000003 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 10:18:15,542] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000007240-0000000003 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 10:26:38,087] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 10:26:38,682] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 10:26:38,719] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 10:26:38,729] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:43,753] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 10:26:43,948] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 10:26:44,011] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:44,692] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:44,733] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:44,755] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 10:26:44,760] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 10:26:44,762] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 10:26:44,768] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:44,989] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:45,010] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:45,018] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:45,118] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 10:26:45,197] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 10:26:45,216] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:45,224] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:45,354] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1100) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:45,373] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 10:26:45,379] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 10:26:45,392] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,410] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-24 10:26:45,419] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:45,445] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,448] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,447] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,468] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,494] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,623] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 10:26:45,624] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 10:26:45,653] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:45,682] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,747] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 10:26:45,785] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,835] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@194681932 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:45,852] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,864] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,898] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,906] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,911] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,995] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,999] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,002] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,024] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,039] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,043] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,046] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,131] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,142] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,145] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,179] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,188] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,214] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,256] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,243] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,269] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,281] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,363] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,375] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 10:26:46,427] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,428] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,463] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,546] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,436] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 10:26:46,587] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 10:26:46,546] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,612] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,647] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,691] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 10:26:46,727] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:46,734] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:46,755] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,771] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:46,773] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:46,809] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:47,414] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:47,418] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,520] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,566] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,617] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,639] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:47,642] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,666] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,725] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,936] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,961] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:48,015] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:48,047] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,149] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,201] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,214] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,249] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,258] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,255] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,283] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,292] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,513] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,616] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,630] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 10:26:48,717] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,723] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:48,771] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,792] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,808] INFO [BrokerLifecycleManager id=6] Incarnation Ola3DycHRHWHMHIujq4rLQ of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:48,827] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,839] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:48,942] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 10:26:48,985] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,008] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 10:26:49,031] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 10:26:49,011] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,163] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,278] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,397] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,470] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1100) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:49,495] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:49,508] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:49,596] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,698] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,799] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,926] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,031] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,054] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:50,057] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:50,139] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,240] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,342] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,453] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,572] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,586] INFO [RaftManager id=6] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:50,682] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,731] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:50,745] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,752] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,784] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,811] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,826] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,885] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,906] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:50,928] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,398] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,401] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,513] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,620] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,787] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,808] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:51,814] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,868] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,898] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,919] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:51,924] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,975] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,999] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,200] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,328] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,442] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,548] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,625] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-24 10:26:52,658] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,662] INFO [MetadataLoader id=6] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,681] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,684] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,707] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,713] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=2) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 10:26:52,753] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 10:26:52,839] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 10:26:52,905] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 5 (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:52,940] INFO Loaded 0 logs in 155ms (kafka.log.LogManager)
[2025-12-24 10:26:52,950] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 10:26:52,958] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 10:26:53,053] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 10:26:53,714] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 10:26:53,730] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 10:26:53,732] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 10:26:53,740] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:26:53,770] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:26:53,777] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 10:26:53,783] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 10:26:53,791] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 10:26:53,849] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:53,912] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:53,916] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 10:26:53,916] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 10:26:53,916] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 10:26:53,918] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 10:26:53,948] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:53,957] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:53,982] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 10:26:54,075] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:54,080] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 10:26:54,095] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 10:26:54,099] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 10:26:54,101] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 10:26:54,110] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 10:26:54,115] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 10:26:54,124] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 10:26:54,126] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 10:26:54,126] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 10:26:54,126] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 10:26:54,127] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 10:26:54,127] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,128] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,128] INFO Kafka startTimeMs: 1766572014127 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,133] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 10:26:59,281] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-12-24 10:26:59,287] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:26:59,289] INFO [Broker id=6] Creating new partition _schemas-0 with topic id 4gVnUI_IQ3yRWz4RgXc--Q. (state.change.logger)
[2025-12-24 10:26:59,316] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:59,322] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 10:26:59,332] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 10:26:59,334] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:26:59,341] INFO [Broker id=6] Leader _schemas-0 with topic id Some(4gVnUI_IQ3yRWz4RgXc--Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:26:59,362] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 10:27:00,427] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 10:27:00,785] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-12-24 10:27:00,787] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-16, __consumer_offsets-13, __consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-25, __consumer_offsets-6, __consumer_offsets-38, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:00,787] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,798] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,806] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,812] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 10:27:00,812] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,813] INFO [Broker id=6] Leader __consumer_offsets-16 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,823] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,827] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,830] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,830] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 10:27:00,831] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,831] INFO [Broker id=6] Leader __consumer_offsets-13 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,836] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,844] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,845] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,846] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 10:27:00,846] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,847] INFO [Broker id=6] Leader __consumer_offsets-45 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,852] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,856] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,860] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,860] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 10:27:00,861] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,861] INFO [Broker id=6] Leader __consumer_offsets-43 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,866] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,877] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,881] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,884] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 10:27:00,884] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,885] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,890] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,893] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,895] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,895] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 10:27:00,896] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,896] INFO [Broker id=6] Leader __consumer_offsets-10 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,900] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,914] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,916] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,918] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 10:27:00,919] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,920] INFO [Broker id=6] Leader __consumer_offsets-22 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,926] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,930] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,930] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,931] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 10:27:00,931] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,932] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,936] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,941] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,942] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,944] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 10:27:00,946] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,947] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,951] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,955] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,956] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,956] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 10:27:00,957] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,957] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,964] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,968] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,969] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,969] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 10:27:00,970] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,970] INFO [Broker id=6] Leader __consumer_offsets-25 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,980] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,994] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,998] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,998] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 10:27:00,999] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,000] INFO [Broker id=6] Leader __consumer_offsets-6 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,007] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,012] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,013] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,014] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 10:27:01,014] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,015] INFO [Broker id=6] Leader __consumer_offsets-38 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,017] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,031] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,032] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,032] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 10:27:01,032] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,032] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,036] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,039] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,040] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,040] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 10:27:01,041] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,041] INFO [Broker id=6] Leader __consumer_offsets-33 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,046] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,049] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,050] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,050] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 10:27:01,050] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,051] INFO [Broker id=6] Leader __consumer_offsets-2 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,053] INFO [Broker id=6] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2025-12-24 10:27:01,054] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,058] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,059] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,059] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 10:27:01,059] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,060] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,060] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,064] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,064] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,065] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 10:27:01,065] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,066] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,066] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,069] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,070] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,071] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 10:27:01,071] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,071] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,072] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,077] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,079] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,079] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 10:27:01,080] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,080] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,080] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,085] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,087] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,087] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 10:27:01,087] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,087] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,088] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,092] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,093] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,093] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 10:27:01,094] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,095] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,096] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,098] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,099] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,100] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 10:27:01,100] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,101] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,101] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,104] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,105] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,105] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 10:27:01,106] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,106] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,107] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,112] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,115] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,115] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 10:27:01,115] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,116] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,116] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,118] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,120] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,122] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 10:27:01,124] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,124] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,125] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,130] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,131] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,131] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 10:27:01,132] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,132] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,132] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,138] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,139] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,139] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 10:27:01,139] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,140] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,140] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,144] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,145] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,145] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 10:27:01,146] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,146] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,146] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,149] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,150] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,151] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 10:27:01,151] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,151] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,151] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,159] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,159] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,160] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 10:27:01,160] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,161] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,161] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,165] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,167] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,167] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 10:27:01,167] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,168] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,169] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,173] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,174] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,176] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 10:27:01,176] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,176] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,176] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,180] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,181] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,181] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 10:27:01,181] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,182] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,182] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,184] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,185] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,186] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 10:27:01,186] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,186] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,186] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,188] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,190] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,190] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 10:27:01,190] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,191] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,191] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,195] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,197] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,197] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 10:27:01,198] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,198] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,199] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,203] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,203] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,204] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 10:27:01,204] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,204] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,205] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,211] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,212] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,212] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 10:27:01,212] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,213] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,213] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,215] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,218] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,219] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 10:27:01,220] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,220] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,220] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,224] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,225] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,225] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 10:27:01,225] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,225] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,226] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,229] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,231] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,232] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 10:27:01,232] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,233] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,233] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,237] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,238] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,239] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 10:27:01,239] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,240] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,240] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,243] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,243] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,244] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,244] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,245] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,245] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,252] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,254] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,257] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 10:27:01,257] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,257] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,258] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,262] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,264] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,264] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 10:27:01,265] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,265] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,266] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,270] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,271] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,277] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 10:27:01,278] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,279] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,279] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,283] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,284] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,284] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 10:27:01,285] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,285] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,287] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,298] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,299] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,300] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 10:27:01,300] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,301] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,302] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,304] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,305] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,305] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 10:27:01,306] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,306] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,306] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-12, __consumer_offsets-24, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,307] INFO [Broker id=6] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 10:27:01,365] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,383] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-48 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,405] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,432] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,434] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,435] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,434] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,439] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,434] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,441] INFO [Broker id=6] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 10:27:01,440] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,442] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,442] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,442] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,443] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,443] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,445] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,445] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,446] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,446] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,446] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,448] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,449] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,449] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,450] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,449] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,452] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,454] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,450] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,455] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,457] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,455] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,458] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,458] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,458] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,459] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,459] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,459] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,458] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,461] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,461] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,462] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,462] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,462] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,462] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,462] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,463] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,463] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,464] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,465] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,465] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,466] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,465] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,479] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,479] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,482] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,483] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,483] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,484] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,483] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,486] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,485] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,488] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,487] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,488] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,490] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,491] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,494] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,495] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,496] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,498] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,499] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,500] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,513] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,514] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,518] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,519] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,520] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,521] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,521] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,522] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,522] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,521] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-16 in 3 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-13 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-45 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,524] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-43 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,524] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,526] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-22 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,526] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,528] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,528] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,528] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,530] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,530] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,531] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-6 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,532] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-38 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,532] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,532] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,533] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-33 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,533] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,533] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,534] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,535] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,535] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,536] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,536] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,536] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,537] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,538] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,538] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,538] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,538] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,538] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,536] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,542] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,542] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,544] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,544] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,545] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,545] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,545] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,546] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,546] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,547] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,547] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,547] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,548] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,548] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,548] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,548] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,548] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,549] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,549] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,550] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,549] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,550] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,550] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,550] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,550] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,551] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,551] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,551] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,551] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,551] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,551] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,552] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,552] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,552] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,550] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,552] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,552] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,553] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,553] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,553] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,552] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,553] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,554] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,553] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,554] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,554] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,554] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,557] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:36:50,663] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:37:00,456] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:04:30,910] INFO [RaftManager id=6] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:04:30,943] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:04:30,957] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:04:30,958] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:14:30,954] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:14:30,959] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:31:28,850] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000007242-0000000003 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 11:31:28,987] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000007242-0000000003 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 11:34:26,480] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 11:34:26,829] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 11:34:26,846] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 11:34:26,864] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:31,348] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 11:34:31,490] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 11:34:31,511] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:31,815] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:31,863] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:31,936] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 11:34:31,942] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 11:34:31,950] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 11:34:31,958] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,300] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,304] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,305] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 1ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,335] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 11:34:32,367] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 11:34:32,368] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:32,383] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:32,435] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1281) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:32,440] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 11:34:32,445] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 11:34:32,509] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:32,521] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-24 11:34:32,553] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,610] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:32,652] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:32,656] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:32,662] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:32,674] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:32,723] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:32,732] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1833181899 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:32,773] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:32,775] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:32,822] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 11:34:32,827] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 11:34:32,825] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:32,926] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:32,965] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:32,982] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 11:34:33,069] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,075] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:33,078] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:33,175] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,282] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,387] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,489] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,595] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,696] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,799] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,805] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 11:34:33,900] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,914] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 11:34:33,931] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 11:34:34,002] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,012] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 11:34:34,041] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,043] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1281) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:34,050] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,063] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,097] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,104] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,262] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,263] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,287] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,292] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:34,296] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,350] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,354] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,367] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 1 (org.apache.kafka.raft.FollowerState)
[2025-12-24 11:34:34,372] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,367] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,375] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,377] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,375] INFO [MetadataLoader id=6] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,396] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,455] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 11:34:34,456] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,457] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,465] INFO [BrokerLifecycleManager id=6] Incarnation RIgiB4K7QEajlfzPq1b62g of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,470] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,510] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,510] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,510] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,512] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,512] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 11:34:34,514] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=4, epoch=1) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 11:34:34,523] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 11:34:34,545] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 11:34:34,573] INFO Loaded 0 logs in 48ms (kafka.log.LogManager)
[2025-12-24 11:34:34,583] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 11:34:34,591] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 11:34:34,600] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 10 (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,615] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 11:34:34,812] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 11:34:34,822] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 11:34:34,822] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 11:34:34,823] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:34,831] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:34,833] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 11:34:34,839] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 11:34:34,839] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 11:34:34,858] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,928] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,929] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 11:34:34,930] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 11:34:34,930] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 11:34:34,937] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 11:34:34,943] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:34,947] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,956] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 11:34:35,019] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:35,023] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 11:34:35,032] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 11:34:35,032] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 11:34:35,033] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 11:34:35,035] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 11:34:35,041] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 11:34:35,061] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 11:34:35,063] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 11:34:35,063] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 11:34:35,064] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 11:34:35,065] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 11:34:35,067] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:35,067] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:35,070] INFO Kafka startTimeMs: 1766576075066 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:35,074] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 11:34:39,360] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-12-24 11:34:39,363] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:39,365] INFO [Broker id=6] Creating new partition _schemas-0 with topic id 0IctBlPsRtCyKBZU8rL83Q. (state.change.logger)
[2025-12-24 11:34:39,388] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:39,394] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 11:34:39,399] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 11:34:39,401] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:39,403] INFO [Broker id=6] Leader _schemas-0 with topic id Some(0IctBlPsRtCyKBZU8rL83Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:39,420] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 11:34:40,353] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-12-24 11:34:40,355] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-11, __consumer_offsets-12, __consumer_offsets-44, __consumer_offsets-41, __consumer_offsets-21, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-32, __consumer_offsets-29, __consumer_offsets-26, __consumer_offsets-8, __consumer_offsets-5, __consumer_offsets-37, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:40,357] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,364] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,371] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,371] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 11:34:40,372] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,372] INFO [Broker id=6] Leader __consumer_offsets-47 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,377] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,382] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,383] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,384] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 11:34:40,385] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,385] INFO [Broker id=6] Leader __consumer_offsets-16 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,390] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,395] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,399] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,399] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 11:34:40,400] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,400] INFO [Broker id=6] Leader __consumer_offsets-11 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,404] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,409] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,412] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,413] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 11:34:40,413] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,414] INFO [Broker id=6] Leader __consumer_offsets-12 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,420] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,424] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,426] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,426] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 11:34:40,426] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,427] INFO [Broker id=6] Leader __consumer_offsets-44 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,431] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,439] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,441] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,441] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 11:34:40,441] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,442] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,445] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,451] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,453] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,453] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 11:34:40,454] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,455] INFO [Broker id=6] Leader __consumer_offsets-21 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,460] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,464] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,465] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,466] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 11:34:40,466] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,467] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,473] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,476] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,477] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,478] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,478] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,479] INFO [Broker id=6] Leader __consumer_offsets-0 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,482] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,487] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,489] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,489] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 11:34:40,490] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,491] INFO [Broker id=6] Leader __consumer_offsets-32 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,499] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,502] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,504] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,504] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 11:34:40,504] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,505] INFO [Broker id=6] Leader __consumer_offsets-29 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,509] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,515] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,517] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,518] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 11:34:40,519] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,519] INFO [Broker id=6] Leader __consumer_offsets-26 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,532] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,541] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,543] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,544] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 11:34:40,544] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,544] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,551] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,556] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,557] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,558] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 11:34:40,558] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,559] INFO [Broker id=6] Leader __consumer_offsets-5 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,564] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,569] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,571] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,572] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 11:34:40,572] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,573] INFO [Broker id=6] Leader __consumer_offsets-37 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,582] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,589] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,589] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,590] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 11:34:40,593] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,594] INFO [Broker id=6] Leader __consumer_offsets-34 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,606] INFO [Broker id=6] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2025-12-24 11:34:40,609] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,621] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,622] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,623] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 11:34:40,624] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,626] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,627] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,631] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,632] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,633] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 11:34:40,633] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,633] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,634] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,637] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,638] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,638] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 11:34:40,640] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,640] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,641] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,645] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,649] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,652] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 11:34:40,654] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,654] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,655] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,662] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,664] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,665] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 11:34:40,666] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,668] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,668] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,674] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,676] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,677] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 11:34:40,678] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,679] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,680] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,688] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,689] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,689] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 11:34:40,690] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,690] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,691] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,698] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,700] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,701] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 11:34:40,701] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,702] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,703] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,713] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,715] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,716] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 11:34:40,717] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,718] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,718] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,721] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,723] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,724] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 11:34:40,724] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,725] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,726] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,731] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,732] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,733] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 11:34:40,734] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,734] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,735] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,739] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,740] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,741] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 11:34:40,741] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,742] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,742] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,747] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,751] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,751] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 11:34:40,751] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,752] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,753] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,759] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,761] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,762] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 11:34:40,762] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,762] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,763] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,766] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,768] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,768] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 11:34:40,769] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,769] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,769] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,776] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,777] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,777] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 11:34:40,778] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,778] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,779] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,786] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,787] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,787] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 11:34:40,788] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,788] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,789] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,793] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,795] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,795] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 11:34:40,795] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,796] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,796] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,801] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,803] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,806] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 11:34:40,806] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,806] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,807] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,811] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,811] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,812] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 11:34:40,812] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,815] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,817] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,825] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,829] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,832] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 11:34:40,834] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,838] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,845] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,858] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,861] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,872] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 11:34:40,876] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,886] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,910] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,922] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,923] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,923] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 11:34:40,924] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,924] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,924] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,932] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,934] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,935] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 11:34:40,935] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,936] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,936] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,938] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,940] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,941] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 11:34:40,942] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,942] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,942] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,950] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,950] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,951] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 11:34:40,951] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,952] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,952] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,959] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,960] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,962] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 11:34:40,962] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,963] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,964] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,966] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,967] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,968] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 11:34:40,968] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,968] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,968] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,976] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,978] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,978] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 11:34:40,978] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,978] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,979] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,983] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,984] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,985] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 11:34:40,985] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,985] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,986] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,988] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,990] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,991] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 11:34:40,992] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,993] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,993] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,995] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,996] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,996] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 11:34:40,997] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,997] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,997] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,000] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,002] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,002] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 11:34:41,002] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,002] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,003] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,009] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,012] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,012] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 11:34:41,014] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,014] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,015] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,016] INFO [Broker id=6] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 11:34:41,034] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,036] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-48 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,037] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,040] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,043] INFO [Broker id=6] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 11:34:41,041] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,040] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,044] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,044] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,045] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,046] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,045] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,049] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,049] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,049] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,048] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,050] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,050] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,050] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,051] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,051] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,051] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,052] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,052] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,052] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,053] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,053] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,053] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,053] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,054] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,054] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,054] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,055] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,054] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,055] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,055] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,056] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,056] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,056] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,057] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,057] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,057] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,057] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,058] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,058] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,058] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,059] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,059] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,059] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,059] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,060] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,059] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,060] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,060] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,061] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,061] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,061] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,062] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,063] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,062] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,063] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,063] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,064] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,064] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,064] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,065] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,065] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,065] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,065] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,066] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,066] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,067] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,067] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,089] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,095] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,097] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,097] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,098] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,098] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,099] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,099] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,100] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,100] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,101] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,101] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,102] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,102] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,102] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,103] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,103] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,103] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,104] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,105] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,112] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,113] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,113] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,114] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,115] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,116] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,114] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-47 in 15 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,118] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-16 in 20 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,117] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,121] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,121] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,122] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,122] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,123] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,124] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,124] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-11 in 26 milliseconds for epoch 0, of which 21 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,129] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-12 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,129] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-44 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,129] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,130] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 28 milliseconds for epoch 0, of which 27 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,130] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,130] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,131] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-21 in 29 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,131] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,132] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,132] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,132] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,134] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,134] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-0 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,135] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,135] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-32 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,136] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-29 in 23 milliseconds for epoch 0, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,135] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,136] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-26 in 21 milliseconds for epoch 0, of which 21 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,137] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,137] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,138] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,137] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,138] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-5 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,138] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,139] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-37 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,139] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,140] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,140] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,140] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,140] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,140] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,141] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,141] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,141] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,141] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,141] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,141] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,142] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,142] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,142] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,142] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,142] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,142] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,142] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,143] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,139] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-34 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,143] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,144] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,144] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,144] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,145] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,144] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,145] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,145] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,145] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,146] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,146] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,146] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,145] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,148] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,148] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,149] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,149] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,149] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,150] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,150] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,150] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,150] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,151] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,148] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,152] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,152] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,152] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,153] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,154] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,151] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,154] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,154] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,154] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,155] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,155] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,154] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,155] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,155] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,155] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,156] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,157] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,157] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,157] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,158] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,158] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,159] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,156] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,160] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,160] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,160] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,161] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,162] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 11:34:41,162] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,163] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,164] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,164] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,164] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,164] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,165] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,165] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,165] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,166] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,166] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,167] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,167] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,167] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,168] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,168] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,168] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,199] INFO [GroupCoordinator 6]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-93ef8cd3-199a-48cd-af46-94c116b674a3 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,227] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-93ef8cd3-199a-48cd-af46-94c116b674a3 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:44,233] INFO [GroupCoordinator 6]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:44,256] INFO [GroupCoordinator 6]: Assignment received from leader sr-1-93ef8cd3-199a-48cd-af46-94c116b674a3 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:44:34,203] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:44:34,221] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:44:38,979] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:25:13,609] INFO [RaftManager id=6] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5986, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:25:13,630] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:25:13,689] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5986, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:25:13,734] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:30:26,432] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:26,437] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 6546 due to node 2 being disconnected (elapsed time since creation: 253449ms, elapsed time since send: 253449ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:26,515] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:26,527] INFO [RaftManager id=6] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:26,540] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:27,048] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:27,049] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:30:59,006] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6165, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6165, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:59,878] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:59,884] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:35:22,273] INFO [RaftManager id=6] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,298] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,305] INFO [RaftManager id=6] Completed transition to Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,317] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,328] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 6286 (kafka.log.UnifiedLog)
[2025-12-24 12:35:22,346] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 6286 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 12:35:22,346] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6286 (kafka.log.UnifiedLog$)
[2025-12-24 12:35:22,352] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 6286 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:35:22,353] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 6ms for segment recovery from offset 6286 (kafka.log.UnifiedLog$)
[2025-12-24 12:35:22,353] INFO [RaftManager id=6] Truncated to offset 6286 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 12:35:22,891] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:35:22,893] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:40:53,615] INFO [RaftManager id=6] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,648] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,663] INFO [RaftManager id=6] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,676] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,680] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 6405 (kafka.log.UnifiedLog)
[2025-12-24 12:40:53,692] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 6405 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,693] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6405 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,695] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=6286, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000006286.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:40:53,708] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 6405 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:40:53,708] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 5ms for segment recovery from offset 6405 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,709] INFO [RaftManager id=6] Truncated to offset 6405 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 12:40:54,492] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:40:54,494] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:47:56,431] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000007247-0000000009 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 12:47:56,612] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000007247-0000000009 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 12:50:53,797] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:50:53,803] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:28,500] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-93ef8cd3-199a-48cd-af46-94c116b674a3 on LeaveGroup; client reason: consumer poll timeout has expired.) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:28,510] INFO [GroupCoordinator 6]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:28,539] INFO [GroupCoordinator 6]: Member MemberMetadata(memberId=sr-1-93ef8cd3-199a-48cd-af46-94c116b674a3, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.15, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: consumer poll timeout has expired. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:28,575] INFO [GroupCoordinator 6]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-1b0a7467-68ac-42ad-bac2-74ce00d339fa and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:28,615] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 2 (__consumer_offsets-29) (reason: Adding new member sr-1-1b0a7467-68ac-42ad-bac2-74ce00d339fa with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:28,825] INFO [RaftManager id=6] Completed transition to Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13281, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 13:52:28,906] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13281, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 13:52:29,601] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:29,604] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 13:52:31,630] INFO [GroupCoordinator 6]: Stabilized group schema-registry generation 3 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:31,649] INFO [GroupCoordinator 6]: Assignment received from leader sr-1-1b0a7467-68ac-42ad-bac2-74ce00d339fa for group schema-registry for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:01:51,140] INFO [RaftManager id=6] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14357, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:01:51,184] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=11, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14357, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:01:51,654] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:01:51,655] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:02:23,346] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000014422-0000000011 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 14:02:23,405] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000014422-0000000011 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 14:04:55,450] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 3 (__consumer_offsets-29) (reason: Removing member sr-1-1b0a7467-68ac-42ad-bac2-74ce00d339fa on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:04:55,453] INFO [GroupCoordinator 6]: Group schema-registry with generation 4 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:04:55,466] INFO [GroupCoordinator 6]: Member MemberMetadata(memberId=sr-1-1b0a7467-68ac-42ad-bac2-74ce00d339fa, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.15, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:06:55,251] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 14:06:55,908] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 14:06:55,933] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 14:06:55,936] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,109] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 14:07:02,346] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 14:07:02,355] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,573] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,617] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,729] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 14:07:02,741] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 14:07:02,762] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 14:07:02,782] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,984] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:02,991] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:02,994] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:03,050] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 14:07:03,096] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,102] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,103] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 14:07:03,161] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1763) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:03,168] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 14:07:03,168] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 14:07:03,243] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,244] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-24 14:07:03,258] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:03,296] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,298] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,302] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,309] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,315] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1168284729 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,352] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,359] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 14:07:03,363] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 14:07:03,407] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:03,416] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 14:07:03,455] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,512] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1763) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:03,557] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,613] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,624] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,662] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,666] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,678] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,771] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,822] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,825] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,886] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,990] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 14:07:03,990] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,029] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 14:07:04,032] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 14:07:04,044] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 14:07:04,067] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,074] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,076] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,085] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,104] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,110] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,110] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,130] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,154] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,155] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,166] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,278] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,300] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,317] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 14:07:04,324] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,337] INFO [BrokerLifecycleManager id=6] Incarnation BYDbgOhLTUS3fVivUiketg of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:04,354] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,389] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,417] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,417] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 14:07:04,419] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 14:07:04,420] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 14:07:04,441] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,445] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,518] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,622] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,723] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,826] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,927] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,949] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:04,974] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=9, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-24 14:07:04,979] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 9 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,982] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 9 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,983] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 8 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,983] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 8 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,984] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 8 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,984] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=8, epoch=2) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 14:07:04,985] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 14:07:04,987] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,989] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 14:07:04,995] INFO Loaded 0 logs in 9ms (kafka.log.LogManager)
[2025-12-24 14:07:04,996] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 14:07:04,997] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 14:07:05,003] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 14:07:05,003] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:05,013] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:05,034] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:05,096] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 10 (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,295] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 14:07:05,299] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 14:07:05,301] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:05,301] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 14:07:05,303] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:05,303] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 14:07:05,304] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 14:07:05,304] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 14:07:05,311] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 8 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:05,360] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,362] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 14:07:05,363] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 14:07:05,363] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 14:07:05,365] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 14:07:05,369] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:05,372] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 14:07:05,377] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,431] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,431] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 14:07:05,433] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 14:07:05,433] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 14:07:05,434] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 14:07:05,435] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 14:07:05,437] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 14:07:05,442] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 14:07:05,442] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 14:07:05,443] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 14:07:05,443] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 14:07:05,444] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 14:07:05,445] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,445] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,446] INFO Kafka startTimeMs: 1766585225444 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,447] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 14:07:09,207] INFO Sent auto-creation request for Set(_schemas) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 14:07:09,272] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-12-24 14:07:09,275] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:09,278] INFO [Broker id=6] Creating new partition _schemas-0 with topic id zuwcO5jNTBGzR0UELwhRXw. (state.change.logger)
[2025-12-24 14:07:09,319] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:09,329] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 14:07:09,336] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 14:07:09,341] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:09,345] INFO [Broker id=6] Leader _schemas-0 with topic id Some(zuwcO5jNTBGzR0UELwhRXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:09,420] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 14:07:10,782] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-12-24 14:07:10,784] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-16, __consumer_offsets-46, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-31, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-5, __consumer_offsets-6, __consumer_offsets-36, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:10,785] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,792] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,794] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,796] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 14:07:10,798] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,799] INFO [Broker id=6] Leader __consumer_offsets-16 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,805] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,809] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,811] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,811] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 14:07:10,813] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,814] INFO [Broker id=6] Leader __consumer_offsets-46 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,822] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,828] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,831] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,832] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 14:07:10,833] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,834] INFO [Broker id=6] Leader __consumer_offsets-43 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,841] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,845] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,848] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,848] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 14:07:10,849] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,849] INFO [Broker id=6] Leader __consumer_offsets-12 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,861] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,865] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,867] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,868] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 14:07:10,868] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,869] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,877] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,880] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,883] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,884] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 14:07:10,884] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,884] INFO [Broker id=6] Leader __consumer_offsets-10 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,889] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,896] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,897] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,899] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 14:07:10,900] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,902] INFO [Broker id=6] Leader __consumer_offsets-21 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,907] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,910] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,912] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,912] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 14:07:10,913] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,913] INFO [Broker id=6] Leader __consumer_offsets-19 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,917] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,923] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,924] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,925] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 14:07:10,925] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,925] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,929] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,934] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,936] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,936] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 14:07:10,937] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,937] INFO [Broker id=6] Leader __consumer_offsets-29 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,942] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,945] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,945] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,946] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 14:07:10,947] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,947] INFO [Broker id=6] Leader __consumer_offsets-25 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,952] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,955] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,956] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,957] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 14:07:10,957] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,958] INFO [Broker id=6] Leader __consumer_offsets-5 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,963] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,966] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,966] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,967] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 14:07:10,967] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,967] INFO [Broker id=6] Leader __consumer_offsets-6 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,971] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,974] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,974] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,974] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 14:07:10,975] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,976] INFO [Broker id=6] Leader __consumer_offsets-36 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,980] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,983] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,984] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,984] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 14:07:10,985] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,985] INFO [Broker id=6] Leader __consumer_offsets-33 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,989] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,992] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,993] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,993] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 14:07:10,994] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,994] INFO [Broker id=6] Leader __consumer_offsets-2 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,998] INFO [Broker id=6] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2025-12-24 14:07:10,999] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,002] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,004] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,005] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 14:07:11,005] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,005] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,006] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,009] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,010] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,010] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 14:07:11,010] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,011] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,011] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,016] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,017] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,017] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 14:07:11,018] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,019] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,020] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,023] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,025] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,025] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 14:07:11,026] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,026] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,026] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,031] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,032] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,033] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 14:07:11,033] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,034] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,034] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,037] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,038] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,038] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 14:07:11,039] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,039] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,039] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,044] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,045] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,045] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 14:07:11,045] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,046] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,046] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,049] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,050] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,051] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 14:07:11,051] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,051] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,052] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,056] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,057] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,057] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 14:07:11,058] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,058] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,058] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,061] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,064] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,065] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 14:07:11,065] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,066] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,066] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,071] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,072] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,073] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 14:07:11,074] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,074] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,074] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,093] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,106] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,107] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 14:07:11,107] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,108] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,109] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,115] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,116] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,117] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 14:07:11,117] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,117] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,117] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,121] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,122] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,122] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 14:07:11,122] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,123] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,123] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,126] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,127] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,129] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 14:07:11,130] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,130] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,130] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,134] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,136] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,137] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 14:07:11,137] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,137] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,138] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,141] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,142] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,143] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 14:07:11,143] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,143] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,144] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,146] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,147] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,148] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 14:07:11,149] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,149] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,149] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,153] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,154] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,154] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 14:07:11,155] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,155] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,155] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,158] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,159] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,160] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 14:07:11,161] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,161] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,161] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,164] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,165] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,165] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 14:07:11,165] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,165] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,166] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,168] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,168] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,169] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 14:07:11,169] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,170] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,170] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,174] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,175] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,176] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 14:07:11,176] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,176] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,177] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,180] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,180] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,182] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 14:07:11,183] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,183] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,184] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,192] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,193] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,193] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 14:07:11,194] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,195] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,196] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,204] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,205] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,205] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 14:07:11,205] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,205] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,206] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,209] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,210] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,211] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 14:07:11,211] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,212] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,212] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,216] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,217] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,217] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,217] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,217] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,217] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,220] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,221] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,221] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 14:07:11,222] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,223] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,224] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,227] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,228] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,228] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 14:07:11,228] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,229] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,229] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,232] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,233] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,263] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 14:07:11,264] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,264] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,264] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,269] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,271] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,272] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 14:07:11,272] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,273] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,274] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,277] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,290] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,292] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 14:07:11,293] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,293] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,293] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,296] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,297] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,297] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 14:07:11,297] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,297] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,297] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,298] INFO [Broker id=6] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 14:07:11,308] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,309] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-48 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,310] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,310] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,311] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,311] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,311] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,311] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,311] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,311] INFO [Broker id=6] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 14:07:11,311] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,312] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,312] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,312] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,312] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,312] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,312] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,312] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,313] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,313] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,313] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,313] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,313] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,313] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,313] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,313] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,313] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,313] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,314] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,314] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,314] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,314] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,315] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,315] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,315] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,316] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,316] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,316] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,316] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,316] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,316] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,316] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,316] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,316] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,316] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,317] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,317] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,317] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,317] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,317] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,317] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,317] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,317] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,317] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,318] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,318] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,318] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,318] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,318] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,323] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,324] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,325] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,325] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,325] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,325] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,326] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,326] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,326] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,326] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,326] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,327] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-16 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,327] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,327] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-46 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,327] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,327] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,327] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,327] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,328] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,328] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,328] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-12 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,329] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,330] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,330] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,330] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,330] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,330] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,331] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,330] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-10 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,331] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-21 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,331] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-19 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,332] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-29 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,332] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-5 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-6 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-36 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-2 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,343] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,523] INFO [GroupCoordinator 6]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-60b84273-2263-4f20-a98b-65ecee1ca368 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,529] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-60b84273-2263-4f20-a98b-65ecee1ca368 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:14,538] INFO [GroupCoordinator 6]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:14,574] INFO [GroupCoordinator 6]: Assignment received from leader sr-1-60b84273-2263-4f20-a98b-65ecee1ca368 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:17:04,952] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:09,329] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:24,718] INFO [RaftManager id=6] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1298, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:17:24,733] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:24,810] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1298, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:17:24,836] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,226] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:26:55,278] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,341] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,390] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:26:55,392] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,443] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,518] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:26:55,521] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,555] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:26:55,574] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,603] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 2429 (kafka.log.UnifiedLog)
[2025-12-24 14:26:55,667] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 2429 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,668] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2429 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,678] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2429 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:26:55,679] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 11ms for segment recovery from offset 2429 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,679] INFO [RaftManager id=6] Truncated to offset 2429 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:35:24,679] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-12-24 14:35:24,712] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,714] INFO [Broker id=6] Creating new partition financial_transactions-0 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,743] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,749] INFO Created log for partition financial_transactions-0 in /tmp/kafka-logs/financial_transactions-0 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,751] INFO [Partition financial_transactions-0 broker=6] No checkpointed highwatermark is found for partition financial_transactions-0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,751] INFO [Partition financial_transactions-0 broker=6] Log loaded for partition financial_transactions-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,754] INFO [Broker id=6] Leader financial_transactions-0 with topic id Some(EMZmT9B4Rquk5_L89orVRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:35:24,762] INFO [Broker id=6] Transitioning 4 partition(s) to local followers. (state.change.logger)
[2025-12-24 14:35:24,763] INFO [Broker id=6] Creating new partition financial_transactions-1 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,769] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,770] INFO Created log for partition financial_transactions-1 in /tmp/kafka-logs/financial_transactions-1 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,773] INFO [Partition financial_transactions-1 broker=6] No checkpointed highwatermark is found for partition financial_transactions-1 (kafka.cluster.Partition)
[2025-12-24 14:35:24,774] INFO [Partition financial_transactions-1 broker=6] Log loaded for partition financial_transactions-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,774] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,775] INFO [Broker id=6] Creating new partition financial_transactions-2 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,779] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,780] INFO Created log for partition financial_transactions-2 in /tmp/kafka-logs/financial_transactions-2 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,781] INFO [Partition financial_transactions-2 broker=6] No checkpointed highwatermark is found for partition financial_transactions-2 (kafka.cluster.Partition)
[2025-12-24 14:35:24,781] INFO [Partition financial_transactions-2 broker=6] Log loaded for partition financial_transactions-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,781] INFO [Broker id=6] Follower financial_transactions-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,782] INFO [Broker id=6] Creating new partition financial_transactions-3 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,786] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,787] INFO Created log for partition financial_transactions-3 in /tmp/kafka-logs/financial_transactions-3 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,787] INFO [Partition financial_transactions-3 broker=6] No checkpointed highwatermark is found for partition financial_transactions-3 (kafka.cluster.Partition)
[2025-12-24 14:35:24,787] INFO [Partition financial_transactions-3 broker=6] Log loaded for partition financial_transactions-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,788] INFO [Broker id=6] Follower financial_transactions-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,788] INFO [Broker id=6] Creating new partition financial_transactions-4 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,794] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,794] INFO Created log for partition financial_transactions-4 in /tmp/kafka-logs/financial_transactions-4 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,795] INFO [Partition financial_transactions-4 broker=6] No checkpointed highwatermark is found for partition financial_transactions-4 (kafka.cluster.Partition)
[2025-12-24 14:35:24,795] INFO [Partition financial_transactions-4 broker=6] Log loaded for partition financial_transactions-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,795] INFO [Broker id=6] Follower financial_transactions-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,796] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-1, financial_transactions-2, financial_transactions-3, financial_transactions-4) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,796] INFO [Broker id=6] Stopped fetchers as part of become-follower for 4 partitions (state.change.logger)
[2025-12-24 14:35:24,803] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(financial_transactions-1 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-4 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,804] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(financial_transactions-2 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-3 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,804] INFO [Broker id=6] Started fetchers as part of become-follower for 4 partitions (state.change.logger)
[2025-12-24 14:35:24,990] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:24,991] INFO [UnifiedLog partition=financial_transactions-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:35:24,992] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:24,995] INFO [UnifiedLog partition=financial_transactions-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:35:24,992] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:24,998] INFO [UnifiedLog partition=financial_transactions-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:35:25,000] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:25,002] INFO [UnifiedLog partition=financial_transactions-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:36:55,579] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:53:22,430] INFO [RaftManager id=6] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,555] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,607] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,609] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 5421 (kafka.log.UnifiedLog)
[2025-12-24 14:53:22,625] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 5421 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,627] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 5421 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,627] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=2429, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000002429.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:53:22,638] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 5421 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:53:22,638] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 8ms for snapshot load and 3ms for segment recovery from offset 5421 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,638] INFO [RaftManager id=6] Truncated to offset 5421 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:53:23,557] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:53:23,560] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 15:03:22,494] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 15:03:22,987] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 15:08:35,181] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000007239-0000000007 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 15:08:35,392] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000007239-0000000007 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 16:18:13,800] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-60b84273-2263-4f20-a98b-65ecee1ca368 on LeaveGroup; client reason: consumer poll timeout has expired.) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:13,816] INFO [GroupCoordinator 6]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:13,857] INFO [GroupCoordinator 6]: Member MemberMetadata(memberId=sr-1-60b84273-2263-4f20-a98b-65ecee1ca368, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.14, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: consumer poll timeout has expired. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:13,873] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:13,880] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:13,939] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:13,942] INFO [GroupCoordinator 6]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-96233839-16cd-4b20-9e0d-76829fd13cc4 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:13,990] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:13,993] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,005] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 2 (__consumer_offsets-29) (reason: Adding new member sr-1-96233839-16cd-4b20-9e0d-76829fd13cc4 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:14,046] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,066] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:14,066] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,078] INFO [RaftManager id=6] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:18:14,097] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:18:14,117] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,122] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:14,123] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,174] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,194] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:14,194] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,245] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:17,018] INFO [GroupCoordinator 6]: Stabilized group schema-registry generation 3 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:17,032] INFO [GroupCoordinator 6]: Assignment received from leader sr-1-96233839-16cd-4b20-9e0d-76829fd13cc4 for group schema-registry for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:21:55,428] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000014410-0000000008 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 16:21:55,469] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000014410-0000000008 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 16:28:14,157] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,414] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,431] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 16906 due to node 2 being disconnected (elapsed time since creation: 1021266ms, elapsed time since send: 1021265ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,500] INFO [RaftManager id=6] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=15282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:46:14,564] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=15282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:46:15,162] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:15,163] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:56:14,573] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,344] INFO [RaftManager id=6] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=21099, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 17:34:58,386] INFO [RaftManager id=6] Completed transition to Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 17:34:58,420] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,521] INFO [RaftManager id=6] Completed transition to Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 17:34:58,590] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=21099, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 17:34:58,630] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:38:56,461] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000021574-0000000013 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 17:38:56,525] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000021574-0000000013 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 17:44:58,724] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:44:58,736] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:41,480] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,000] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 31136 due to node 2 being disconnected (elapsed time since creation: 2135ms, elapsed time since send: 2134ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,031] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,046] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 7120 due to node 2 being disconnected (elapsed time since creation: 4505ms, elapsed time since send: 4505ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,084] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,303] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-24 18:35:43,656] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,657] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,707] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,718] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,719] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,770] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,785] INFO [RaftManager id=6] Completed transition to Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:43,810] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,970] INFO [RaftManager id=6] Completed transition to Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:44,326] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:44,341] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 28352 (kafka.log.UnifiedLog)
[2025-12-24 18:35:44,341] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:44,393] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:44,405] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:44,408] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 28352 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 18:35:44,410] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 28352 (kafka.log.UnifiedLog$)
[2025-12-24 18:35:44,411] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=5421, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000005421.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 18:35:44,454] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 28352 with 0 producer ids in 10 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 18:35:44,455] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 12ms for snapshot load and 33ms for segment recovery from offset 28352 (kafka.log.UnifiedLog$)
[2025-12-24 18:35:44,455] INFO [RaftManager id=6] Truncated to offset 28352 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 18:35:44,458] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:38:56,572] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000028735-0000000016 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 18:38:56,628] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000028735-0000000016 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 18:45:43,999] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:45:44,040] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,668] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,703] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 20:32:35,768] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 20:32:35,805] INFO [RaftManager id=6] Completed transition to Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33975, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 20:32:35,819] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,866] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33975, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 20:32:35,923] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 21:41:15,988] INFO [RaftManager id=6] Completed transition to Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,012] INFO [RaftManager id=6] Completed transition to Unattached(epoch=19, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,091] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=19, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=19, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,941] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 21:41:16,942] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 22:57:16,671] INFO [RaftManager id=6] Completed transition to Unattached(epoch=20, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=19, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34091, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 22:57:16,730] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=20, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34091, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=20, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 22:57:17,687] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 22:57:17,693] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 23:41:42,704] INFO [RaftManager id=6] Completed transition to Unattached(epoch=21, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=20, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 23:41:42,728] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=21, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=21, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 23:41:44,296] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 23:41:44,301] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 00:44:16,903] INFO [RaftManager id=6] Completed transition to Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=21, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:16,946] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=22, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:17,049] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=23, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=22, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:17,053] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 34210 (kafka.log.UnifiedLog)
[2025-12-25 00:44:17,065] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 34210 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 00:44:17,066] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 34210 (kafka.log.UnifiedLog$)
[2025-12-25 00:44:17,066] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=28352, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000028352.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 00:44:17,103] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 34210 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 00:44:17,103] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 32ms for segment recovery from offset 34210 (kafka.log.UnifiedLog$)
[2025-12-25 00:44:17,103] INFO [RaftManager id=6] Truncated to offset 34210 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 00:44:18,675] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 00:44:18,676] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:19:59,875] INFO [RaftManager id=6] Completed transition to Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=23, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:19:59,903] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=24, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:19:59,917] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=25, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=24, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:19:59,920] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 34270 (kafka.log.UnifiedLog)
[2025-12-25 01:19:59,926] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 34270 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 01:19:59,927] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 34270 (kafka.log.UnifiedLog$)
[2025-12-25 01:19:59,928] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=34210, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000034210.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:19:59,937] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 34270 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:19:59,937] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 6ms for segment recovery from offset 34270 (kafka.log.UnifiedLog$)
[2025-12-25 01:19:59,938] INFO [RaftManager id=6] Truncated to offset 34270 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 01:20:01,373] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:20:01,378] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:29:59,876] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:29:59,977] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:05,366] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:05,552] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:06,892] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:06,960] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:07,106] INFO [RaftManager id=6] Completed transition to Unattached(epoch=26, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=25, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=35590, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:07,723] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:07,910] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 01:31:08,172] INFO [RaftManager id=6] Completed transition to Unattached(epoch=28, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=26, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,224] INFO [RaftManager id=6] Completed transition to Unattached(epoch=29, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=28, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,252] INFO [RaftManager id=6] Completed transition to Unattached(epoch=30, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=29, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,309] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=30, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=35590, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=30, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,316] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 35590 (kafka.log.UnifiedLog)
[2025-12-25 01:31:08,333] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 35590 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,334] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 35590 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,335] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=34270, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000034270.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:31:08,358] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 35590 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:31:08,359] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 21ms for segment recovery from offset 35590 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,359] INFO [RaftManager id=6] Truncated to offset 35590 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 01:31:08,365] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:33:39,887] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000035892-0000000030 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 01:33:39,953] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000035892-0000000030 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 01:54:59,123] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,154] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 40689 due to node 1 being disconnected (elapsed time since creation: 899723ms, elapsed time since send: 899723ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,314] INFO [RaftManager id=6] Completed transition to Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=30, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=36648, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:54:59,370] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=31, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=36648, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:54:59,682] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,682] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 02:04:59,416] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 02:04:59,487] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 02:48:38,777] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000043060-0000000031 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 02:48:38,903] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000043060-0000000031 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 03:46:47,574] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:47,878] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 55199 due to node 3 being disconnected (elapsed time since creation: 2268ms, elapsed time since send: 2268ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,136] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,181] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 12573 due to node 3 being disconnected (elapsed time since creation: 4566ms, elapsed time since send: 4566ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,450] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:46:49,680] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 03:46:50,824] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:51,037] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 55201 due to node 3 being disconnected (elapsed time since creation: 2332ms, elapsed time since send: 2332ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:52,957] INFO [RaftManager id=6] Completed transition to Unattached(epoch=33, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=31, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=50007, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 03:46:53,206] INFO [RaftManager id=6] Completed transition to Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=33, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 03:46:53,216] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:53,469] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=34, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=50007, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 03:46:53,549] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:46:53,651] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:53,662] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:46:53,704] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:48:38,642] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000050217-0000000034 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 03:48:38,761] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000050217-0000000034 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 03:56:53,104] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:56:53,532] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 04:48:39,119] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000057390-0000000034 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 04:48:39,299] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000057390-0000000034 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 04:54:06,341] INFO [RaftManager id=6] Completed transition to Unattached(epoch=35, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=34, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 04:54:06,477] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=35, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=35, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 04:54:07,067] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 04:54:07,083] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:10:03,417] INFO [RaftManager id=6] Completed transition to Unattached(epoch=36, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=35, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,480] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=36, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=36, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,497] INFO [RaftManager id=6] Completed transition to Unattached(epoch=37, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=36, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,509] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=37, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=37, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,512] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 58151 (kafka.log.UnifiedLog)
[2025-12-25 05:10:03,531] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 58151 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,532] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 58151 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,533] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=35590, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000035590.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:10:03,587] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 58151 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:10:03,587] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 10ms for snapshot load and 45ms for segment recovery from offset 58151 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,588] INFO [RaftManager id=6] Truncated to offset 58151 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 05:10:05,270] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:10:05,271] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:27:01,636] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,648] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 64482 due to node 3 being disconnected (elapsed time since creation: 898567ms, elapsed time since send: 898566ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,725] INFO [RaftManager id=6] Completed transition to Unattached(epoch=38, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=37, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,755] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=38, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=38, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,770] INFO [RaftManager id=6] Completed transition to Unattached(epoch=39, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=38, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,785] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=39, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=39, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,788] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 58390 (kafka.log.UnifiedLog)
[2025-12-25 05:27:01,796] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 58390 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,797] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 58390 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,798] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=58151, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000058151.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:27:01,808] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 58390 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:27:01,809] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 8ms for segment recovery from offset 58390 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,809] INFO [RaftManager id=6] Truncated to offset 58390 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 05:27:01,939] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,939] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:37:01,993] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:37:02,010] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 06:18:38,667] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000064553-0000000039 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 06:18:38,806] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000064553-0000000039 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 07:18:38,957] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000071718-0000000039 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 07:18:39,047] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000071718-0000000039 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 07:58:34,527] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 07:58:34,589] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 07:58:34,615] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 07:58:34,634] INFO [RaftManager id=6] Completed transition to Unattached(epoch=40, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=39, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=75411, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 07:58:34,698] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 07:58:34,705] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=40, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=75411, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=40, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 07:58:34,801] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 07:59:20,712] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 3 (__consumer_offsets-29) (reason: Removing member sr-1-96233839-16cd-4b20-9e0d-76829fd13cc4 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 07:59:20,729] INFO [GroupCoordinator 6]: Group schema-registry with generation 4 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 07:59:20,758] INFO [GroupCoordinator 6]: Member MemberMetadata(memberId=sr-1-96233839-16cd-4b20-9e0d-76829fd13cc4, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.14, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:01:44,256] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:01:44,858] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:01:45,347] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:01:45,394] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:52,651] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:01:53,046] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:01:53,074] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,807] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:54,197] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:54,420] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-25 12:01:54,451] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-25 12:01:54,469] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-25 12:01:54,491] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:55,702] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:55,728] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:55,746] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:55,994] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-25 12:01:56,180] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-25 12:01:56,191] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:56,193] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:56,497] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1235) from null (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:01:56,506] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-25 12:01:56,512] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-25 12:01:57,045] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,068] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-25 12:01:57,164] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,241] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:57,276] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:57,328] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:57,333] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:57,333] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,369] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:57,457] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,490] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:01:57,522] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:01:57,560] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,599] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:01:57,628] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1579268396 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:57,680] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,718] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,726] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-25 12:01:57,763] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,802] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,909] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,058] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,169] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,273] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,376] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,643] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,751] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,089] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,249] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,395] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,605] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,783] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,811] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:00,827] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:00,845] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,989] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,006] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,091] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,096] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,210] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,326] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,366] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,341] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,536] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,640] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,750] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,118] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,326] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,427] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,533] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,575] INFO [RaftManager id=6] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1235) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:02,606] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,616] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,624] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,644] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,648] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,731] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,762] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,817] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,953] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,057] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,147] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:02:03,158] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,237] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,243] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,285] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,285] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,306] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,412] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-25 12:02:03,424] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:02:03,472] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-25 12:02:03,541] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,565] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:03,600] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:03,620] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,629] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,629] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,680] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,686] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,693] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,784] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,787] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,795] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,890] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,987] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-25 12:02:03,994] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,993] INFO [RaftManager id=6] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:04,041] INFO [BrokerLifecycleManager id=6] Incarnation 0a7oVJPPTNSdA53Kz0N9aw of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:04,026] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,995] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,054] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:04,054] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:04,088] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:02:04,088] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,090] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:02:04,098] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:02:04,192] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,294] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,396] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,493] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:04,498] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,505] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,515] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 4 (org.apache.kafka.raft.FollowerState)
[2025-12-25 12:02:04,528] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,537] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,563] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,570] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,600] INFO [MetadataLoader id=6] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,608] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,617] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,617] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,618] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=4) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-25 12:02:04,625] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-25 12:02:04,636] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-25 12:02:04,649] INFO Loaded 0 logs in 22ms (kafka.log.LogManager)
[2025-12-25 12:02:04,651] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-25 12:02:04,653] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-25 12:02:04,689] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-25 12:02:04,799] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-25 12:02:04,828] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:04,841] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-25 12:02:04,856] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:04,856] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-25 12:02:04,865] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:04,870] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:02:04,879] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-25 12:02:04,879] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:02:04,958] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,082] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:05,088] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:02:05,090] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:02:05,090] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:02:05,094] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:02:05,104] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:02:05,127] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:02:05,254] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:05,442] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:05,484] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:02:05,487] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:02:05,488] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:02:05,490] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-25 12:02:05,496] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:02:05,510] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:02:05,552] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:02:05,569] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:02:05,576] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:02:05,581] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:02:05,588] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-25 12:02:05,595] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:05,596] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:05,602] INFO Kafka startTimeMs: 1766664125595 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:05,612] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-25 12:02:21,305] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:02:21,336] INFO [Broker id=6] Creating new partition _schemas-0 with topic id 5ynfWlfWR8GWI1r7sCBFzA. (state.change.logger)
[2025-12-25 12:02:21,412] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:21,428] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-25 12:02:21,433] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-25 12:02:21,437] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:21,439] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:21,461] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:21,463] INFO [Broker id=6] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:02:21,581] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:21,600] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(5ynfWlfWR8GWI1r7sCBFzA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:21,611] INFO [Broker id=6] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:02:21,619] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:21,628] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:21,646] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:02:22,757] INFO [Broker id=6] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-25 12:02:22,763] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-42, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-1, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:22,768] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:22,800] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:22,819] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:22,821] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-25 12:02:22,828] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:22,872] INFO [Broker id=6] Leader __consumer_offsets-15 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:22,945] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:22,973] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:22,983] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:22,992] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-25 12:02:22,992] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:22,993] INFO [Broker id=6] Leader __consumer_offsets-48 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,005] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,015] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,018] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,020] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-25 12:02:23,025] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,026] INFO [Broker id=6] Leader __consumer_offsets-13 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,045] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,076] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,078] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,078] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-25 12:02:23,079] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,081] INFO [Broker id=6] Leader __consumer_offsets-46 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,099] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,112] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,116] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,116] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-25 12:02:23,117] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,117] INFO [Broker id=6] Leader __consumer_offsets-11 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,148] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,151] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,154] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,154] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-25 12:02:23,155] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,156] INFO [Broker id=6] Leader __consumer_offsets-42 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,166] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,171] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,175] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,175] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-25 12:02:23,175] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,176] INFO [Broker id=6] Leader __consumer_offsets-22 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,186] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,197] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,199] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,200] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-25 12:02:23,201] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,202] INFO [Broker id=6] Leader __consumer_offsets-18 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,206] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,210] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,212] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,213] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-25 12:02:23,214] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,215] INFO [Broker id=6] Leader __consumer_offsets-32 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,228] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,234] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,236] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,238] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-25 12:02:23,241] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,243] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,254] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,261] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,265] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,266] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-25 12:02:23,267] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,267] INFO [Broker id=6] Leader __consumer_offsets-26 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,278] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,289] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,295] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,297] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-25 12:02:23,300] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,301] INFO [Broker id=6] Leader __consumer_offsets-7 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,313] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,322] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,324] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,325] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-25 12:02:23,325] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,327] INFO [Broker id=6] Leader __consumer_offsets-40 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,337] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,344] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,347] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,348] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-25 12:02:23,348] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,348] INFO [Broker id=6] Leader __consumer_offsets-38 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,352] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,354] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,356] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,356] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-25 12:02:23,357] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,358] INFO [Broker id=6] Leader __consumer_offsets-3 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,362] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,367] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,368] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,368] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-25 12:02:23,369] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,370] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,376] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,384] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,387] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,389] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-25 12:02:23,390] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,391] INFO [Broker id=6] Leader __consumer_offsets-34 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,399] INFO [Broker id=6] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:02:23,402] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,407] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,409] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,410] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-25 12:02:23,410] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,411] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,412] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,420] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,422] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,425] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-25 12:02:23,426] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,429] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,429] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,443] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,493] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,494] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-25 12:02:23,495] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,499] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,500] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,506] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,508] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,508] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-25 12:02:23,508] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,509] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,510] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,515] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,518] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,518] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-25 12:02:23,518] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,519] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,519] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,527] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,530] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,532] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-25 12:02:23,532] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,533] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,536] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,540] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,541] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,542] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-25 12:02:23,542] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,543] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,544] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,562] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,564] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,564] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-25 12:02:23,564] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,565] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,567] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,576] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,577] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,579] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-25 12:02:23,580] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,581] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,581] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,602] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,607] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,613] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-25 12:02:23,613] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,615] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,618] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,629] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,634] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,634] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-25 12:02:23,637] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,637] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,640] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,643] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,644] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,645] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-25 12:02:23,645] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,646] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,646] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,651] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,652] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,652] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-25 12:02:23,653] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,653] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,653] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,657] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,661] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,662] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-25 12:02:23,663] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,664] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,664] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,668] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,671] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,672] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-25 12:02:23,672] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,673] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,675] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,680] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,681] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,682] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-25 12:02:23,682] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,683] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,684] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,686] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,687] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,687] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-25 12:02:23,688] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,688] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,689] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,694] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,697] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,697] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-25 12:02:23,697] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,698] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,698] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,701] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,702] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,702] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-25 12:02:23,703] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,703] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,704] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,707] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,709] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,709] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-25 12:02:23,710] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,710] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,710] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,713] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,714] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,715] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-25 12:02:23,715] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,716] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,716] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,724] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,726] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,730] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,730] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,730] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,731] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,739] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,741] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,743] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-25 12:02:23,744] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,745] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,745] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,752] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,753] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,754] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-25 12:02:23,754] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,754] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,755] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,762] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,765] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,766] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-25 12:02:23,766] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,767] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,767] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,770] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,772] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,772] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-25 12:02:23,773] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,774] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,774] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,781] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,782] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,783] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-25 12:02:23,783] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,784] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,784] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,786] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,788] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,788] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-25 12:02:23,789] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,789] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,789] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,793] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,795] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,795] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-25 12:02:23,795] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,796] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,796] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,803] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,804] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,804] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-25 12:02:23,804] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,805] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,805] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,809] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,810] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,810] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-25 12:02:23,811] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,811] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,811] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,818] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,819] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,820] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-25 12:02:23,820] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,821] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,821] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,825] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,826] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,826] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-25 12:02:23,827] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,827] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,828] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-5, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,828] INFO [Broker id=6] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-25 12:02:23,832] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,833] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-45 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,833] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,833] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,834] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,835] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,835] INFO [Broker id=6] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-25 12:02:23,835] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,838] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,839] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,840] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,841] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,842] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,843] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,844] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,845] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,848] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,850] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,851] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,851] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,853] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,854] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,857] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,858] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,859] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,860] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,861] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,862] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,862] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,864] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,864] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,867] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,869] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,870] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,874] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,881] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,888] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,890] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,891] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,891] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,891] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,891] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,892] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,892] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,892] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,893] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,893] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,894] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,895] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,897] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,897] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,901] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,903] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,904] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,904] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,904] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,903] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-15 in 11 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,905] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,906] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,906] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-48 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,907] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,908] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,908] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-13 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,908] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,909] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-46 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,909] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,909] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-11 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,909] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-42 in 15 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-22 in 13 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,911] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-18 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,911] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-32 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,912] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,912] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,912] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-7 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-40 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,914] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-38 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,917] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,917] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-3 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,917] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,918] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,918] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,918] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,919] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,919] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,919] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,919] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,918] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,920] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-34 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,922] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,922] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,923] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,923] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,923] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,923] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,924] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,923] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,924] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,925] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,924] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,925] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,925] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,927] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,927] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,927] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,928] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,928] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,928] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,927] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,929] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,929] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,930] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,931] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,932] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,930] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,933] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,933] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,933] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,933] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,934] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,934] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,934] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,935] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,935] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,935] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,936] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,936] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,937] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,937] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,938] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,939] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,939] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,940] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,940] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,941] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:02:23,941] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,942] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,942] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,943] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,943] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,944] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,945] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,948] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,948] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,948] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,949] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,949] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,950] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,950] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,951] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,953] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,954] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,982] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,983] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,984] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,985] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,986] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,988] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,990] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,992] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,003] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,005] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,006] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,007] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,008] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,009] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,013] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,017] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,018] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,022] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,024] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,027] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,029] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,030] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,030] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,031] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,031] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,032] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,032] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,033] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,033] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,034] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,035] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,036] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,037] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,038] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:12:04,179] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,610] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,635] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,692] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,703] INFO [RaftManager id=6] Completed transition to Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3911, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:46:35,780] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,992] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3911, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:46:36,087] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:36,134] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:36,136] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:36,187] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:34,234] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:57:34,699] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:57:34,743] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:57:34,750] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:40,351] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:57:40,699] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:57:40,715] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:41,174] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:41,200] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:41,319] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-25 12:57:41,327] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-25 12:57:41,341] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-25 12:57:41,361] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:41,785] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:41,796] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:41,813] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:41,961] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-25 12:57:42,091] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-25 12:57:42,132] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:42,137] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:42,326] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1978) from null (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:42,332] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-25 12:57:42,354] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-25 12:57:42,473] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:42,500] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-25 12:57:42,533] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:42,615] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:42,661] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:42,675] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:42,687] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:42,718] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:42,729] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:42,847] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:42,957] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:42,958] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:57:42,962] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:57:42,973] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:43,064] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,073] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-25 12:57:43,084] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@612573758 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:43,101] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,116] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,170] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,222] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,234] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,248] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,257] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,266] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,267] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,279] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,388] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,389] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,392] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,407] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,409] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,415] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,420] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,498] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,508] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,518] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,529] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,536] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,606] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,707] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,765] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,781] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,794] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,801] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,812] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,816] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,812] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,938] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,959] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:57:44,043] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,146] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-25 12:57:44,151] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,154] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:57:44,188] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-25 12:57:44,262] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,279] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:44,285] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,298] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,318] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,327] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,355] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,368] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,375] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,504] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,545] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:44,608] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:44,632] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,934] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:44,942] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:44,936] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,977] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:44,992] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:45,073] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,153] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:45,169] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:45,174] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,198] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-25 12:57:45,212] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:45,253] INFO [BrokerLifecycleManager id=6] Incarnation U9TMo7VQTES8dF2MUiZfKQ of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:45,282] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,282] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:45,383] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,495] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,528] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:45,529] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:45,555] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:57:45,565] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:57:45,565] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:57:45,565] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,672] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,718] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1978) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:45,779] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,902] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,013] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,115] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,191] INFO [RaftManager id=6] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:46,220] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,321] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,373] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:46,410] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,412] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,427] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,452] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,452] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,529] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,576] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-25 12:57:46,581] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,633] INFO [MetadataLoader id=6] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,676] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,676] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,677] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,683] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=2) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-25 12:57:46,687] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 5 (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:46,688] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-25 12:57:46,707] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-25 12:57:46,815] INFO Loaded 0 logs in 100ms (kafka.log.LogManager)
[2025-12-25 12:57:46,871] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-25 12:57:46,968] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-25 12:57:46,975] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-25 12:57:47,630] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-25 12:57:47,662] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-25 12:57:47,662] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-25 12:57:47,663] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:47,701] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:47,708] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:57:47,719] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:57:47,719] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-25 12:57:47,846] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:48,011] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,026] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:57:48,032] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:57:48,034] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:57:48,056] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:57:48,064] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,119] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:48,157] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:57:48,263] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,265] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:57:48,277] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:57:48,278] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:57:48,283] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-25 12:57:48,307] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:57:48,323] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:57:48,373] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:57:48,377] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:57:48,377] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:57:48,377] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:57:48,379] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-25 12:57:48,383] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:48,387] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:48,398] INFO Kafka startTimeMs: 1766667468382 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:48,420] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-25 12:57:55,755] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:57:55,771] INFO [Broker id=6] Creating new partition _schemas-0 with topic id wkzSqzyiRv-1bUOz43_kpQ. (state.change.logger)
[2025-12-25 12:57:55,823] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:55,832] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-25 12:57:55,836] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-25 12:57:55,840] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:55,844] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:55,845] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:55,846] INFO [Broker id=6] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:57:55,863] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:55,866] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(wkzSqzyiRv-1bUOz43_kpQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:55,867] INFO [Broker id=6] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:57:55,868] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:55,870] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:55,884] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:57:57,003] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-25 12:57:57,224] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-12-25 12:57:57,229] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-20, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:57,233] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,268] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,270] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,272] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-25 12:57:57,273] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,275] INFO [Broker id=6] Leader __consumer_offsets-13 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,322] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,338] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,344] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,348] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-25 12:57:57,353] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,360] INFO [Broker id=6] Leader __consumer_offsets-46 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,371] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,378] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,380] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,380] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-25 12:57:57,381] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,381] INFO [Broker id=6] Leader __consumer_offsets-11 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,391] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,399] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,401] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,401] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-25 12:57:57,402] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,403] INFO [Broker id=6] Leader __consumer_offsets-44 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,407] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,413] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,415] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,415] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-25 12:57:57,417] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,417] INFO [Broker id=6] Leader __consumer_offsets-24 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,427] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,443] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,444] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,445] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-25 12:57:57,445] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,446] INFO [Broker id=6] Leader __consumer_offsets-21 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,450] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,494] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,497] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,502] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-25 12:57:57,503] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,503] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,533] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,547] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,550] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,550] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-25 12:57:57,550] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,551] INFO [Broker id=6] Leader __consumer_offsets-17 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,564] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,576] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,582] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,583] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-25 12:57:57,584] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,585] INFO [Broker id=6] Leader __consumer_offsets-32 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,602] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,627] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,632] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,635] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-25 12:57:57,642] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,644] INFO [Broker id=6] Leader __consumer_offsets-27 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,661] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,674] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,675] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,676] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-25 12:57:57,677] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,681] INFO [Broker id=6] Leader __consumer_offsets-7 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,699] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,704] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,706] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,707] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-25 12:57:57,707] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,708] INFO [Broker id=6] Leader __consumer_offsets-40 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,720] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,748] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,749] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,752] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-25 12:57:57,759] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,762] INFO [Broker id=6] Leader __consumer_offsets-37 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,784] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,793] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,803] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,803] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-25 12:57:57,803] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,804] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,821] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,845] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,851] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,851] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-25 12:57:57,854] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,854] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,872] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,905] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,910] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,919] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-25 12:57:58,007] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,012] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:58,114] INFO [Broker id=6] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:57:58,114] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,129] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,130] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,131] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-25 12:57:58,131] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,132] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,132] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,144] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,146] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,146] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-25 12:57:58,147] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,147] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,147] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,151] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,152] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,153] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-25 12:57:58,153] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,153] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,153] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,165] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,177] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,178] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-25 12:57:58,178] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,178] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,179] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,197] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,204] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,208] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-25 12:57:58,208] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,208] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,209] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,214] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,221] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,232] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-25 12:57:58,232] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,232] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,232] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,265] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,269] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,275] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-25 12:57:58,275] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,276] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,277] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,296] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,305] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,306] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-25 12:57:58,308] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,309] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,309] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,317] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,329] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,338] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-25 12:57:58,339] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,340] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,345] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,370] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,371] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,371] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-25 12:57:58,371] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,372] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,375] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,413] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,424] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,426] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-25 12:57:58,426] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,426] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,427] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,432] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,433] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,436] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-25 12:57:58,436] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,436] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,438] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,451] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,453] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,454] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-25 12:57:58,454] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,454] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,455] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,461] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,468] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,469] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-25 12:57:58,470] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,470] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,470] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,477] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,482] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,483] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-25 12:57:58,483] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,485] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,487] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,574] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,588] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,590] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-25 12:57:58,595] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,600] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,601] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,616] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,617] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,618] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-25 12:57:58,618] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,620] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,620] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,625] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,629] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,629] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-25 12:57:58,630] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,630] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,630] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,636] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,637] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,637] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-25 12:57:58,638] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,638] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,639] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,649] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,650] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,651] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-25 12:57:58,651] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,652] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,653] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,714] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,715] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,724] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-25 12:57:58,729] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,742] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,751] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,761] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,766] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,767] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-25 12:57:58,768] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,771] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,777] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,787] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,790] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,791] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-25 12:57:58,792] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,792] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,792] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,821] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,828] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,831] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-25 12:57:58,835] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,841] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,842] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,850] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,851] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,851] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-25 12:57:58,852] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,853] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,853] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,868] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,875] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,883] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-25 12:57:58,888] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,889] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,890] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,895] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,901] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,905] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,906] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,911] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,913] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,926] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,930] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,931] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-25 12:57:58,934] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,935] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,936] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,962] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,973] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,975] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-25 12:57:58,976] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,978] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,979] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,985] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,991] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,992] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-25 12:57:58,992] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,992] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,993] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:59,007] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:59,014] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:59,014] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-25 12:57:59,015] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:59,016] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:59,016] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:59,031] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:59,032] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:59,032] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-25 12:57:59,033] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:59,033] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:59,034] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:59,040] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:59,041] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:59,044] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-25 12:57:59,045] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:59,046] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:59,046] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:59,058] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:59,061] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:59,063] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-25 12:57:59,064] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:59,064] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:59,066] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-6, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:59,066] INFO [Broker id=6] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-25 12:57:59,105] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-48 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:59,108] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:59,109] INFO [Broker id=6] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-25 12:57:59,121] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,122] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,125] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,125] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,126] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,126] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,128] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,127] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,128] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,130] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,131] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,130] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,132] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,135] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,138] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,142] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,133] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,148] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,146] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,151] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,145] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-13 in 16 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,154] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-46 in 19 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,150] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,156] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-11 in 14 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,156] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,163] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-44 in 12 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,170] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-24 in 3 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,159] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,168] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,174] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,177] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,176] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,179] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,181] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-21 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,181] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,185] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,187] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,188] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,183] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,190] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,193] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,188] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,194] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,200] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,193] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,201] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,201] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,202] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,198] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-17 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,203] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,202] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,203] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,203] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-32 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,205] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-27 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,203] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,212] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,211] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-7 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,204] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,217] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,215] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-40 in 3 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,214] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,219] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,218] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,221] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,220] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,224] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,219] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,232] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,226] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,238] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,239] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,224] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,242] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,239] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,251] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,247] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,253] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,255] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 15 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,255] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,260] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,261] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,258] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,262] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,263] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,262] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,265] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,262] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,271] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,263] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,271] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,272] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,272] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,273] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,272] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,273] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,277] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,277] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,278] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,274] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,279] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,280] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,280] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,283] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,283] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,286] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,290] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,293] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,295] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,297] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,298] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,298] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,298] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,299] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,299] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,301] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,301] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,301] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,301] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,302] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,302] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,302] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,302] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,299] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,302] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,311] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,311] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,311] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,313] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,310] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,314] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,314] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,314] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,314] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,315] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,315] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,315] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,315] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,315] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,315] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,316] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,316] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,317] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,317] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,318] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,319] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,320] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,320] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,320] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,320] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,318] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,321] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,322] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,322] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,322] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,323] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,324] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,325] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,325] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,326] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,326] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,329] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,329] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,329] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,330] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,330] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,330] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,330] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,330] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,322] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,331] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,332] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,331] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:57:59,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,343] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,344] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,346] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,347] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,347] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,348] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,348] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,349] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,350] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,369] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,369] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,370] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,370] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,371] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,371] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,371] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,372] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,372] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,374] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,376] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,376] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,377] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,378] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,378] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,379] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,380] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,381] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,382] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,383] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,386] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,388] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,389] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,389] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,389] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,390] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,390] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,390] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,391] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,391] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,391] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,391] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,392] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,392] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 13:11:09,715] INFO [RaftManager id=6] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=345, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:11:09,878] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=345, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:11:11,138] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:11:11,165] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:17:49,913] INFO [RaftManager id=6] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=643, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:17:50,016] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=643, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:17:50,162] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:17:50,164] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:23:12,263] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:27:16,396] INFO [RaftManager id=6] Completed transition to Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,460] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,485] INFO [RaftManager id=6] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,500] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,547] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 1301 (kafka.log.UnifiedLog)
[2025-12-25 13:27:16,571] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 1301 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 13:27:16,572] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 1301 (kafka.log.UnifiedLog$)
[2025-12-25 13:27:16,579] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 1301 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 13:27:16,579] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 7ms for segment recovery from offset 1301 (kafka.log.UnifiedLog$)
[2025-12-25 13:27:16,580] INFO [RaftManager id=6] Truncated to offset 1301 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 13:27:17,801] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:27:17,803] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:29:19,676] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:29:19,850] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1607 due to node 1 being disconnected (elapsed time since creation: 2230ms, elapsed time since send: 2229ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:29:20,223] INFO [RaftManager id=6] Completed transition to Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1536, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:20,321] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:29:20,476] INFO [RaftManager id=6] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:20,612] INFO [RaftManager id=6] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:20,995] INFO [RaftManager id=6] Completed transition to Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:21,146] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1536, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:21,245] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:39:21,285] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:39:21,301] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:49:23,690] INFO [RaftManager id=6] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,752] INFO [RaftManager id=6] Completed transition to Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,883] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:24,069] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:49:24,082] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:59:23,486] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:59:23,987] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:19:04,186] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000007224-0000000012 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 14:19:04,700] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000007224-0000000012 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 14:31:37,582] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:39,131] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 9283 due to node 3 being disconnected (elapsed time since creation: 2181ms, elapsed time since send: 2181ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,400] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,408] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,457] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,515] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,535] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,574] INFO [RaftManager id=6] Completed transition to Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=8719, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:40,729] INFO [RaftManager id=6] Completed transition to Unattached(epoch=14, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:41,117] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=14, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=8719, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=14, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:41,310] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 14:31:41,423] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:41,443] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 8720 (kafka.log.UnifiedLog)
[2025-12-25 14:31:41,568] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 8720 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,573] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 8720 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,585] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=1301, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000001301.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 14:31:41,765] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 8720 with 0 producer ids in 16 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 14:31:41,772] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 99ms for snapshot load and 97ms for segment recovery from offset 8720 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,782] INFO [RaftManager id=6] Truncated to offset 8720 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 14:41:40,227] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:41:40,812] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
