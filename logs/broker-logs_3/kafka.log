[2025-12-24 09:10:28,425] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 09:10:28,610] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 09:10:28,616] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 09:10:28,617] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,424] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 09:10:32,532] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 09:10:32,546] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,734] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,759] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,803] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 09:10:32,817] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 09:10:32,826] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 09:10:32,835] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,998] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:33,004] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:33,009] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:33,036] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 09:10:33,088] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 09:10:33,094] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,101] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,126] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1673) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:33,128] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 09:10:33,128] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 09:10:33,148] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,149] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-24 09:10:33,160] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:33,171] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,172] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,173] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,174] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,180] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1365125390 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,211] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 09:10:33,211] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 09:10:33,221] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:33,231] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 09:10:33,251] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,357] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,450] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1673) (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:33,464] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,516] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,524] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,577] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,584] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,608] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,637] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,638] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,690] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,706] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,711] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,722] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,725] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,758] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 09:10:33,794] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,824] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 09:10:33,840] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 09:10:33,892] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 09:10:33,899] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,908] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,911] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,926] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:33,975] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,002] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,021] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,022] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,027] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,032] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,039] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,126] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,140] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,140] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,212] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:34,215] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:34,230] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,233] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 09:10:34,235] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,242] INFO [BrokerLifecycleManager id=6] Incarnation vtr74vqdTLSxQ5CpiSk4wA of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,266] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,332] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,349] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,348] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 09:10:34,358] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 09:10:34,359] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 09:10:34,451] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,512] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:34,528] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,540] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,554] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,556] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,565] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,569] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:34,575] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,625] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,658] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,662] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 1 (org.apache.kafka.raft.FollowerState)
[2025-12-24 09:10:34,671] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,682] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,689] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 7 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,690] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,691] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,691] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,692] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=6, epoch=1) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 09:10:34,695] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 09:10:34,699] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 09:10:34,712] INFO Loaded 0 logs in 17ms (kafka.log.LogManager)
[2025-12-24 09:10:34,716] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 09:10:34,720] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 09:10:34,730] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 09:10:34,811] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,812] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 09:10:34,814] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 09:10:34,871] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,951] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 09:10:34,964] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 09:10:34,965] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 09:10:34,967] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:34,971] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:34,973] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 09:10:34,975] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 09:10:34,975] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 09:10:34,985] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 09:10:34,985] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,997] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 09:10:35,003] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:35,015] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 09:10:35,077] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:35,078] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 09:10:35,080] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 09:10:35,080] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 09:10:35,081] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 09:10:35,089] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 09:10:35,091] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 09:10:35,099] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 09:10:35,099] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 09:10:35,100] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 09:10:35,100] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 09:10:35,100] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 09:10:35,100] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,101] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,101] INFO Kafka startTimeMs: 1766567435100 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,102] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 09:10:38,172] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-24 09:10:38,178] INFO [Broker id=6] Creating new partition _schemas-0 with topic id Im1NE7r1TMiyWVwhbgXZ_g. (state.change.logger)
[2025-12-24 09:10:38,195] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:38,197] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 09:10:38,200] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 09:10:38,201] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:38,203] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:38,205] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:38,206] INFO [Broker id=6] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 09:10:38,231] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:38,233] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(Im1NE7r1TMiyWVwhbgXZ_g),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:38,233] INFO [Broker id=6] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 09:10:38,235] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:38,236] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:38,244] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 09:10:38,965] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 09:10:39,164] INFO [Broker id=6] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-24 09:10:39,166] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-44, __consumer_offsets-10, __consumer_offsets-23, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-8, __consumer_offsets-40, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,168] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,177] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,180] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,180] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 09:10:39,181] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,182] INFO [Broker id=6] Leader __consumer_offsets-15 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,198] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,210] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,212] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,214] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 09:10:39,214] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,215] INFO [Broker id=6] Leader __consumer_offsets-45 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,223] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,230] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,240] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,241] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 09:10:39,242] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,242] INFO [Broker id=6] Leader __consumer_offsets-14 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,254] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,269] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,270] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,271] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 09:10:39,271] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,272] INFO [Broker id=6] Leader __consumer_offsets-44 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,277] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,279] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,281] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,281] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 09:10:39,281] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,282] INFO [Broker id=6] Leader __consumer_offsets-10 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,288] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,292] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,294] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,295] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 09:10:39,295] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,296] INFO [Broker id=6] Leader __consumer_offsets-23 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,300] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,304] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,305] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,305] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 09:10:39,305] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,306] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,311] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,314] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,315] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,315] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 09:10:39,316] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,316] INFO [Broker id=6] Leader __consumer_offsets-49 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,321] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,324] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,325] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,325] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 09:10:39,325] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,326] INFO [Broker id=6] Leader __consumer_offsets-30 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,330] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,335] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,337] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,340] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 09:10:39,340] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,341] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,347] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,350] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,351] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,352] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 09:10:39,353] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,354] INFO [Broker id=6] Leader __consumer_offsets-26 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,361] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,370] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,373] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,374] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 09:10:39,375] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,376] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,381] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,389] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,390] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,391] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 09:10:39,391] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,392] INFO [Broker id=6] Leader __consumer_offsets-40 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,396] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,400] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,401] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,401] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 09:10:39,402] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,402] INFO [Broker id=6] Leader __consumer_offsets-37 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,405] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,412] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,414] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,414] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 09:10:39,415] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,415] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,420] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,430] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,431] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,432] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 09:10:39,432] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,433] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,437] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,439] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,440] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,440] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 09:10:39,440] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,441] INFO [Broker id=6] Leader __consumer_offsets-2 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,447] INFO [Broker id=6] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-24 09:10:39,448] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,451] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,452] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,452] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 09:10:39,452] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,453] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,453] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,455] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,456] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,456] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 09:10:39,457] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,457] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,457] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,462] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,465] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,465] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 09:10:39,466] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,467] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,467] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,471] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,473] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,474] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 09:10:39,475] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,475] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,475] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,479] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,480] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,480] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 09:10:39,481] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,481] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,482] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,487] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,488] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,488] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 09:10:39,488] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,489] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,489] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,494] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,495] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,495] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 09:10:39,496] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,496] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,496] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,498] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,500] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,500] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 09:10:39,500] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,500] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,501] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,503] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,504] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,504] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 09:10:39,505] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,505] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,505] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,508] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,509] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,509] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 09:10:39,509] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,510] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,510] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,514] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,515] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,516] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 09:10:39,516] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,517] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,517] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,520] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,521] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,522] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 09:10:39,522] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,522] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,523] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,528] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,529] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,529] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 09:10:39,529] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,530] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,530] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,533] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,535] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,536] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 09:10:39,536] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,537] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,537] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,543] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,543] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,544] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 09:10:39,544] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,544] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,545] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,547] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,549] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,550] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 09:10:39,550] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,550] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,550] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,553] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,555] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,555] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 09:10:39,556] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,556] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,557] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,561] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,562] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,563] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 09:10:39,563] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,564] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,565] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,567] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,570] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,571] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 09:10:39,571] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,571] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,572] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,576] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,577] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,578] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 09:10:39,578] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,578] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,579] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,582] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,583] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,584] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 09:10:39,585] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,585] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,586] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,592] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,593] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,593] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 09:10:39,594] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,594] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,594] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,597] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,598] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,598] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 09:10:39,599] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,599] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,600] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,605] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,607] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,609] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 09:10:39,609] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,610] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,610] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,613] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,614] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,616] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 09:10:39,617] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,617] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,617] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,620] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,621] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,622] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 09:10:39,622] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,622] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,623] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,626] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,626] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,627] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,627] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,627] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,628] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,633] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,634] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,635] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 09:10:39,636] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,637] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,637] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,644] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,647] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,649] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 09:10:39,649] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,650] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,650] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,655] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,656] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,656] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 09:10:39,656] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,657] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,657] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,660] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,661] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,661] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 09:10:39,661] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,662] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,662] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,668] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,669] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,669] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 09:10:39,669] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,669] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,669] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,673] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,674] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,674] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 09:10:39,675] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,675] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,676] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-7, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,676] INFO [Broker id=6] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 09:10:39,677] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-48 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,679] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,680] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,680] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,680] INFO [Broker id=6] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 09:10:39,680] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,681] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,681] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,681] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,681] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,682] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,682] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,682] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,682] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,683] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,683] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,684] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,684] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,684] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,684] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,685] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,685] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,685] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,685] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,685] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,686] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,686] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,686] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,687] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,687] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,687] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,688] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,688] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,689] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,689] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,690] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,693] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,693] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,694] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,694] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,694] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,695] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,695] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,695] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,696] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,696] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,696] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,696] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,697] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,697] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,697] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,698] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,698] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,698] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,698] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,699] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,699] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,699] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,699] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,699] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,699] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,700] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,700] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,700] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,701] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,701] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,699] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-15 in 4 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,702] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,702] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,702] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,703] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,703] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,704] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-45 in 10 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,705] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,705] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-14 in 10 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,705] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,705] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,706] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,706] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,707] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,707] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,707] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-44 in 11 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,708] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,708] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,708] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-10 in 12 milliseconds for epoch 0, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,708] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,708] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,709] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,709] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,709] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,709] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,709] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,711] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,708] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-23 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,711] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,712] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,712] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,713] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,712] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,713] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,714] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,714] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-49 in 16 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,715] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-30 in 17 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,715] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,715] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,715] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,715] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,716] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-26 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,716] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,717] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,717] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,718] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,718] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,718] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,718] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,719] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 19 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,719] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,720] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,719] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-40 in 19 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,721] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,721] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-37 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,722] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,722] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,722] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,723] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,723] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,724] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,723] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-2 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,724] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,725] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,726] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,726] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,726] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,727] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,727] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,728] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,728] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,728] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,728] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,731] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,729] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,732] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,731] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,732] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,732] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,732] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,732] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,733] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,732] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,734] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,734] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,734] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,734] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,734] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,734] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,735] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,734] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,735] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,735] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,735] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,736] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,736] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,736] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,737] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,736] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,737] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,738] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,738] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 09:10:39,738] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,738] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,739] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,739] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,740] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,740] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,741] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,741] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,741] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,741] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,742] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,742] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,743] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,743] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,743] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,743] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,744] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,744] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,744] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,932] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,933] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,933] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,933] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,933] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,933] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,934] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,934] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,934] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,934] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,935] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,935] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,935] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,935] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,935] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,935] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,936] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,936] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,937] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,937] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,938] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,938] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,938] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,938] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,939] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,939] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,939] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,939] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,939] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,939] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,940] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,940] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,940] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,940] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:20:34,330] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:20:39,154] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:07:11,941] INFO [RaftManager id=6] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6779, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:07:11,968] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6779, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:07:13,392] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:07:13,394] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:15:24,171] INFO [RaftManager id=6] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6898, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:15:24,183] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6898, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:15:25,956] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:15:25,956] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:18:15,407] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000007240-0000000003 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 10:18:15,542] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000007240-0000000003 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 10:26:38,087] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 10:26:38,682] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 10:26:38,719] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 10:26:38,729] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:43,753] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 10:26:43,948] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 10:26:44,011] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:44,692] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:44,733] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:44,755] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 10:26:44,760] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 10:26:44,762] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 10:26:44,768] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:44,989] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:45,010] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:45,018] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:45,118] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 10:26:45,197] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 10:26:45,216] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:45,224] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:45,354] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1100) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:45,373] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 10:26:45,379] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 10:26:45,392] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,410] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-24 10:26:45,419] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:45,445] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,448] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,447] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,468] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,494] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,623] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 10:26:45,624] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 10:26:45,653] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:45,682] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,747] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 10:26:45,785] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,835] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@194681932 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:45,852] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,864] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,898] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,906] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,911] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,995] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,999] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,002] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,024] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,039] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,043] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,046] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,131] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,142] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,145] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,179] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,188] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,214] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,256] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,243] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,269] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,281] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,363] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,375] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 10:26:46,427] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,428] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,463] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,546] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,436] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 10:26:46,587] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 10:26:46,546] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,612] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,647] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,691] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 10:26:46,727] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:46,734] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:46,755] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,771] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:46,773] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:46,809] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:47,414] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:47,418] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,520] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,566] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,617] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,639] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:47,642] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,666] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,725] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,936] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,961] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:48,015] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:48,047] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,149] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,201] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,214] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,249] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,258] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,255] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,283] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,292] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,513] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,616] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,630] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 10:26:48,717] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,723] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:48,771] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,792] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,808] INFO [BrokerLifecycleManager id=6] Incarnation Ola3DycHRHWHMHIujq4rLQ of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:48,827] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,839] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:48,942] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 10:26:48,985] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,008] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 10:26:49,031] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 10:26:49,011] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,163] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,278] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,397] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,470] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1100) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:49,495] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:49,508] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:49,596] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,698] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,799] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,926] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,031] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,054] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:50,057] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:50,139] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,240] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,342] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,453] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,572] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,586] INFO [RaftManager id=6] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:50,682] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,731] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:50,745] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,752] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,784] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,811] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,826] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,885] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,906] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:50,928] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,398] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,401] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,513] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,620] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,787] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,808] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:51,814] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,868] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,898] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,919] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:51,924] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,975] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,999] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,200] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,328] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,442] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,548] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,625] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-24 10:26:52,658] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,662] INFO [MetadataLoader id=6] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,681] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,684] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,707] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,713] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=2) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 10:26:52,753] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 10:26:52,839] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 10:26:52,905] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 5 (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:52,940] INFO Loaded 0 logs in 155ms (kafka.log.LogManager)
[2025-12-24 10:26:52,950] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 10:26:52,958] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 10:26:53,053] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 10:26:53,714] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 10:26:53,730] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 10:26:53,732] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 10:26:53,740] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:26:53,770] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:26:53,777] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 10:26:53,783] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 10:26:53,791] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 10:26:53,849] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:53,912] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:53,916] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 10:26:53,916] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 10:26:53,916] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 10:26:53,918] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 10:26:53,948] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:53,957] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:53,982] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 10:26:54,075] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:54,080] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 10:26:54,095] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 10:26:54,099] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 10:26:54,101] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 10:26:54,110] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 10:26:54,115] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 10:26:54,124] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 10:26:54,126] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 10:26:54,126] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 10:26:54,126] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 10:26:54,127] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 10:26:54,127] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,128] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,128] INFO Kafka startTimeMs: 1766572014127 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,133] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 10:26:59,281] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-12-24 10:26:59,287] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:26:59,289] INFO [Broker id=6] Creating new partition _schemas-0 with topic id 4gVnUI_IQ3yRWz4RgXc--Q. (state.change.logger)
[2025-12-24 10:26:59,316] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:59,322] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 10:26:59,332] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 10:26:59,334] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:26:59,341] INFO [Broker id=6] Leader _schemas-0 with topic id Some(4gVnUI_IQ3yRWz4RgXc--Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:26:59,362] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 10:27:00,427] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 10:27:00,785] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-12-24 10:27:00,787] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-16, __consumer_offsets-13, __consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-25, __consumer_offsets-6, __consumer_offsets-38, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:00,787] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,798] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,806] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,812] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 10:27:00,812] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,813] INFO [Broker id=6] Leader __consumer_offsets-16 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,823] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,827] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,830] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,830] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 10:27:00,831] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,831] INFO [Broker id=6] Leader __consumer_offsets-13 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,836] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,844] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,845] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,846] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 10:27:00,846] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,847] INFO [Broker id=6] Leader __consumer_offsets-45 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,852] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,856] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,860] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,860] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 10:27:00,861] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,861] INFO [Broker id=6] Leader __consumer_offsets-43 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,866] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,877] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,881] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,884] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 10:27:00,884] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,885] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,890] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,893] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,895] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,895] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 10:27:00,896] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,896] INFO [Broker id=6] Leader __consumer_offsets-10 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,900] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,914] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,916] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,918] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 10:27:00,919] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,920] INFO [Broker id=6] Leader __consumer_offsets-22 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,926] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,930] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,930] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,931] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 10:27:00,931] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,932] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,936] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,941] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,942] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,944] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 10:27:00,946] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,947] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,951] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,955] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,956] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,956] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 10:27:00,957] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,957] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,964] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,968] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,969] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,969] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 10:27:00,970] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,970] INFO [Broker id=6] Leader __consumer_offsets-25 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,980] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,994] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,998] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,998] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 10:27:00,999] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,000] INFO [Broker id=6] Leader __consumer_offsets-6 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,007] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,012] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,013] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,014] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 10:27:01,014] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,015] INFO [Broker id=6] Leader __consumer_offsets-38 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,017] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,031] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,032] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,032] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 10:27:01,032] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,032] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,036] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,039] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,040] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,040] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 10:27:01,041] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,041] INFO [Broker id=6] Leader __consumer_offsets-33 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,046] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,049] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,050] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,050] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 10:27:01,050] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,051] INFO [Broker id=6] Leader __consumer_offsets-2 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,053] INFO [Broker id=6] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2025-12-24 10:27:01,054] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,058] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,059] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,059] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 10:27:01,059] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,060] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,060] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,064] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,064] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,065] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 10:27:01,065] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,066] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,066] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,069] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,070] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,071] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 10:27:01,071] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,071] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,072] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,077] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,079] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,079] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 10:27:01,080] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,080] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,080] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,085] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,087] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,087] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 10:27:01,087] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,087] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,088] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,092] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,093] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,093] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 10:27:01,094] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,095] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,096] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,098] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,099] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,100] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 10:27:01,100] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,101] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,101] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,104] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,105] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,105] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 10:27:01,106] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,106] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,107] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,112] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,115] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,115] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 10:27:01,115] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,116] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,116] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,118] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,120] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,122] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 10:27:01,124] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,124] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,125] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,130] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,131] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,131] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 10:27:01,132] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,132] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,132] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,138] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,139] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,139] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 10:27:01,139] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,140] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,140] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,144] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,145] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,145] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 10:27:01,146] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,146] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,146] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,149] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,150] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,151] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 10:27:01,151] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,151] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,151] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,159] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,159] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,160] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 10:27:01,160] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,161] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,161] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,165] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,167] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,167] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 10:27:01,167] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,168] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,169] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,173] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,174] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,176] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 10:27:01,176] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,176] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,176] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,180] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,181] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,181] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 10:27:01,181] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,182] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,182] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,184] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,185] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,186] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 10:27:01,186] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,186] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,186] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,188] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,190] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,190] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 10:27:01,190] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,191] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,191] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,195] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,197] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,197] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 10:27:01,198] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,198] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,199] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,203] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,203] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,204] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 10:27:01,204] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,204] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,205] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,211] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,212] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,212] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 10:27:01,212] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,213] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,213] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,215] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,218] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,219] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 10:27:01,220] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,220] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,220] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,224] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,225] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,225] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 10:27:01,225] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,225] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,226] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,229] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,231] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,232] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 10:27:01,232] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,233] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,233] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,237] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,238] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,239] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 10:27:01,239] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,240] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,240] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,243] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,243] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,244] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,244] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,245] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,245] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,252] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,254] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,257] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 10:27:01,257] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,257] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,258] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,262] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,264] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,264] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 10:27:01,265] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,265] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,266] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,270] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,271] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,277] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 10:27:01,278] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,279] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,279] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,283] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,284] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,284] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 10:27:01,285] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,285] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,287] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,298] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,299] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,300] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 10:27:01,300] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,301] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,302] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,304] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,305] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,305] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 10:27:01,306] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,306] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,306] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-12, __consumer_offsets-24, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,307] INFO [Broker id=6] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 10:27:01,365] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,383] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-48 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,405] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,432] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,434] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,435] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,434] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,439] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,434] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,441] INFO [Broker id=6] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 10:27:01,440] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,442] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,442] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,442] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,443] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,443] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,445] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,445] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,446] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,446] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,446] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,448] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,449] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,449] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,450] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,449] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,452] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,454] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,450] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,455] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,457] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,455] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,458] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,458] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,458] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,459] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,459] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,459] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,458] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,461] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,461] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,462] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,462] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,462] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,462] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,462] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,463] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,463] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,464] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,465] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,465] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,466] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,465] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,479] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,479] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,482] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,483] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,483] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,484] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,483] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,486] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,485] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,488] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,487] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,488] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,490] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,491] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,494] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,495] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,496] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,498] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,499] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,500] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,513] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,514] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,518] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,519] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,520] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,521] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,521] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,522] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,522] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,521] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-16 in 3 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-13 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-45 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,524] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-43 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,524] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,526] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-22 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,526] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,528] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,528] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,528] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,530] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,530] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,531] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-6 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,532] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-38 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,532] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,532] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,533] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-33 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,533] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,533] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,534] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,535] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,535] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,536] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,536] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,536] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,537] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,538] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,538] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,538] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,538] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,538] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,536] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,542] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,542] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,544] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,544] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,545] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,545] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,545] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,546] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,546] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,547] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,547] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,547] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,548] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,548] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,548] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,548] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,548] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,549] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,549] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,550] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,549] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,550] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,550] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,550] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,550] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,551] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,551] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,551] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,551] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,551] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,551] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,552] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,552] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,552] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,550] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,552] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,552] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,553] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,553] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,553] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,552] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,553] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,554] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,553] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,554] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,554] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,554] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,555] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 10:27:01,555] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,557] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:36:50,663] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:37:00,456] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:04:30,910] INFO [RaftManager id=6] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:04:30,943] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:04:30,957] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:04:30,958] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:14:30,954] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:14:30,959] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:31:28,850] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000007242-0000000003 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 11:31:28,987] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000007242-0000000003 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 11:34:26,480] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 11:34:26,829] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 11:34:26,846] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 11:34:26,864] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:31,348] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 11:34:31,490] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 11:34:31,511] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:31,815] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:31,863] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:31,936] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 11:34:31,942] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 11:34:31,950] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 11:34:31,958] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,300] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,304] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,305] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 1ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,335] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 11:34:32,367] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 11:34:32,368] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:32,383] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:32,435] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1281) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:32,440] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 11:34:32,445] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 11:34:32,509] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:32,521] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-24 11:34:32,553] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,610] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:32,652] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:32,656] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:32,662] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:32,674] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:32,723] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:32,732] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1833181899 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:32,773] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:32,775] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:32,822] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 11:34:32,827] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 11:34:32,825] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:32,926] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:32,965] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:32,982] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 11:34:33,069] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,075] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:33,078] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:33,175] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,282] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,387] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,489] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,595] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,696] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,799] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,805] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 11:34:33,900] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,914] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 11:34:33,931] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 11:34:34,002] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,012] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 11:34:34,041] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,043] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1281) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:34,050] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,063] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,097] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,104] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,262] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,263] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,287] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,292] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:34,296] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,350] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,354] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,367] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 1 (org.apache.kafka.raft.FollowerState)
[2025-12-24 11:34:34,372] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,367] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,375] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,377] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,375] INFO [MetadataLoader id=6] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,396] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,455] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 11:34:34,456] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,457] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,465] INFO [BrokerLifecycleManager id=6] Incarnation RIgiB4K7QEajlfzPq1b62g of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,470] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,510] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,510] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,510] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,512] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,512] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 11:34:34,514] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=4, epoch=1) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 11:34:34,523] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 11:34:34,545] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 11:34:34,573] INFO Loaded 0 logs in 48ms (kafka.log.LogManager)
[2025-12-24 11:34:34,583] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 11:34:34,591] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 11:34:34,600] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 10 (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,615] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 11:34:34,812] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 11:34:34,822] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 11:34:34,822] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 11:34:34,823] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:34,831] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:34,833] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 11:34:34,839] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 11:34:34,839] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 11:34:34,858] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,928] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,929] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 11:34:34,930] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 11:34:34,930] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 11:34:34,937] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 11:34:34,943] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:34,947] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,956] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 11:34:35,019] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:35,023] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 11:34:35,032] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 11:34:35,032] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 11:34:35,033] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 11:34:35,035] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 11:34:35,041] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 11:34:35,061] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 11:34:35,063] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 11:34:35,063] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 11:34:35,064] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 11:34:35,065] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 11:34:35,067] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:35,067] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:35,070] INFO Kafka startTimeMs: 1766576075066 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:35,074] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 11:34:39,360] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-12-24 11:34:39,363] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:39,365] INFO [Broker id=6] Creating new partition _schemas-0 with topic id 0IctBlPsRtCyKBZU8rL83Q. (state.change.logger)
[2025-12-24 11:34:39,388] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:39,394] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 11:34:39,399] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 11:34:39,401] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:39,403] INFO [Broker id=6] Leader _schemas-0 with topic id Some(0IctBlPsRtCyKBZU8rL83Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:39,420] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 11:34:40,353] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-12-24 11:34:40,355] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-11, __consumer_offsets-12, __consumer_offsets-44, __consumer_offsets-41, __consumer_offsets-21, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-32, __consumer_offsets-29, __consumer_offsets-26, __consumer_offsets-8, __consumer_offsets-5, __consumer_offsets-37, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:40,357] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,364] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,371] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,371] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 11:34:40,372] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,372] INFO [Broker id=6] Leader __consumer_offsets-47 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,377] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,382] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,383] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,384] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 11:34:40,385] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,385] INFO [Broker id=6] Leader __consumer_offsets-16 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,390] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,395] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,399] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,399] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 11:34:40,400] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,400] INFO [Broker id=6] Leader __consumer_offsets-11 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,404] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,409] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,412] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,413] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 11:34:40,413] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,414] INFO [Broker id=6] Leader __consumer_offsets-12 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,420] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,424] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,426] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,426] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 11:34:40,426] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,427] INFO [Broker id=6] Leader __consumer_offsets-44 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,431] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,439] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,441] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,441] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 11:34:40,441] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,442] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,445] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,451] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,453] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,453] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 11:34:40,454] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,455] INFO [Broker id=6] Leader __consumer_offsets-21 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,460] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,464] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,465] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,466] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 11:34:40,466] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,467] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,473] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,476] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,477] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,478] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,478] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,479] INFO [Broker id=6] Leader __consumer_offsets-0 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,482] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,487] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,489] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,489] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 11:34:40,490] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,491] INFO [Broker id=6] Leader __consumer_offsets-32 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,499] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,502] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,504] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,504] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 11:34:40,504] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,505] INFO [Broker id=6] Leader __consumer_offsets-29 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,509] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,515] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,517] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,518] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 11:34:40,519] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,519] INFO [Broker id=6] Leader __consumer_offsets-26 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,532] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,541] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,543] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,544] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 11:34:40,544] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,544] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,551] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,556] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,557] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,558] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 11:34:40,558] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,559] INFO [Broker id=6] Leader __consumer_offsets-5 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,564] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,569] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,571] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,572] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 11:34:40,572] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,573] INFO [Broker id=6] Leader __consumer_offsets-37 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,582] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,589] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,589] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,590] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 11:34:40,593] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,594] INFO [Broker id=6] Leader __consumer_offsets-34 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,606] INFO [Broker id=6] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2025-12-24 11:34:40,609] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,621] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,622] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,623] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 11:34:40,624] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,626] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,627] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,631] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,632] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,633] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 11:34:40,633] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,633] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,634] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,637] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,638] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,638] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 11:34:40,640] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,640] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,641] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,645] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,649] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,652] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 11:34:40,654] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,654] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,655] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,662] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,664] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,665] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 11:34:40,666] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,668] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,668] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,674] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,676] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,677] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 11:34:40,678] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,679] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,680] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,688] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,689] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,689] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 11:34:40,690] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,690] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,691] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,698] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,700] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,701] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 11:34:40,701] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,702] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,703] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,713] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,715] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,716] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 11:34:40,717] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,718] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,718] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,721] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,723] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,724] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 11:34:40,724] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,725] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,726] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,731] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,732] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,733] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 11:34:40,734] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,734] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,735] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,739] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,740] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,741] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 11:34:40,741] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,742] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,742] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,747] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,751] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,751] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 11:34:40,751] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,752] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,753] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,759] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,761] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,762] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 11:34:40,762] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,762] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,763] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,766] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,768] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,768] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 11:34:40,769] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,769] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,769] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,776] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,777] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,777] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 11:34:40,778] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,778] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,779] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,786] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,787] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,787] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 11:34:40,788] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,788] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,789] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,793] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,795] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,795] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 11:34:40,795] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,796] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,796] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,801] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,803] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,806] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 11:34:40,806] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,806] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,807] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,811] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,811] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,812] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 11:34:40,812] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,815] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,817] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,825] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,829] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,832] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 11:34:40,834] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,838] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,845] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,858] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,861] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,872] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 11:34:40,876] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,886] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,910] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,922] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,923] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,923] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 11:34:40,924] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,924] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,924] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,932] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,934] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,935] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 11:34:40,935] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,936] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,936] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,938] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,940] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,941] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 11:34:40,942] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,942] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,942] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,950] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,950] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,951] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 11:34:40,951] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,952] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,952] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,959] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,960] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,962] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 11:34:40,962] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,963] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,964] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,966] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,967] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,968] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 11:34:40,968] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,968] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,968] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,976] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,978] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,978] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 11:34:40,978] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,978] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,979] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,983] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,984] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,985] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 11:34:40,985] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,985] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,986] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,988] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,990] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,991] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 11:34:40,992] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,993] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,993] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,995] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,996] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,996] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 11:34:40,997] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,997] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,997] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,000] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,002] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,002] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 11:34:41,002] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,002] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,003] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,009] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,012] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,012] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 11:34:41,014] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,014] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,015] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,016] INFO [Broker id=6] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 11:34:41,034] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,036] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-48 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,037] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,040] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,043] INFO [Broker id=6] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 11:34:41,041] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,040] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,044] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,044] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,045] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,046] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,045] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,049] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,049] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,049] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,048] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,050] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,050] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,050] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,051] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,051] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,051] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,052] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,052] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,052] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,053] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,053] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,053] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,053] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,054] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,054] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,054] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,055] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,054] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,055] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,055] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,056] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,056] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,056] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,057] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,057] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,057] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,057] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,058] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,058] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,058] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,059] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,059] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,059] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,059] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,060] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,059] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,060] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,060] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,061] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,061] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,061] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,062] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,063] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,062] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,063] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,063] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,064] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,064] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,064] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,065] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,065] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,065] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,065] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,066] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,066] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,067] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,067] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,089] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,095] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,097] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,097] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,098] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,098] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,099] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,099] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,100] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,100] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,101] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,101] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,102] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,102] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,102] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,103] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,103] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,103] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,104] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,105] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,112] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,113] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,113] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,114] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,115] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,116] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,114] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-47 in 15 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,118] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-16 in 20 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,117] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,121] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,121] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,122] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,122] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,123] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,124] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,124] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-11 in 26 milliseconds for epoch 0, of which 21 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,129] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-12 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,129] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-44 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,129] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,130] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 28 milliseconds for epoch 0, of which 27 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,130] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,130] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,131] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-21 in 29 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,131] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,132] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,132] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,132] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,134] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,134] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-0 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,135] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,135] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-32 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,136] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-29 in 23 milliseconds for epoch 0, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,135] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,136] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-26 in 21 milliseconds for epoch 0, of which 21 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,137] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,137] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,138] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,137] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,138] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-5 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,138] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,139] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-37 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,139] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,140] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,140] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,140] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,140] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,140] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,141] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,141] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,141] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,141] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,141] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,141] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,142] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,142] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,142] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,142] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,142] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,142] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,142] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,143] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,139] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-34 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,143] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,144] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,144] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,144] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,145] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,144] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,145] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,145] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,145] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,146] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,146] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,146] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,145] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,148] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,148] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,149] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,149] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,149] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,150] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,150] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,150] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,150] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,151] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,148] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,152] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,152] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,152] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,153] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,154] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,151] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,154] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,154] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,154] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,155] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,155] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,154] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,155] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,155] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,155] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,156] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,157] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,157] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,157] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,158] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,158] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,159] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,156] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,160] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,160] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,160] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,161] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,162] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 11:34:41,162] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,163] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,164] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,164] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,164] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,164] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,165] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,165] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,165] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,166] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,166] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,167] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,167] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,167] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,168] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,168] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,168] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,199] INFO [GroupCoordinator 6]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-93ef8cd3-199a-48cd-af46-94c116b674a3 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,227] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-93ef8cd3-199a-48cd-af46-94c116b674a3 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:44,233] INFO [GroupCoordinator 6]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:44,256] INFO [GroupCoordinator 6]: Assignment received from leader sr-1-93ef8cd3-199a-48cd-af46-94c116b674a3 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:44:34,203] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:44:34,221] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:44:38,979] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:25:13,609] INFO [RaftManager id=6] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5986, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:25:13,630] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:25:13,689] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5986, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:25:13,734] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:30:26,432] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:26,437] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 6546 due to node 2 being disconnected (elapsed time since creation: 253449ms, elapsed time since send: 253449ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:26,515] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:26,527] INFO [RaftManager id=6] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:26,540] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:27,048] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:27,049] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:30:59,006] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6165, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6165, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:59,878] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:59,884] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:35:22,273] INFO [RaftManager id=6] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,298] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,305] INFO [RaftManager id=6] Completed transition to Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,317] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,328] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 6286 (kafka.log.UnifiedLog)
[2025-12-24 12:35:22,346] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 6286 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 12:35:22,346] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6286 (kafka.log.UnifiedLog$)
[2025-12-24 12:35:22,352] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 6286 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:35:22,353] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 6ms for segment recovery from offset 6286 (kafka.log.UnifiedLog$)
[2025-12-24 12:35:22,353] INFO [RaftManager id=6] Truncated to offset 6286 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 12:35:22,891] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:35:22,893] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:40:53,615] INFO [RaftManager id=6] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,648] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,663] INFO [RaftManager id=6] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,676] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,680] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 6405 (kafka.log.UnifiedLog)
[2025-12-24 12:40:53,692] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 6405 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,693] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6405 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,695] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=6286, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000006286.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:40:53,708] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 6405 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:40:53,708] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 5ms for segment recovery from offset 6405 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,709] INFO [RaftManager id=6] Truncated to offset 6405 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 12:40:54,492] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:40:54,494] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:47:56,431] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000007247-0000000009 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 12:47:56,612] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000007247-0000000009 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 12:50:53,797] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:50:53,803] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:28,500] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-93ef8cd3-199a-48cd-af46-94c116b674a3 on LeaveGroup; client reason: consumer poll timeout has expired.) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:28,510] INFO [GroupCoordinator 6]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:28,539] INFO [GroupCoordinator 6]: Member MemberMetadata(memberId=sr-1-93ef8cd3-199a-48cd-af46-94c116b674a3, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.15, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: consumer poll timeout has expired. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:28,575] INFO [GroupCoordinator 6]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-1b0a7467-68ac-42ad-bac2-74ce00d339fa and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:28,615] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 2 (__consumer_offsets-29) (reason: Adding new member sr-1-1b0a7467-68ac-42ad-bac2-74ce00d339fa with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:28,825] INFO [RaftManager id=6] Completed transition to Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13281, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 13:52:28,906] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13281, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 13:52:29,601] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:29,604] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 13:52:31,630] INFO [GroupCoordinator 6]: Stabilized group schema-registry generation 3 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 13:52:31,649] INFO [GroupCoordinator 6]: Assignment received from leader sr-1-1b0a7467-68ac-42ad-bac2-74ce00d339fa for group schema-registry for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:01:51,140] INFO [RaftManager id=6] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14357, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:01:51,184] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=11, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14357, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:01:51,654] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:01:51,655] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:02:23,346] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000014422-0000000011 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 14:02:23,405] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000014422-0000000011 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 14:04:55,450] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 3 (__consumer_offsets-29) (reason: Removing member sr-1-1b0a7467-68ac-42ad-bac2-74ce00d339fa on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:04:55,453] INFO [GroupCoordinator 6]: Group schema-registry with generation 4 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:04:55,466] INFO [GroupCoordinator 6]: Member MemberMetadata(memberId=sr-1-1b0a7467-68ac-42ad-bac2-74ce00d339fa, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.15, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:06:55,251] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 14:06:55,908] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 14:06:55,933] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 14:06:55,936] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,109] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 14:07:02,346] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 14:07:02,355] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,573] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,617] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,729] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 14:07:02,741] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 14:07:02,762] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 14:07:02,782] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,984] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:02,991] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:02,994] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:03,050] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 14:07:03,096] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,102] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,103] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 14:07:03,161] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1763) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:03,168] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 14:07:03,168] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 14:07:03,243] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,244] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-24 14:07:03,258] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:03,296] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,298] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,302] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,309] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,315] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1168284729 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,352] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,359] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 14:07:03,363] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 14:07:03,407] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:03,416] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 14:07:03,455] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,512] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1763) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:03,557] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,613] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,624] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,662] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,666] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,678] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,771] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,822] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,825] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,886] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,990] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 14:07:03,990] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,029] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 14:07:04,032] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 14:07:04,044] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 14:07:04,067] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,074] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,076] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,085] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,104] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,110] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,110] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,130] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,154] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,155] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,166] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,278] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,300] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,317] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 14:07:04,324] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,337] INFO [BrokerLifecycleManager id=6] Incarnation BYDbgOhLTUS3fVivUiketg of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:04,354] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,389] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,417] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,417] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 14:07:04,419] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 14:07:04,420] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 14:07:04,441] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,445] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,518] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,622] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,723] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,826] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,927] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,949] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:04,974] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=9, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-24 14:07:04,979] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 9 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,982] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 9 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,983] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 8 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,983] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 8 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,984] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 8 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,984] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=8, epoch=2) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 14:07:04,985] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 14:07:04,987] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,989] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 14:07:04,995] INFO Loaded 0 logs in 9ms (kafka.log.LogManager)
[2025-12-24 14:07:04,996] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 14:07:04,997] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 14:07:05,003] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 14:07:05,003] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:05,013] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:05,034] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:05,096] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 10 (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,295] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 14:07:05,299] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 14:07:05,301] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:05,301] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 14:07:05,303] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:05,303] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 14:07:05,304] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 14:07:05,304] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 14:07:05,311] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 8 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:05,360] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,362] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 14:07:05,363] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 14:07:05,363] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 14:07:05,365] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 14:07:05,369] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:05,372] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 14:07:05,377] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,431] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,431] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 14:07:05,433] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 14:07:05,433] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 14:07:05,434] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 14:07:05,435] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 14:07:05,437] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 14:07:05,442] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 14:07:05,442] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 14:07:05,443] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 14:07:05,443] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 14:07:05,444] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 14:07:05,445] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,445] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,446] INFO Kafka startTimeMs: 1766585225444 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,447] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 14:07:09,207] INFO Sent auto-creation request for Set(_schemas) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 14:07:09,272] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-12-24 14:07:09,275] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:09,278] INFO [Broker id=6] Creating new partition _schemas-0 with topic id zuwcO5jNTBGzR0UELwhRXw. (state.change.logger)
[2025-12-24 14:07:09,319] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:09,329] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 14:07:09,336] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 14:07:09,341] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:09,345] INFO [Broker id=6] Leader _schemas-0 with topic id Some(zuwcO5jNTBGzR0UELwhRXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:09,420] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 14:07:10,782] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-12-24 14:07:10,784] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-16, __consumer_offsets-46, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-31, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-5, __consumer_offsets-6, __consumer_offsets-36, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:10,785] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,792] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,794] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,796] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 14:07:10,798] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,799] INFO [Broker id=6] Leader __consumer_offsets-16 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,805] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,809] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,811] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,811] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 14:07:10,813] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,814] INFO [Broker id=6] Leader __consumer_offsets-46 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,822] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,828] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,831] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,832] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 14:07:10,833] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,834] INFO [Broker id=6] Leader __consumer_offsets-43 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,841] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,845] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,848] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,848] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 14:07:10,849] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,849] INFO [Broker id=6] Leader __consumer_offsets-12 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,861] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,865] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,867] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,868] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 14:07:10,868] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,869] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,877] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,880] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,883] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,884] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 14:07:10,884] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,884] INFO [Broker id=6] Leader __consumer_offsets-10 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,889] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,896] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,897] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,899] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 14:07:10,900] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,902] INFO [Broker id=6] Leader __consumer_offsets-21 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,907] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,910] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,912] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,912] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 14:07:10,913] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,913] INFO [Broker id=6] Leader __consumer_offsets-19 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,917] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,923] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,924] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,925] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 14:07:10,925] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,925] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,929] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,934] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,936] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,936] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 14:07:10,937] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,937] INFO [Broker id=6] Leader __consumer_offsets-29 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,942] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,945] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,945] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,946] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 14:07:10,947] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,947] INFO [Broker id=6] Leader __consumer_offsets-25 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,952] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,955] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,956] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,957] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 14:07:10,957] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,958] INFO [Broker id=6] Leader __consumer_offsets-5 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,963] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,966] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,966] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,967] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 14:07:10,967] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,967] INFO [Broker id=6] Leader __consumer_offsets-6 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,971] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,974] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,974] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,974] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 14:07:10,975] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,976] INFO [Broker id=6] Leader __consumer_offsets-36 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,980] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,983] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,984] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,984] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 14:07:10,985] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,985] INFO [Broker id=6] Leader __consumer_offsets-33 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,989] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,992] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,993] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,993] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 14:07:10,994] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,994] INFO [Broker id=6] Leader __consumer_offsets-2 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,998] INFO [Broker id=6] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2025-12-24 14:07:10,999] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,002] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,004] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,005] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 14:07:11,005] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,005] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,006] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,009] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,010] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,010] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 14:07:11,010] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,011] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,011] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,016] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,017] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,017] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 14:07:11,018] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,019] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,020] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,023] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,025] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,025] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 14:07:11,026] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,026] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,026] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,031] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,032] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,033] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 14:07:11,033] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,034] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,034] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,037] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,038] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,038] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 14:07:11,039] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,039] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,039] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,044] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,045] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,045] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 14:07:11,045] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,046] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,046] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,049] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,050] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,051] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 14:07:11,051] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,051] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,052] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,056] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,057] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,057] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 14:07:11,058] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,058] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,058] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,061] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,064] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,065] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 14:07:11,065] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,066] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,066] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,071] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,072] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,073] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 14:07:11,074] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,074] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,074] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,093] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,106] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,107] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 14:07:11,107] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,108] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,109] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,115] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,116] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,117] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 14:07:11,117] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,117] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,117] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,121] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,122] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,122] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 14:07:11,122] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,123] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,123] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,126] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,127] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,129] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 14:07:11,130] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,130] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,130] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,134] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,136] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,137] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 14:07:11,137] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,137] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,138] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,141] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,142] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,143] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 14:07:11,143] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,143] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,144] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,146] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,147] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,148] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 14:07:11,149] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,149] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,149] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,153] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,154] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,154] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 14:07:11,155] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,155] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,155] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,158] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,159] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,160] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 14:07:11,161] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,161] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,161] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,164] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,165] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,165] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 14:07:11,165] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,165] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,166] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,168] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,168] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,169] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 14:07:11,169] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,170] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,170] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,174] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,175] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,176] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 14:07:11,176] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,176] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,177] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,180] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,180] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,182] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 14:07:11,183] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,183] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,184] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,192] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,193] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,193] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 14:07:11,194] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,195] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,196] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,204] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,205] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,205] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 14:07:11,205] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,205] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,206] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,209] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,210] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,211] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 14:07:11,211] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,212] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,212] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,216] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,217] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,217] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,217] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,217] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,217] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,220] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,221] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,221] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 14:07:11,222] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,223] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,224] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,227] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,228] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,228] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 14:07:11,228] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,229] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,229] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,232] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,233] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,263] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 14:07:11,264] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,264] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,264] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,269] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,271] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,272] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 14:07:11,272] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,273] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,274] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,277] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,290] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,292] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 14:07:11,293] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,293] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,293] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,296] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,297] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,297] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 14:07:11,297] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,297] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,297] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,298] INFO [Broker id=6] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 14:07:11,308] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,309] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-48 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,310] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,310] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,311] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,311] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,311] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,311] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,311] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,311] INFO [Broker id=6] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-24 14:07:11,311] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,312] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,312] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,312] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,312] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,312] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,312] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,312] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,313] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,313] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,313] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,313] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,313] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,313] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,313] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,313] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,313] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,313] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,314] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,314] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,314] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,314] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,314] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,315] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,315] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,315] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,315] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,316] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,316] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,316] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,316] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,316] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,316] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,316] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,316] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,316] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,316] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,317] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,317] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,317] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,317] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,317] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,317] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,317] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,317] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,317] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,318] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,318] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,318] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,318] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,318] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,323] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,324] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,325] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,325] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,325] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,325] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,326] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,326] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,326] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,326] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,326] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,327] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-16 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,327] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,327] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-46 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,327] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,327] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,327] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,327] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,328] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,328] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,328] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-12 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,329] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,330] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,330] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,330] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,330] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,330] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,331] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,330] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-10 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,331] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-21 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,331] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-19 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,331] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,332] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-29 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,332] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,332] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-5 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-6 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-36 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,333] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,334] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-2 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,335] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,336] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,337] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,339] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,340] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,343] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,523] INFO [GroupCoordinator 6]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-60b84273-2263-4f20-a98b-65ecee1ca368 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,529] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-60b84273-2263-4f20-a98b-65ecee1ca368 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:14,538] INFO [GroupCoordinator 6]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:14,574] INFO [GroupCoordinator 6]: Assignment received from leader sr-1-60b84273-2263-4f20-a98b-65ecee1ca368 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:17:04,952] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:09,329] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:24,718] INFO [RaftManager id=6] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1298, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:17:24,733] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:24,810] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1298, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:17:24,836] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,226] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:26:55,278] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,341] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,390] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:26:55,392] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,443] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,518] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:26:55,521] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,555] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:26:55,574] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,603] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 2429 (kafka.log.UnifiedLog)
[2025-12-24 14:26:55,667] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 2429 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,668] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2429 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,678] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2429 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:26:55,679] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 11ms for segment recovery from offset 2429 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,679] INFO [RaftManager id=6] Truncated to offset 2429 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:35:24,679] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-12-24 14:35:24,712] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,714] INFO [Broker id=6] Creating new partition financial_transactions-0 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,743] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,749] INFO Created log for partition financial_transactions-0 in /tmp/kafka-logs/financial_transactions-0 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,751] INFO [Partition financial_transactions-0 broker=6] No checkpointed highwatermark is found for partition financial_transactions-0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,751] INFO [Partition financial_transactions-0 broker=6] Log loaded for partition financial_transactions-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,754] INFO [Broker id=6] Leader financial_transactions-0 with topic id Some(EMZmT9B4Rquk5_L89orVRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:35:24,762] INFO [Broker id=6] Transitioning 4 partition(s) to local followers. (state.change.logger)
[2025-12-24 14:35:24,763] INFO [Broker id=6] Creating new partition financial_transactions-1 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,769] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,770] INFO Created log for partition financial_transactions-1 in /tmp/kafka-logs/financial_transactions-1 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,773] INFO [Partition financial_transactions-1 broker=6] No checkpointed highwatermark is found for partition financial_transactions-1 (kafka.cluster.Partition)
[2025-12-24 14:35:24,774] INFO [Partition financial_transactions-1 broker=6] Log loaded for partition financial_transactions-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,774] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,775] INFO [Broker id=6] Creating new partition financial_transactions-2 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,779] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,780] INFO Created log for partition financial_transactions-2 in /tmp/kafka-logs/financial_transactions-2 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,781] INFO [Partition financial_transactions-2 broker=6] No checkpointed highwatermark is found for partition financial_transactions-2 (kafka.cluster.Partition)
[2025-12-24 14:35:24,781] INFO [Partition financial_transactions-2 broker=6] Log loaded for partition financial_transactions-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,781] INFO [Broker id=6] Follower financial_transactions-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,782] INFO [Broker id=6] Creating new partition financial_transactions-3 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,786] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,787] INFO Created log for partition financial_transactions-3 in /tmp/kafka-logs/financial_transactions-3 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,787] INFO [Partition financial_transactions-3 broker=6] No checkpointed highwatermark is found for partition financial_transactions-3 (kafka.cluster.Partition)
[2025-12-24 14:35:24,787] INFO [Partition financial_transactions-3 broker=6] Log loaded for partition financial_transactions-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,788] INFO [Broker id=6] Follower financial_transactions-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,788] INFO [Broker id=6] Creating new partition financial_transactions-4 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,794] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,794] INFO Created log for partition financial_transactions-4 in /tmp/kafka-logs/financial_transactions-4 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,795] INFO [Partition financial_transactions-4 broker=6] No checkpointed highwatermark is found for partition financial_transactions-4 (kafka.cluster.Partition)
[2025-12-24 14:35:24,795] INFO [Partition financial_transactions-4 broker=6] Log loaded for partition financial_transactions-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,795] INFO [Broker id=6] Follower financial_transactions-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,796] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-1, financial_transactions-2, financial_transactions-3, financial_transactions-4) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,796] INFO [Broker id=6] Stopped fetchers as part of become-follower for 4 partitions (state.change.logger)
[2025-12-24 14:35:24,803] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(financial_transactions-1 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-4 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,804] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(financial_transactions-2 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-3 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,804] INFO [Broker id=6] Started fetchers as part of become-follower for 4 partitions (state.change.logger)
[2025-12-24 14:35:24,990] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:24,991] INFO [UnifiedLog partition=financial_transactions-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:35:24,992] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:24,995] INFO [UnifiedLog partition=financial_transactions-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:35:24,992] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:24,998] INFO [UnifiedLog partition=financial_transactions-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:35:25,000] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:25,002] INFO [UnifiedLog partition=financial_transactions-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:36:55,579] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:53:22,430] INFO [RaftManager id=6] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,555] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,607] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,609] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 5421 (kafka.log.UnifiedLog)
[2025-12-24 14:53:22,625] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 5421 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,627] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 5421 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,627] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=2429, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000002429.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:53:22,638] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 5421 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:53:22,638] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 8ms for snapshot load and 3ms for segment recovery from offset 5421 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,638] INFO [RaftManager id=6] Truncated to offset 5421 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:53:23,557] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:53:23,560] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 15:03:22,494] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 15:03:22,987] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 15:08:35,181] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000007239-0000000007 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 15:08:35,392] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000007239-0000000007 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 16:18:13,800] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-60b84273-2263-4f20-a98b-65ecee1ca368 on LeaveGroup; client reason: consumer poll timeout has expired.) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:13,816] INFO [GroupCoordinator 6]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:13,857] INFO [GroupCoordinator 6]: Member MemberMetadata(memberId=sr-1-60b84273-2263-4f20-a98b-65ecee1ca368, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.14, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: consumer poll timeout has expired. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:13,873] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:13,880] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:13,939] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:13,942] INFO [GroupCoordinator 6]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-96233839-16cd-4b20-9e0d-76829fd13cc4 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:13,990] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:13,993] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,005] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 2 (__consumer_offsets-29) (reason: Adding new member sr-1-96233839-16cd-4b20-9e0d-76829fd13cc4 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:14,046] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,066] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:14,066] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,078] INFO [RaftManager id=6] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:18:14,097] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:18:14,117] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,122] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:14,123] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,174] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,194] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:14,194] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,245] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:17,018] INFO [GroupCoordinator 6]: Stabilized group schema-registry generation 3 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:18:17,032] INFO [GroupCoordinator 6]: Assignment received from leader sr-1-96233839-16cd-4b20-9e0d-76829fd13cc4 for group schema-registry for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 16:21:55,428] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000014410-0000000008 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 16:21:55,469] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000014410-0000000008 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 16:28:14,157] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,414] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,431] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 16906 due to node 2 being disconnected (elapsed time since creation: 1021266ms, elapsed time since send: 1021265ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,500] INFO [RaftManager id=6] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=15282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:46:14,564] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=15282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:46:15,162] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:15,163] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:56:14,573] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,344] INFO [RaftManager id=6] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=21099, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 17:34:58,386] INFO [RaftManager id=6] Completed transition to Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 17:34:58,420] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,521] INFO [RaftManager id=6] Completed transition to Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 17:34:58,590] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=21099, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 17:34:58,630] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:38:56,461] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000021574-0000000013 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 17:38:56,525] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000021574-0000000013 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 17:44:58,724] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:44:58,736] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:41,480] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,000] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 31136 due to node 2 being disconnected (elapsed time since creation: 2135ms, elapsed time since send: 2134ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,031] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,046] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 7120 due to node 2 being disconnected (elapsed time since creation: 4505ms, elapsed time since send: 4505ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,084] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,303] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-24 18:35:43,656] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,657] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,707] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,718] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,719] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,770] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,785] INFO [RaftManager id=6] Completed transition to Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:43,810] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,970] INFO [RaftManager id=6] Completed transition to Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:44,326] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:44,341] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 28352 (kafka.log.UnifiedLog)
[2025-12-24 18:35:44,341] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:44,393] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:44,405] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:44,408] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 28352 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 18:35:44,410] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 28352 (kafka.log.UnifiedLog$)
[2025-12-24 18:35:44,411] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=5421, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000005421.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 18:35:44,454] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 28352 with 0 producer ids in 10 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 18:35:44,455] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 12ms for snapshot load and 33ms for segment recovery from offset 28352 (kafka.log.UnifiedLog$)
[2025-12-24 18:35:44,455] INFO [RaftManager id=6] Truncated to offset 28352 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 18:35:44,458] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:38:56,572] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000028735-0000000016 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 18:38:56,628] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000028735-0000000016 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 18:45:43,999] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:45:44,040] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,668] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,703] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 20:32:35,768] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 20:32:35,805] INFO [RaftManager id=6] Completed transition to Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33975, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 20:32:35,819] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,866] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33975, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 20:32:35,923] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 21:41:15,988] INFO [RaftManager id=6] Completed transition to Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,012] INFO [RaftManager id=6] Completed transition to Unattached(epoch=19, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,091] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=19, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=19, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,941] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 21:41:16,942] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 22:57:16,671] INFO [RaftManager id=6] Completed transition to Unattached(epoch=20, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=19, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34091, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 22:57:16,730] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=20, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34091, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=20, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 22:57:17,687] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 22:57:17,693] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 23:41:42,704] INFO [RaftManager id=6] Completed transition to Unattached(epoch=21, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=20, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 23:41:42,728] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=21, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=21, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 23:41:44,296] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 23:41:44,301] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 00:44:16,903] INFO [RaftManager id=6] Completed transition to Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=21, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:16,946] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=22, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:17,049] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=23, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=22, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:17,053] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 34210 (kafka.log.UnifiedLog)
[2025-12-25 00:44:17,065] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 34210 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 00:44:17,066] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 34210 (kafka.log.UnifiedLog$)
[2025-12-25 00:44:17,066] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=28352, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000028352.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 00:44:17,103] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 34210 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 00:44:17,103] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 32ms for segment recovery from offset 34210 (kafka.log.UnifiedLog$)
[2025-12-25 00:44:17,103] INFO [RaftManager id=6] Truncated to offset 34210 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 00:44:18,675] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 00:44:18,676] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:19:59,875] INFO [RaftManager id=6] Completed transition to Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=23, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:19:59,903] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=24, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:19:59,917] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=25, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=24, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:19:59,920] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 34270 (kafka.log.UnifiedLog)
[2025-12-25 01:19:59,926] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 34270 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 01:19:59,927] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 34270 (kafka.log.UnifiedLog$)
[2025-12-25 01:19:59,928] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=34210, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000034210.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:19:59,937] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 34270 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:19:59,937] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 6ms for segment recovery from offset 34270 (kafka.log.UnifiedLog$)
[2025-12-25 01:19:59,938] INFO [RaftManager id=6] Truncated to offset 34270 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 01:20:01,373] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:20:01,378] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:29:59,876] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:29:59,977] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:05,366] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:05,552] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:06,892] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:06,960] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:07,106] INFO [RaftManager id=6] Completed transition to Unattached(epoch=26, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=25, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=35590, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:07,723] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:07,910] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 01:31:08,172] INFO [RaftManager id=6] Completed transition to Unattached(epoch=28, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=26, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,224] INFO [RaftManager id=6] Completed transition to Unattached(epoch=29, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=28, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,252] INFO [RaftManager id=6] Completed transition to Unattached(epoch=30, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=29, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,309] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=30, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=35590, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=30, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,316] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 35590 (kafka.log.UnifiedLog)
[2025-12-25 01:31:08,333] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 35590 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,334] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 35590 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,335] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=34270, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000034270.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:31:08,358] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 35590 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:31:08,359] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 21ms for segment recovery from offset 35590 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,359] INFO [RaftManager id=6] Truncated to offset 35590 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 01:31:08,365] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:33:39,887] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000035892-0000000030 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 01:33:39,953] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000035892-0000000030 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 01:54:59,123] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,154] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 40689 due to node 1 being disconnected (elapsed time since creation: 899723ms, elapsed time since send: 899723ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,314] INFO [RaftManager id=6] Completed transition to Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=30, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=36648, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:54:59,370] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=31, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=36648, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:54:59,682] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,682] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 02:04:59,416] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 02:04:59,487] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 02:48:38,777] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000043060-0000000031 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 02:48:38,903] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000043060-0000000031 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 03:46:47,574] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:47,878] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 55199 due to node 3 being disconnected (elapsed time since creation: 2268ms, elapsed time since send: 2268ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,136] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,181] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 12573 due to node 3 being disconnected (elapsed time since creation: 4566ms, elapsed time since send: 4566ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,450] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:46:49,680] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 03:46:50,824] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:51,037] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 55201 due to node 3 being disconnected (elapsed time since creation: 2332ms, elapsed time since send: 2332ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:52,957] INFO [RaftManager id=6] Completed transition to Unattached(epoch=33, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=31, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=50007, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 03:46:53,206] INFO [RaftManager id=6] Completed transition to Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=33, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 03:46:53,216] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:53,469] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=34, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=50007, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 03:46:53,549] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:46:53,651] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:53,662] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:46:53,704] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:48:38,642] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000050217-0000000034 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 03:48:38,761] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000050217-0000000034 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 03:56:53,104] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:56:53,532] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 04:48:39,119] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000057390-0000000034 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 04:48:39,299] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000057390-0000000034 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 04:54:06,341] INFO [RaftManager id=6] Completed transition to Unattached(epoch=35, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=34, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 04:54:06,477] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=35, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=35, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 04:54:07,067] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 04:54:07,083] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:10:03,417] INFO [RaftManager id=6] Completed transition to Unattached(epoch=36, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=35, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,480] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=36, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=36, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,497] INFO [RaftManager id=6] Completed transition to Unattached(epoch=37, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=36, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,509] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=37, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=37, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,512] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 58151 (kafka.log.UnifiedLog)
[2025-12-25 05:10:03,531] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 58151 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,532] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 58151 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,533] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=35590, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000035590.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:10:03,587] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 58151 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:10:03,587] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 10ms for snapshot load and 45ms for segment recovery from offset 58151 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,588] INFO [RaftManager id=6] Truncated to offset 58151 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 05:10:05,270] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:10:05,271] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:27:01,636] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,648] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 64482 due to node 3 being disconnected (elapsed time since creation: 898567ms, elapsed time since send: 898566ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,725] INFO [RaftManager id=6] Completed transition to Unattached(epoch=38, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=37, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,755] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=38, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=38, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,770] INFO [RaftManager id=6] Completed transition to Unattached(epoch=39, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=38, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,785] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=39, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=39, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,788] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 58390 (kafka.log.UnifiedLog)
[2025-12-25 05:27:01,796] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 58390 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,797] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 58390 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,798] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=58151, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000058151.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:27:01,808] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 58390 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:27:01,809] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 8ms for segment recovery from offset 58390 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,809] INFO [RaftManager id=6] Truncated to offset 58390 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 05:27:01,939] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,939] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:37:01,993] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:37:02,010] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 06:18:38,667] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000064553-0000000039 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 06:18:38,806] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000064553-0000000039 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 07:18:38,957] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000071718-0000000039 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 07:18:39,047] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000071718-0000000039 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 07:58:34,527] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 07:58:34,589] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 07:58:34,615] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 07:58:34,634] INFO [RaftManager id=6] Completed transition to Unattached(epoch=40, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=39, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=75411, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 07:58:34,698] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 07:58:34,705] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=40, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=75411, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=40, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 07:58:34,801] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 07:59:20,712] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 3 (__consumer_offsets-29) (reason: Removing member sr-1-96233839-16cd-4b20-9e0d-76829fd13cc4 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 07:59:20,729] INFO [GroupCoordinator 6]: Group schema-registry with generation 4 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 07:59:20,758] INFO [GroupCoordinator 6]: Member MemberMetadata(memberId=sr-1-96233839-16cd-4b20-9e0d-76829fd13cc4, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.14, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:01:44,256] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:01:44,858] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:01:45,347] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:01:45,394] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:52,651] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:01:53,046] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:01:53,074] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,807] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:54,197] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:54,420] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-25 12:01:54,451] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-25 12:01:54,469] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-25 12:01:54,491] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:55,702] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:55,728] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:55,746] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:55,994] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-25 12:01:56,180] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-25 12:01:56,191] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:56,193] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:56,497] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1235) from null (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:01:56,506] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-25 12:01:56,512] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-25 12:01:57,045] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,068] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-25 12:01:57,164] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,241] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:57,276] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:57,328] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:57,333] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:57,333] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,369] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:57,457] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,490] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:01:57,522] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:01:57,560] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,599] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:01:57,628] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1579268396 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:57,680] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,718] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,726] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-25 12:01:57,763] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,802] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,909] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,058] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,169] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,273] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,376] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,643] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,751] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,089] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,249] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,395] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,605] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,783] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,811] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:00,827] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:00,845] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,989] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,006] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,091] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,096] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,210] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,326] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,366] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,341] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,536] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,640] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,750] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,118] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,326] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,427] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,533] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,575] INFO [RaftManager id=6] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1235) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:02,606] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,616] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,624] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,644] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,648] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,731] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,762] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,817] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,953] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,057] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,147] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:02:03,158] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,237] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,243] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,285] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,285] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,306] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,412] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-25 12:02:03,424] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:02:03,472] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-25 12:02:03,541] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,565] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:03,600] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:03,620] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,629] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,629] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,680] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,686] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,693] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,784] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,787] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,795] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,890] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,987] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-25 12:02:03,994] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,993] INFO [RaftManager id=6] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:04,041] INFO [BrokerLifecycleManager id=6] Incarnation 0a7oVJPPTNSdA53Kz0N9aw of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:04,026] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:03,995] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,054] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:04,054] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:04,088] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:02:04,088] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,090] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:02:04,098] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:02:04,192] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,294] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,396] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,493] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:04,498] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,505] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,515] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 4 (org.apache.kafka.raft.FollowerState)
[2025-12-25 12:02:04,528] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,537] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,563] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,570] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,600] INFO [MetadataLoader id=6] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,608] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,617] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,617] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,618] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=4) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-25 12:02:04,625] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-25 12:02:04,636] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-25 12:02:04,649] INFO Loaded 0 logs in 22ms (kafka.log.LogManager)
[2025-12-25 12:02:04,651] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-25 12:02:04,653] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-25 12:02:04,689] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-25 12:02:04,799] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-25 12:02:04,828] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:04,841] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-25 12:02:04,856] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:04,856] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-25 12:02:04,865] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:04,870] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:02:04,879] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-25 12:02:04,879] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:02:04,958] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,082] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:05,088] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:02:05,090] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:02:05,090] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:02:05,094] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:02:05,104] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:02:05,127] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:02:05,254] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:05,442] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:05,484] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:02:05,487] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:02:05,488] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:02:05,490] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-25 12:02:05,496] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:02:05,510] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:02:05,552] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:02:05,569] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:02:05,576] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:02:05,581] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:02:05,588] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-25 12:02:05,595] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:05,596] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:05,602] INFO Kafka startTimeMs: 1766664125595 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:05,612] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-25 12:02:21,305] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:02:21,336] INFO [Broker id=6] Creating new partition _schemas-0 with topic id 5ynfWlfWR8GWI1r7sCBFzA. (state.change.logger)
[2025-12-25 12:02:21,412] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:21,428] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-25 12:02:21,433] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-25 12:02:21,437] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:21,439] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:21,461] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:21,463] INFO [Broker id=6] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:02:21,581] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:21,600] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(5ynfWlfWR8GWI1r7sCBFzA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:21,611] INFO [Broker id=6] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:02:21,619] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:21,628] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:21,646] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:02:22,757] INFO [Broker id=6] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-25 12:02:22,763] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-42, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-1, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:22,768] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:22,800] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:22,819] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:22,821] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-25 12:02:22,828] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:22,872] INFO [Broker id=6] Leader __consumer_offsets-15 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:22,945] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:22,973] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:22,983] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:22,992] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-25 12:02:22,992] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:22,993] INFO [Broker id=6] Leader __consumer_offsets-48 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,005] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,015] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,018] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,020] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-25 12:02:23,025] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,026] INFO [Broker id=6] Leader __consumer_offsets-13 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,045] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,076] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,078] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,078] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-25 12:02:23,079] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,081] INFO [Broker id=6] Leader __consumer_offsets-46 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,099] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,112] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,116] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,116] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-25 12:02:23,117] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,117] INFO [Broker id=6] Leader __consumer_offsets-11 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,148] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,151] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,154] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,154] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-25 12:02:23,155] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,156] INFO [Broker id=6] Leader __consumer_offsets-42 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,166] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,171] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,175] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,175] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-25 12:02:23,175] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,176] INFO [Broker id=6] Leader __consumer_offsets-22 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,186] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,197] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,199] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,200] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-25 12:02:23,201] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,202] INFO [Broker id=6] Leader __consumer_offsets-18 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,206] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,210] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,212] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,213] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-25 12:02:23,214] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,215] INFO [Broker id=6] Leader __consumer_offsets-32 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,228] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,234] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,236] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,238] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-25 12:02:23,241] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,243] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,254] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,261] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,265] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,266] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-25 12:02:23,267] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,267] INFO [Broker id=6] Leader __consumer_offsets-26 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,278] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,289] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,295] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,297] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-25 12:02:23,300] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,301] INFO [Broker id=6] Leader __consumer_offsets-7 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,313] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,322] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,324] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,325] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-25 12:02:23,325] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,327] INFO [Broker id=6] Leader __consumer_offsets-40 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,337] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,344] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,347] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,348] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-25 12:02:23,348] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,348] INFO [Broker id=6] Leader __consumer_offsets-38 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,352] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,354] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,356] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,356] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-25 12:02:23,357] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,358] INFO [Broker id=6] Leader __consumer_offsets-3 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,362] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,367] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,368] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,368] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-25 12:02:23,369] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,370] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,376] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,384] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,387] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,389] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-25 12:02:23,390] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,391] INFO [Broker id=6] Leader __consumer_offsets-34 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,399] INFO [Broker id=6] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:02:23,402] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,407] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,409] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,410] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-25 12:02:23,410] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,411] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,412] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,420] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,422] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,425] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-25 12:02:23,426] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,429] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,429] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,443] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,493] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,494] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-25 12:02:23,495] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,499] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,500] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,506] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,508] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,508] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-25 12:02:23,508] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,509] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,510] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,515] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,518] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,518] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-25 12:02:23,518] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,519] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,519] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,527] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,530] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,532] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-25 12:02:23,532] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,533] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,536] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,540] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,541] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,542] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-25 12:02:23,542] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,543] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,544] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,562] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,564] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,564] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-25 12:02:23,564] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,565] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,567] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,576] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,577] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,579] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-25 12:02:23,580] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,581] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,581] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,602] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,607] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,613] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-25 12:02:23,613] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,615] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,618] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,629] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,634] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,634] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-25 12:02:23,637] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,637] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,640] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,643] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,644] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,645] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-25 12:02:23,645] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,646] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,646] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,651] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,652] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,652] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-25 12:02:23,653] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,653] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,653] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,657] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,661] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,662] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-25 12:02:23,663] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,664] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,664] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,668] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,671] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,672] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-25 12:02:23,672] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,673] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,675] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,680] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,681] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,682] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-25 12:02:23,682] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,683] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,684] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,686] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,687] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,687] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-25 12:02:23,688] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,688] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,689] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,694] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,697] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,697] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-25 12:02:23,697] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,698] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,698] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,701] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,702] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,702] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-25 12:02:23,703] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,703] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,704] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,707] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,709] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,709] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-25 12:02:23,710] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,710] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,710] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,713] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,714] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,715] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-25 12:02:23,715] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,716] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,716] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,724] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,726] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,730] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,730] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,730] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,731] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,739] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,741] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,743] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-25 12:02:23,744] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,745] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,745] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,752] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,753] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,754] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-25 12:02:23,754] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,754] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,755] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,762] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,765] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,766] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-25 12:02:23,766] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,767] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,767] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,770] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,772] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,772] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-25 12:02:23,773] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,774] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,774] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,781] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,782] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,783] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-25 12:02:23,783] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,784] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,784] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,786] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,788] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,788] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-25 12:02:23,789] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,789] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,789] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,793] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,795] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,795] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-25 12:02:23,795] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,796] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,796] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,803] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,804] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,804] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-25 12:02:23,804] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,805] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,805] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,809] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,810] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,810] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-25 12:02:23,811] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,811] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,811] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,818] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,819] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,820] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-25 12:02:23,820] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,821] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,821] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,825] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,826] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,826] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-25 12:02:23,827] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,827] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,828] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-5, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,828] INFO [Broker id=6] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-25 12:02:23,832] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,833] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-45 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,833] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,833] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,834] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,835] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,835] INFO [Broker id=6] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-25 12:02:23,835] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,838] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,839] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,840] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,841] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,842] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,843] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,844] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,845] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,848] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,850] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,851] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,851] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,853] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,854] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,857] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,858] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,859] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,860] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,861] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,862] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,862] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,864] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,864] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,867] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,869] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,870] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,874] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,881] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,888] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,890] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,891] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,891] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,891] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,891] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,892] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,892] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,892] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,893] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,893] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,894] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,895] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,897] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,897] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,901] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,903] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,904] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,904] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,904] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,903] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-15 in 11 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,905] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,906] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,906] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-48 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,907] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,908] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,908] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-13 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,908] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,909] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-46 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,909] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,909] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-11 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,909] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-42 in 15 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-22 in 13 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,911] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-18 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,911] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-32 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,912] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,912] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,912] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-7 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-40 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,914] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-38 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,917] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,917] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-3 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,917] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,918] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,918] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,918] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,919] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,919] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,919] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,919] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,918] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,920] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-34 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,922] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,922] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,923] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,923] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,923] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,923] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,924] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,923] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,924] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,925] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,924] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,925] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,925] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,927] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,927] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,927] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,928] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,928] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,928] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,927] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,929] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,929] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,930] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,931] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,932] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,930] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,933] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,933] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,933] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,933] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,934] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,934] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,934] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,935] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,935] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,935] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,936] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,936] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,937] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,937] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,938] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,939] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,939] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,940] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,940] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,941] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:02:23,941] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,942] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,942] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,943] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,943] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,944] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,945] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,948] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,948] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,948] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,949] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,949] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,950] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,950] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,951] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,953] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,954] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,982] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,983] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,984] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,985] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,986] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,988] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,990] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,992] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,003] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,005] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,006] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,007] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,008] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,009] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,013] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,017] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,018] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,022] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,024] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,027] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,029] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,030] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,030] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,031] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,031] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,032] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,032] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,033] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,033] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,034] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,035] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,036] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,037] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:24,038] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:12:04,179] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,610] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,635] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,692] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,703] INFO [RaftManager id=6] Completed transition to Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3911, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:46:35,780] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,992] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3911, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:46:36,087] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:36,134] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:36,136] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:36,187] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:34,234] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:57:34,699] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:57:34,743] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:57:34,750] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:40,351] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:57:40,699] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:57:40,715] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:41,174] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:41,200] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:41,319] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-25 12:57:41,327] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-25 12:57:41,341] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-12-25 12:57:41,361] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:41,785] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:41,796] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:41,813] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:41,961] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-25 12:57:42,091] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-25 12:57:42,132] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:42,137] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:42,326] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1978) from null (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:42,332] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-25 12:57:42,354] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-25 12:57:42,473] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:42,500] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-12-25 12:57:42,533] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:42,615] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:42,661] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:42,675] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:42,687] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:42,718] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:42,729] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:42,847] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:42,957] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:42,958] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:57:42,962] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:57:42,973] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:43,064] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,073] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-25 12:57:43,084] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@612573758 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:43,101] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,116] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,170] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,222] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,234] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,248] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,257] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,266] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,267] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,279] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,388] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,389] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,392] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,407] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,409] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,415] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,420] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,498] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,508] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,518] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,529] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,536] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,606] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,707] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,765] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,781] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,794] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,801] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,812] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,816] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:43,812] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,938] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:43,959] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:57:44,043] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,146] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-25 12:57:44,151] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,154] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:57:44,188] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-25 12:57:44,262] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,279] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:44,285] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,298] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,318] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,327] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,355] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,368] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:44,375] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,504] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,545] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:44,608] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:44,632] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,934] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:44,942] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:44,936] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:44,977] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:44,992] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:45,073] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,153] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:45,169] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:45,174] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,198] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-25 12:57:45,212] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:45,253] INFO [BrokerLifecycleManager id=6] Incarnation U9TMo7VQTES8dF2MUiZfKQ of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:45,282] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,282] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:45,383] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,495] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,528] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:45,529] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:57:45,555] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:57:45,565] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:57:45,565] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:57:45,565] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,672] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,718] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1978) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:45,779] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,902] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,013] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,115] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,191] INFO [RaftManager id=6] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:46,220] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,321] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,373] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:46,410] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,412] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,427] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,452] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,452] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,529] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,576] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-25 12:57:46,581] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,633] INFO [MetadataLoader id=6] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,676] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,676] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,677] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,683] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=2) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-25 12:57:46,687] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 5 (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:46,688] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-25 12:57:46,707] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-25 12:57:46,815] INFO Loaded 0 logs in 100ms (kafka.log.LogManager)
[2025-12-25 12:57:46,871] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-25 12:57:46,968] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-25 12:57:46,975] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-25 12:57:47,630] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-25 12:57:47,662] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-25 12:57:47,662] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-25 12:57:47,663] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:47,701] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:47,708] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:57:47,719] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:57:47,719] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-25 12:57:47,846] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:48,011] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,026] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:57:48,032] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:57:48,034] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:57:48,056] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:57:48,064] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,119] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:48,157] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:57:48,263] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,265] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:57:48,277] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:57:48,278] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:57:48,283] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-12-25 12:57:48,307] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:57:48,323] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:57:48,373] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:57:48,377] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:57:48,377] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:57:48,377] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:57:48,379] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-25 12:57:48,383] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:48,387] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:48,398] INFO Kafka startTimeMs: 1766667468382 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:48,420] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-25 12:57:55,755] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:57:55,771] INFO [Broker id=6] Creating new partition _schemas-0 with topic id wkzSqzyiRv-1bUOz43_kpQ. (state.change.logger)
[2025-12-25 12:57:55,823] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:55,832] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-25 12:57:55,836] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-25 12:57:55,840] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:55,844] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:55,845] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:55,846] INFO [Broker id=6] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:57:55,863] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:55,866] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(wkzSqzyiRv-1bUOz43_kpQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:55,867] INFO [Broker id=6] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:57:55,868] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:55,870] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:55,884] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:57:57,003] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-25 12:57:57,224] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-12-25 12:57:57,229] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-20, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:57,233] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,268] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,270] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,272] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-25 12:57:57,273] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,275] INFO [Broker id=6] Leader __consumer_offsets-13 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,322] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,338] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,344] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,348] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-25 12:57:57,353] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,360] INFO [Broker id=6] Leader __consumer_offsets-46 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,371] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,378] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,380] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,380] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-25 12:57:57,381] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,381] INFO [Broker id=6] Leader __consumer_offsets-11 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,391] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,399] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,401] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,401] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-25 12:57:57,402] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,403] INFO [Broker id=6] Leader __consumer_offsets-44 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,407] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,413] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,415] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,415] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-25 12:57:57,417] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,417] INFO [Broker id=6] Leader __consumer_offsets-24 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,427] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,443] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,444] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,445] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-25 12:57:57,445] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,446] INFO [Broker id=6] Leader __consumer_offsets-21 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,450] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,494] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,497] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,502] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-25 12:57:57,503] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,503] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,533] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,547] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,550] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,550] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-25 12:57:57,550] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,551] INFO [Broker id=6] Leader __consumer_offsets-17 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,564] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,576] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,582] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,583] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-25 12:57:57,584] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,585] INFO [Broker id=6] Leader __consumer_offsets-32 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,602] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,627] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,632] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,635] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-25 12:57:57,642] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,644] INFO [Broker id=6] Leader __consumer_offsets-27 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,661] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,674] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,675] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,676] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-25 12:57:57,677] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,681] INFO [Broker id=6] Leader __consumer_offsets-7 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,699] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,704] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,706] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,707] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-25 12:57:57,707] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,708] INFO [Broker id=6] Leader __consumer_offsets-40 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,720] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,748] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,749] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,752] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-25 12:57:57,759] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,762] INFO [Broker id=6] Leader __consumer_offsets-37 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,784] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,793] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,803] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,803] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-25 12:57:57,803] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,804] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,821] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,845] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,851] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,851] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-25 12:57:57,854] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,854] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,872] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,905] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,910] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,919] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-25 12:57:58,007] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,012] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:58,114] INFO [Broker id=6] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:57:58,114] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,129] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,130] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,131] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-25 12:57:58,131] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,132] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,132] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,144] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,146] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,146] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-25 12:57:58,147] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,147] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,147] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,151] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,152] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,153] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-25 12:57:58,153] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,153] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,153] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,165] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,177] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,178] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-25 12:57:58,178] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,178] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,179] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,197] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,204] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,208] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-25 12:57:58,208] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,208] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,209] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,214] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,221] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,232] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-25 12:57:58,232] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,232] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,232] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,265] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,269] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,275] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-25 12:57:58,275] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,276] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,277] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,296] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,305] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,306] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-25 12:57:58,308] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,309] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,309] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,317] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,329] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,338] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-25 12:57:58,339] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,340] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,345] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,370] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,371] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,371] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-25 12:57:58,371] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,372] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,375] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,413] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,424] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,426] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-25 12:57:58,426] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,426] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,427] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,432] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,433] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,436] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-25 12:57:58,436] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,436] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,438] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,451] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,453] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,454] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-25 12:57:58,454] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,454] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,455] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,461] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,468] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,469] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-25 12:57:58,470] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,470] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,470] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,477] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,482] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,483] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-25 12:57:58,483] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,485] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,487] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,574] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,588] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,590] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-25 12:57:58,595] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,600] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,601] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,616] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,617] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,618] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-25 12:57:58,618] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,620] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,620] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,625] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,629] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,629] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-25 12:57:58,630] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,630] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,630] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,636] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,637] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,637] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-25 12:57:58,638] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,638] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,639] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,649] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,650] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,651] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-25 12:57:58,651] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,652] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,653] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,714] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,715] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,724] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-25 12:57:58,729] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,742] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,751] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,761] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,766] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,767] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-25 12:57:58,768] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,771] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,777] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,787] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,790] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,791] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-25 12:57:58,792] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,792] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,792] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,821] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,828] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,831] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-25 12:57:58,835] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,841] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,842] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,850] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,851] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,851] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-25 12:57:58,852] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,853] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,853] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,868] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,875] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,883] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-25 12:57:58,888] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,889] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,890] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,895] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,901] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,905] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,906] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,911] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,913] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,926] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,930] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,931] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-25 12:57:58,934] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,935] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,936] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,962] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,973] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,975] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-25 12:57:58,976] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,978] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,979] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,985] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,991] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,992] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-25 12:57:58,992] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,992] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,993] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:59,007] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:59,014] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:59,014] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-25 12:57:59,015] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:59,016] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:59,016] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:59,031] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:59,032] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:59,032] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-25 12:57:59,033] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:59,033] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:59,034] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:59,040] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:59,041] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:59,044] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-25 12:57:59,045] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:59,046] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:59,046] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:59,058] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:59,061] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:59,063] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-25 12:57:59,064] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:59,064] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:59,066] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-6, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:59,066] INFO [Broker id=6] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-25 12:57:59,105] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-48 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:59,108] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:59,109] INFO [Broker id=6] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-25 12:57:59,121] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,122] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,125] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,125] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,126] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,126] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,128] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,127] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,128] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,130] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,131] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,130] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,132] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,135] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,138] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,142] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,133] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,148] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,146] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,151] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,145] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-13 in 16 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,154] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-46 in 19 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,150] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,156] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-11 in 14 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,156] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,163] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-44 in 12 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,170] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-24 in 3 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,159] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,168] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,174] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,177] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,176] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,179] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,181] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-21 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,181] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,185] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,187] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,188] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,183] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,190] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,193] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,188] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,194] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,200] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,193] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,201] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,201] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,202] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,198] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-17 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,203] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,202] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,203] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,203] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-32 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,205] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-27 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,203] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,212] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,211] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-7 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,204] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,217] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,215] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-40 in 3 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,214] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,219] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,218] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,221] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,220] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,224] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,219] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,232] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,226] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,238] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,239] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,224] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,242] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,239] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,251] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,247] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,253] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,255] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 15 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,255] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,260] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,261] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,258] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,262] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,263] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,262] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,265] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,262] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,271] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,263] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,271] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,272] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,272] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,273] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,272] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,273] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,277] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,277] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,278] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,274] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,279] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,280] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,280] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,283] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,283] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,286] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,290] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,293] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,295] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,297] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,298] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,298] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,298] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,299] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,299] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,301] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,301] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,301] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,301] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,302] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,302] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,302] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,302] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,299] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,302] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,311] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,311] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,311] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,313] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,310] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,314] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,314] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,314] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,314] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,315] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,315] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,315] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,315] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,315] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,315] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,316] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,316] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,317] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,317] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,318] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,319] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,320] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,320] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,320] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,320] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,318] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,321] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,322] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,322] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,322] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,323] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,324] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,325] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,325] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,326] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,326] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,329] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,329] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,329] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,330] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,330] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,330] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,330] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,330] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,322] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,331] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,332] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,331] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:57:59,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,343] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,344] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,346] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,347] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,347] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,348] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,348] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,349] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,350] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,369] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,369] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,370] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,370] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,371] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,371] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,371] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,372] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,372] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,374] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,376] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,376] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,377] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,378] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,378] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,379] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,380] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,381] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,382] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,383] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,386] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,388] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,389] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,389] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,389] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,390] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,390] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,390] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,391] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,391] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,391] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,391] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,392] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,392] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 13:11:09,715] INFO [RaftManager id=6] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=345, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:11:09,878] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=345, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:11:11,138] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:11:11,165] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:17:49,913] INFO [RaftManager id=6] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=643, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:17:50,016] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=643, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:17:50,162] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:17:50,164] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:23:12,263] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:27:16,396] INFO [RaftManager id=6] Completed transition to Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,460] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,485] INFO [RaftManager id=6] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,500] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,547] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 1301 (kafka.log.UnifiedLog)
[2025-12-25 13:27:16,571] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 1301 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 13:27:16,572] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 1301 (kafka.log.UnifiedLog$)
[2025-12-25 13:27:16,579] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 1301 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 13:27:16,579] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 7ms for segment recovery from offset 1301 (kafka.log.UnifiedLog$)
[2025-12-25 13:27:16,580] INFO [RaftManager id=6] Truncated to offset 1301 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 13:27:17,801] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:27:17,803] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:29:19,676] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:29:19,850] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1607 due to node 1 being disconnected (elapsed time since creation: 2230ms, elapsed time since send: 2229ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:29:20,223] INFO [RaftManager id=6] Completed transition to Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1536, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:20,321] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:29:20,476] INFO [RaftManager id=6] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:20,612] INFO [RaftManager id=6] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:20,995] INFO [RaftManager id=6] Completed transition to Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:21,146] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1536, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:21,245] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:39:21,285] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:39:21,301] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:49:23,690] INFO [RaftManager id=6] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,752] INFO [RaftManager id=6] Completed transition to Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,883] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:24,069] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:49:24,082] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:59:23,486] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:59:23,987] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:19:04,186] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000007224-0000000012 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 14:19:04,700] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000007224-0000000012 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 14:31:37,582] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:39,131] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 9283 due to node 3 being disconnected (elapsed time since creation: 2181ms, elapsed time since send: 2181ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,400] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,408] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,457] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,515] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,535] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,574] INFO [RaftManager id=6] Completed transition to Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=8719, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:40,729] INFO [RaftManager id=6] Completed transition to Unattached(epoch=14, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:41,117] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=14, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=8719, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=14, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:41,310] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 14:31:41,423] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:41,443] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 8720 (kafka.log.UnifiedLog)
[2025-12-25 14:31:41,568] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 8720 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,573] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 8720 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,585] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=1301, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000001301.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 14:31:41,765] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 8720 with 0 producer ids in 16 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 14:31:41,772] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 99ms for snapshot load and 97ms for segment recovery from offset 8720 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,782] INFO [RaftManager id=6] Truncated to offset 8720 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 14:41:40,227] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:41:40,812] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:12:20,831] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2026-01-16 08:12:24,390] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2026-01-16 08:12:24,625] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2026-01-16 08:12:24,632] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:04,803] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2026-01-16 08:13:05,849] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2026-01-16 08:13:05,875] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:06,538] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:06,561] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:06,599] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2026-01-16 08:13:06,621] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2026-01-16 08:13:06,627] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2026-01-16 08:13:06,631] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:07,419] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:13:07,446] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2026-01-16 08:13:07,446] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2026-01-16 08:13:07,743] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2026-01-16 08:13:07,927] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2026-01-16 08:13:07,963] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:13:07,981] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:13:08,355] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1051) from null (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:13:08,389] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2026-01-16 08:13:08,402] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2026-01-16 08:13:08,649] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:08,700] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2026-01-16 08:13:08,780] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:08,835] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:08,892] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:08,996] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:09,113] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2026-01-16 08:13:09,121] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:09,161] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2026-01-16 08:13:09,226] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:09,271] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2026-01-16 08:13:09,272] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2026-01-16 08:13:09,340] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:09,453] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:09,567] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:09,675] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:09,684] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2026-01-16 08:13:09,687] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2026-01-16 08:13:09,728] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:09,755] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2026-01-16 08:13:09,783] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:09,796] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1663969303 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:13:09,854] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:09,865] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:09,896] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:10,008] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:10,129] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:10,203] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,233] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,242] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:10,284] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,286] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,304] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,313] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,346] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,359] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,348] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:10,420] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,442] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,466] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,467] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,466] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:10,495] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,554] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,569] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:10,587] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,646] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,712] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,732] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,744] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:10,803] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,813] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,917] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:10,930] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:10,957] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:11,058] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:11,254] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:11,360] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:11,387] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:11,393] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:12,313] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,346] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:12,375] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:12,403] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:12,416] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:12,471] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,498] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:12,513] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:12,602] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,723] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,841] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,909] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:12,910] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:12,954] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,004] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,023] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,069] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,199] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,333] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,442] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,504] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,527] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,549] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,562] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,559] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,679] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,782] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,884] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,968] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:14,010] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,988] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:14,059] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:14,077] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:14,187] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:14,291] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:14,399] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:14,513] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:14,568] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:14,656] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:14,680] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:14,710] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:14,706] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2026-01-16 08:13:14,751] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:14,807] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:14,915] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,040] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,146] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,162] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2026-01-16 08:13:15,189] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2026-01-16 08:13:15,234] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:15,236] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:15,255] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:15,281] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:15,282] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,370] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2026-01-16 08:13:15,394] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,465] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:15,514] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,571] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:15,625] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,728] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,858] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:16,753] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:16,877] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,008] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,122] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,186] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:17,244] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:17,248] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,369] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,389] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:17,434] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:17,473] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,500] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:17,595] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,700] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,802] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:17,813] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,816] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:17,950] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,053] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,084] INFO [RaftManager id=6] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1051) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:13:18,099] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2026-01-16 08:13:18,122] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:18,160] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,171] INFO [BrokerLifecycleManager id=6] Incarnation ZQCVEQ68Qt-xAZQAypi_kw of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:13:18,270] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,324] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:18,412] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,526] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,642] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,761] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,868] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,878] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2026-01-16 08:13:18,887] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,895] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2026-01-16 08:13:18,897] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2026-01-16 08:13:18,994] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,110] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,225] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,343] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,344] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:13:19,394] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:19,400] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:19,409] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:19,432] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:19,460] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,571] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,684] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,791] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,853] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:19,927] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:19,930] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,048] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,157] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,274] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,385] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,442] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 3 (org.apache.kafka.raft.FollowerState)
[2026-01-16 08:13:20,526] INFO [MetadataLoader id=6] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,611] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,637] INFO [MetadataLoader id=6] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,757] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 5 (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:13:20,780] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,794] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,812] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,834] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=3) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2026-01-16 08:13:20,922] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2026-01-16 08:13:21,233] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2026-01-16 08:13:21,487] INFO Loaded 0 logs in 525ms (kafka.log.LogManager)
[2026-01-16 08:13:21,504] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2026-01-16 08:13:21,519] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2026-01-16 08:13:21,606] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2026-01-16 08:13:26,283] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2026-01-16 08:13:26,524] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2026-01-16 08:13:26,626] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2026-01-16 08:13:26,696] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:13:26,761] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:13:26,902] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2026-01-16 08:13:27,374] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2026-01-16 08:13:27,402] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2026-01-16 08:13:28,272] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:29,004] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:13:29,056] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2026-01-16 08:13:29,069] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2026-01-16 08:13:29,856] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2026-01-16 08:13:29,887] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:13:30,072] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2026-01-16 08:13:30,230] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:30,373] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2026-01-16 08:13:30,633] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:13:30,637] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2026-01-16 08:13:30,716] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2026-01-16 08:13:30,720] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2026-01-16 08:13:30,728] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2026-01-16 08:13:30,786] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2026-01-16 08:13:30,840] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2026-01-16 08:13:30,935] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2026-01-16 08:13:30,960] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2026-01-16 08:13:30,967] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2026-01-16 08:13:30,970] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2026-01-16 08:13:30,979] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2026-01-16 08:13:30,993] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2026-01-16 08:13:31,002] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2026-01-16 08:13:31,003] INFO Kafka startTimeMs: 1768551210993 (org.apache.kafka.common.utils.AppInfoParser)
[2026-01-16 08:13:31,062] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2026-01-16 08:14:38,404] INFO [RaftManager id=6] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=157, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:14:38,637] INFO [RaftManager id=6] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:14:38,856] INFO [RaftManager id=6] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:14:39,534] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:14:39,892] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=157, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:14:39,961] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:14:47,372] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:14:47,780] INFO [Broker id=6] Creating new partition _schemas-0 with topic id UjN16IrSRlipZQeCwkGrdQ. (state.change.logger)
[2026-01-16 08:14:52,308] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:14:52,919] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2026-01-16 08:14:54,421] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2026-01-16 08:14:54,681] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:14:54,789] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:14:54,852] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:14:54,889] INFO [Broker id=6] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2026-01-16 08:14:56,779] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:14:58,065] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(UjN16IrSRlipZQeCwkGrdQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:14:58,082] INFO [Broker id=6] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2026-01-16 08:14:58,213] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:14:58,325] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:14:59,802] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2026-01-16 08:15:05,820] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2026-01-16 08:15:06,020] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-9, __consumer_offsets-21, __consumer_offsets-20, __consumer_offsets-17, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-3, __consumer_offsets-35, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:15:06,063] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:06,525] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:06,545] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:06,553] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2026-01-16 08:15:06,557] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:06,584] INFO [Broker id=6] Leader __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:06,736] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:06,794] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:06,804] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:06,806] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2026-01-16 08:15:06,809] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:06,810] INFO [Broker id=6] Leader __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:06,849] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:06,919] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:06,942] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:06,942] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2026-01-16 08:15:06,945] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:06,947] INFO [Broker id=6] Leader __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:06,988] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:07,488] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:07,662] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:07,709] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2026-01-16 08:15:07,710] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:07,718] INFO [Broker id=6] Leader __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:07,797] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:07,875] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:07,885] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:07,886] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2026-01-16 08:15:07,886] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:07,888] INFO [Broker id=6] Leader __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:07,929] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:07,977] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,006] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,008] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2026-01-16 08:15:08,009] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,013] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,033] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,046] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,050] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,053] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2026-01-16 08:15:08,054] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,057] INFO [Broker id=6] Leader __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,078] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,092] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,102] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,102] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2026-01-16 08:15:08,103] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,105] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,124] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,212] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,233] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,246] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2026-01-16 08:15:08,246] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,249] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,283] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,326] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,340] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,342] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2026-01-16 08:15:08,360] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,363] INFO [Broker id=6] Leader __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,388] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,520] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,527] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,544] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2026-01-16 08:15:08,557] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,560] INFO [Broker id=6] Leader __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,590] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,632] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,640] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,650] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2026-01-16 08:15:08,651] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,654] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,706] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,762] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,772] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,773] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2026-01-16 08:15:08,775] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,776] INFO [Broker id=6] Leader __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,821] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:09,300] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:09,490] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:09,514] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2026-01-16 08:15:09,533] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:09,537] INFO [Broker id=6] Leader __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:09,639] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:09,827] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:09,855] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:09,871] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2026-01-16 08:15:09,891] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:09,893] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:09,945] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,082] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,090] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,091] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2026-01-16 08:15:10,091] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,091] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:10,111] INFO [Broker id=6] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:15:10,112] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,151] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,174] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,188] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2026-01-16 08:15:10,190] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,191] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,193] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,329] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,345] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,348] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2026-01-16 08:15:10,357] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,360] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,362] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,395] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,418] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,448] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2026-01-16 08:15:10,461] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,466] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,472] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,681] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,689] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,713] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2026-01-16 08:15:10,718] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,721] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,722] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,761] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,764] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,766] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2026-01-16 08:15:10,767] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,769] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,776] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,792] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,797] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,798] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2026-01-16 08:15:10,798] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,799] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,799] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,901] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,934] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,991] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2026-01-16 08:15:10,998] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,003] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,030] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,059] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,063] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,066] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2026-01-16 08:15:11,070] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,071] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,077] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,127] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,132] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,144] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2026-01-16 08:15:11,146] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,147] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,148] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,187] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,194] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,198] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2026-01-16 08:15:11,199] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,200] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,202] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,255] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,263] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,275] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2026-01-16 08:15:11,276] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,279] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,282] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,306] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,313] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,324] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2026-01-16 08:15:11,330] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,332] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,333] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,392] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,402] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,415] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2026-01-16 08:15:11,423] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,429] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,448] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,567] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,579] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,579] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2026-01-16 08:15:11,580] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,584] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,596] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,673] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,693] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,699] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2026-01-16 08:15:11,709] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,714] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,718] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,797] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,808] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,813] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2026-01-16 08:15:11,815] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,823] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,829] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,956] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:12,004] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:12,017] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2026-01-16 08:15:12,030] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:12,034] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:12,049] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:12,179] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:12,199] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:12,201] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2026-01-16 08:15:12,202] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:12,206] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:12,209] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:12,594] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:12,668] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:12,674] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2026-01-16 08:15:12,690] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:12,691] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:12,692] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:12,724] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:12,739] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:12,741] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2026-01-16 08:15:12,742] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:12,745] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:12,751] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:12,861] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:12,920] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:12,927] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2026-01-16 08:15:12,927] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:12,932] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:12,944] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,035] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,047] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,055] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2026-01-16 08:15:13,068] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,070] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,079] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,128] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,136] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,141] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2026-01-16 08:15:13,142] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,144] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,146] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,197] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,217] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,224] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2026-01-16 08:15:13,225] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,227] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,230] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,286] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,293] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,295] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2026-01-16 08:15:13,301] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,303] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,304] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,402] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,417] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,427] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2026-01-16 08:15:13,431] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,433] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,440] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,614] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,654] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,659] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,666] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,667] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,676] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,793] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,864] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,887] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2026-01-16 08:15:13,888] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,894] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,909] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:14,019] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:14,305] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:14,308] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2026-01-16 08:15:14,310] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:14,312] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:14,313] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:14,341] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:14,349] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:14,352] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2026-01-16 08:15:14,356] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:14,361] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:14,364] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:14,395] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:14,427] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:14,428] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2026-01-16 08:15:14,437] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:14,449] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:14,452] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:14,486] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:14,496] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:14,502] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2026-01-16 08:15:14,505] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:14,507] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:14,509] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:14,576] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:14,606] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:14,625] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2026-01-16 08:15:14,629] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:14,633] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:14,634] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:14,719] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:14,752] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:14,757] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2026-01-16 08:15:14,763] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:14,764] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:14,783] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-30, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-36, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-14, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-6, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:15:14,792] INFO [Broker id=6] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2026-01-16 08:15:15,020] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,046] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,049] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,057] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,064] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,070] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,074] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,064] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-48 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:15:15,075] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,081] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,083] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,089] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,093] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,090] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:15:15,100] INFO [Broker id=6] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2026-01-16 08:15:15,097] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,108] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,112] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,113] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,113] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,119] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,119] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,120] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,123] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,127] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,129] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,143] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,146] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,147] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,149] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,150] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,152] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,160] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,161] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,161] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,166] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,171] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,174] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,255] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,287] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,298] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,303] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,305] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,308] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,314] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,315] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,316] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,323] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,325] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,326] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,326] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,331] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,333] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,335] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,340] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,343] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,348] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,348] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,348] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,348] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,348] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,349] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,349] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,349] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,349] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,349] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,350] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,352] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,352] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,355] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,372] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-45 in 74 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,372] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-43 in 67 milliseconds for epoch 0, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,375] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-12 in 61 milliseconds for epoch 0, of which 59 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,379] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-9 in 63 milliseconds for epoch 0, of which 60 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,392] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-21 in 67 milliseconds for epoch 0, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,396] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,410] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 84 milliseconds for epoch 0, of which 75 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,413] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-17 in 80 milliseconds for epoch 0, of which 79 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,417] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 77 milliseconds for epoch 0, of which 77 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,425] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 77 milliseconds for epoch 0, of which 77 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,438] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-26 in 90 milliseconds for epoch 0, of which 90 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,443] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-39 in 95 milliseconds for epoch 0, of which 94 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,424] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,455] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 106 milliseconds for epoch 0, of which 101 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,427] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,463] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,472] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,476] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,476] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,477] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,458] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,483] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,466] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-37 in 117 milliseconds for epoch 0, of which 116 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,485] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-3 in 135 milliseconds for epoch 0, of which 135 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,483] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,501] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,483] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,502] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,493] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 141 milliseconds for epoch 0, of which 140 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,505] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 147 milliseconds for epoch 0, of which 146 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,504] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,513] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,503] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,521] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,520] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,529] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,527] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,532] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,536] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,544] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,545] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,545] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,546] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,547] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,743] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,530] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,753] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,752] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,756] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,535] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,761] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,759] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,767] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,756] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,769] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,769] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,771] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,765] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,775] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,776] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,772] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,780] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,780] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,786] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,776] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,787] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,786] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,790] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,790] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,786] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,791] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,791] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,792] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,792] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,790] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,795] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,795] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,799] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,800] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,800] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,796] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,801] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,800] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,802] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,800] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,804] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,803] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,808] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,801] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,809] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,808] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,811] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,808] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,814] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,812] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,815] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,810] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,821] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,818] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,827] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,816] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,828] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,828] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,828] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,828] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,828] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,834] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,828] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,838] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,838] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,834] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,840] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,840] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,845] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,838] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,853] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,849] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,855] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,842] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,856] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,855] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,856] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,853] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,860] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,861] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,859] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,865] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,857] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,867] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,866] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,870] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,865] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,871] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,871] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,870] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,875] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,874] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,880] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,874] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,883] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,879] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,886] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,883] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,890] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,886] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,891] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,891] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,892] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,890] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,893] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,899] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,894] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,908] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2026-01-16 08:15:53,425] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:15:53,534] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:15:54,575] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:15:54,805] INFO [RaftManager id=6] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:15:54,990] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:15:55,012] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:15:55,019] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:15:55,077] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:15:55,163] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:15:55,170] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:15:55,221] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:19:00,774] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:00,830] INFO [RaftManager id=6] Completed transition to Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=714, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:01,259] INFO [RaftManager id=6] Completed transition to Unattached(epoch=14, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:01,344] INFO [RaftManager id=6] Completed transition to Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=14, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:01,878] INFO [RaftManager id=6] Completed transition to Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:02,033] INFO [RaftManager id=6] Completed transition to Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:02,543] INFO [RaftManager id=6] Completed transition to Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:02,638] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=714, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:02,709] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:19:03,245] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:03,254] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:19:03,306] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:19:39,594] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:40,454] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 835 due to node 1 being disconnected (elapsed time since creation: 2065ms, elapsed time since send: 2063ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:42,894] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:43,315] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 211 due to node 1 being disconnected (elapsed time since creation: 5016ms, elapsed time since send: 5016ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:43,359] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:19:46,961] INFO [RaftManager id=6] Completed transition to Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:47,233] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:19:47,906] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:48,355] INFO [RaftManager id=6] Completed transition to Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:52,489] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:19:52,641] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:54,249] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 845 due to node 3 being disconnected (elapsed time since creation: 2844ms, elapsed time since send: 2844ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:57,429] INFO [RaftManager id=6] Completed transition to Unattached(epoch=28, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:59,170] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:20:06,768] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:07,040] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 852 due to node 3 being disconnected (elapsed time since creation: 8161ms, elapsed time since send: 8161ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:10,658] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:11,259] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 854 due to node 3 being disconnected (elapsed time since creation: 2579ms, elapsed time since send: 2009ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:13,314] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:20:18,054] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:19,011] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 856 due to node 1 being disconnected (elapsed time since creation: 6690ms, elapsed time since send: 5015ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:19,054] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:20:19,416] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:19,794] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 857 due to node 2 being disconnected (elapsed time since creation: 3711ms, elapsed time since send: 3244ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:25,693] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:20:38,307] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:38,856] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 865 due to node 3 being disconnected (elapsed time since creation: 11909ms, elapsed time since send: 11450ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:40,827] INFO [RaftManager id=6] Completed transition to Unattached(epoch=46, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=28, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:20:42,698] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:20:55,052] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:20:59,632] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:59,918] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 872 due to node 1 being disconnected (elapsed time since creation: 5899ms, elapsed time since send: 5432ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:59,920] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:59,952] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 871 due to node 3 being disconnected (elapsed time since creation: 8286ms, elapsed time since send: 5432ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:08,661] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:08,956] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 877 due to node 3 being disconnected (elapsed time since creation: 5096ms, elapsed time since send: 4266ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:12,180] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:12,422] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 879 due to node 2 being disconnected (elapsed time since creation: 5890ms, elapsed time since send: 2103ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:13,169] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:21:22,778] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:23,536] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 886 due to node 3 being disconnected (elapsed time since creation: 3018ms, elapsed time since send: 2405ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:27,404] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:21:30,279] INFO [RaftManager id=6] Completed transition to Unattached(epoch=63, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=46, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:21:41,803] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:42,565] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 893 due to node 1 being disconnected (elapsed time since creation: 4372ms, elapsed time since send: 4303ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:46,065] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:21:51,987] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:52,368] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 898 due to node 1 being disconnected (elapsed time since creation: 3797ms, elapsed time since send: 3726ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:52,402] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:52,421] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 897 due to node 2 being disconnected (elapsed time since creation: 5802ms, elapsed time since send: 4374ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:56,514] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:56,719] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 899 due to node 1 being disconnected (elapsed time since creation: 4573ms, elapsed time since send: 2765ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:58,846] INFO [RaftManager id=6] Completed transition to Unattached(epoch=78, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=63, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:01,487] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:22:06,224] INFO [RaftManager id=6] Completed transition to Unattached(epoch=81, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=78, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:07,950] INFO [RaftManager id=6] Completed transition to Unattached(epoch=82, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=81, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:11,290] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:11,633] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 909 due to node 1 being disconnected (elapsed time since creation: 2114ms, elapsed time since send: 2085ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:15,292] INFO [RaftManager id=6] Completed transition to Unattached(epoch=86, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=82, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:16,138] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:22:17,202] INFO [RaftManager id=6] Completed transition to Unattached(epoch=87, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=86, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:17,671] INFO [RaftManager id=6] Completed transition to Unattached(epoch=88, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=87, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:23,513] INFO [RaftManager id=6] Completed transition to Unattached(epoch=89, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=88, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:28,877] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:29,632] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 918 due to node 3 being disconnected (elapsed time since creation: 3655ms, elapsed time since send: 3276ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:30,279] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:22:36,934] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:37,423] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 920 due to node 3 being disconnected (elapsed time since creation: 4746ms, elapsed time since send: 4746ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:41,851] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:42,070] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 925 due to node 2 being disconnected (elapsed time since creation: 2139ms, elapsed time since send: 2132ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:42,087] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:42,096] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 924 due to node 3 being disconnected (elapsed time since creation: 4163ms, elapsed time since send: 3085ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:45,675] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:22:52,814] INFO [RaftManager id=6] Completed transition to Unattached(epoch=100, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=89, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:54,401] INFO [RaftManager id=6] Completed transition to Unattached(epoch=102, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=100, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:56,756] INFO [RaftManager id=6] Completed transition to Unattached(epoch=103, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=102, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:00,285] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:23:00,557] INFO [RaftManager id=6] Completed transition to Unattached(epoch=104, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=103, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:03,121] INFO [RaftManager id=6] Completed transition to Unattached(epoch=106, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=104, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:06,248] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:07,132] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 937 due to node 1 being disconnected (elapsed time since creation: 2152ms, elapsed time since send: 2033ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:09,071] INFO [RaftManager id=6] Completed transition to Unattached(epoch=110, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=106, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:10,252] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=110, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=110, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:10,352] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:23:15,247] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:23:15,396] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:23:18,374] INFO [RaftManager id=6] Completed transition to Unattached(epoch=112, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=110, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:20,937] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:21,323] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 942 due to node 2 being disconnected (elapsed time since creation: 2278ms, elapsed time since send: 2089ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:27,100] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:27,415] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:28,385] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 944 due to node 2 being disconnected (elapsed time since creation: 2332ms, elapsed time since send: 2332ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:31,402] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:23:32,498] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:23:32,579] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=121, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=112, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:39,446] INFO [RaftManager id=6] Completed transition to Unattached(epoch=123, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=121, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:43,817] INFO [RaftManager id=6] Completed transition to Unattached(epoch=126, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=123, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:44,718] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:47,674] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:23:48,198] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=130, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=126, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:48,281] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:23:56,007] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:02,361] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 958 due to node 3 being disconnected (elapsed time since creation: 4373ms, elapsed time since send: 3696ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:04,340] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:04,867] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:24:08,317] INFO [RaftManager id=6] Completed transition to Unattached(epoch=134, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=130, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:24:13,297] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:14,752] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 964 due to node 2 being disconnected (elapsed time since creation: 3179ms, elapsed time since send: 2950ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:15,085] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:17,463] INFO [RaftManager id=6] Completed transition to Unattached(epoch=138, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=134, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:24:19,335] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:24:19,442] INFO [RaftManager id=6] Completed transition to Unattached(epoch=139, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=138, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:24:19,587] INFO [RaftManager id=6] Completed transition to Unattached(epoch=141, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=139, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:24:20,764] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=141, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=141, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:24:20,855] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:29,065] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:30,018] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 978 due to node 1 being disconnected (elapsed time since creation: 4123ms, elapsed time since send: 4070ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:32,379] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:32,925] INFO [RaftManager id=6] Completed transition to Unattached(epoch=145, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=141, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:24:33,752] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:24:36,885] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:36,922] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=147, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=145, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:24:50,094] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:51,854] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:56,862] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:57,249] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:24:57,361] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:57,557] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 991 due to node 1 being disconnected (elapsed time since creation: 5433ms, elapsed time since send: 5166ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:58,429] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:59,180] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:59,524] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:59,542] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:59,596] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:59,814] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:59,835] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:59,885] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:01,628] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:01,749] INFO [RaftManager id=6] Completed transition to Unattached(epoch=156, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=147, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:02,483] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:25:08,620] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:25:09,567] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:09,568] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=158, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=156, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:09,991] INFO [RaftManager id=6] Completed transition to Unattached(epoch=161, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=158, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:10,363] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:10,710] INFO [RaftManager id=6] Completed transition to Unattached(epoch=162, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=161, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:11,022] INFO [RaftManager id=6] Completed transition to Unattached(epoch=163, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=162, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:13,724] INFO [RaftManager id=6] Completed transition to Unattached(epoch=165, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=163, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:14,393] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:25:17,027] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=166, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=165, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:17,063] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:19,199] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:23,917] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1004 due to node 2 being disconnected (elapsed time since creation: 2041ms, elapsed time since send: 2003ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:24,013] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:24,439] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight API_VERSIONS request with correlation id 241 due to node 2 being disconnected (elapsed time since creation: 6654ms, elapsed time since send: 6654ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:24,760] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:24,929] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:25:25,128] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=170, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=166, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:30,364] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:30,698] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:31,275] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:31,733] INFO [RaftManager id=6] Completed transition to Unattached(epoch=174, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=170, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:33,576] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:25:34,850] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=174, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=174, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:36,101] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:39,620] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:39,832] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:39,870] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:40,243] INFO [RaftManager id=6] Completed transition to Unattached(epoch=176, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=174, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:40,433] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:42,042] INFO [RaftManager id=6] Completed transition to Unattached(epoch=179, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=176, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:42,256] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=179, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=179, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:42,268] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:44,086] INFO [Broker id=6] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:25:45,153] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:45,188] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:45,280] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:45,290] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:45,356] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:45,379] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:45,429] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:45,464] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:45,466] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:45,516] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:45,569] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:45,589] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:45,592] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:45,608] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,404] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:46,524] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:46,572] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,583] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,592] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:46,649] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,697] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,721] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,727] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,734] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:46,742] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:46,746] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,751] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,764] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,771] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,774] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,777] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,779] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,780] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,781] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,782] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:46,783] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,783] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,784] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:46,784] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,790] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,793] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,794] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:46,798] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,799] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,803] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,808] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:46,810] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,811] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:46,813] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,816] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:46,820] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:46,930] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-9, __consumer_offsets-21, __consumer_offsets-20, __consumer_offsets-17, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-3, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:25:46,936] INFO [Broker id=6] Stopped fetchers as part of become-follower for 16 partitions (state.change.logger)
[2026-01-16 08:25:46,991] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-45 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),1,0), __consumer_offsets-28 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),1,0), __consumer_offsets-9 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),1,0), __consumer_offsets-26 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),1,0), __consumer_offsets-39 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),1,0), __consumer_offsets-37 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),1,0), __consumer_offsets-35 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),1,0), __consumer_offsets-17 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),1,0), __consumer_offsets-1 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),1,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:25:47,036] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-31 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-43 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-12 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-8 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-21 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-3 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-20 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:25:47,658] INFO [Broker id=6] Started fetchers as part of become-follower for 16 partitions (state.change.logger)
[2026-01-16 08:25:49,005] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,041] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,424] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,433] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,463] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,465] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,468] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,469] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,477] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,485] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,490] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,532] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,533] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,539] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,557] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,568] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,543] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,576] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,576] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,576] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,576] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,576] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,576] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,577] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,577] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,587] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,626] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,641] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,665] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,690] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,734] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,637] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,770] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,772] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,781] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,785] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,788] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,791] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,793] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,798] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,796] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,801] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,805] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,816] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,821] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,821] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,827] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,833] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,836] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,836] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,843] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,842] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,857] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,866] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,868] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,868] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,875] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,883] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,876] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,891] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,886] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,903] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,906] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,906] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,906] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:49,914] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,917] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,019] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:49,917] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,331] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,328] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,340] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,341] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,343] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,344] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,348] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,348] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,348] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,351] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,353] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,345] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,355] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,362] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,379] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,354] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,422] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,423] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,437] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,463] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,434] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,523] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,481] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,857] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,857] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,858] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,860] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,860] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,870] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,871] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,872] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,872] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,873] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,873] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,873] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,874] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,874] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,874] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,874] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,874] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,874] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,874] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,875] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,875] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,874] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,880] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,883] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,884] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,884] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,885] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,885] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,885] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,893] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,893] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,893] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,894] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,900] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,900] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,901] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,901] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,901] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,901] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,901] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,901] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,901] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,901] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,902] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,902] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,902] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,902] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,902] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,902] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,902] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,903] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:25:50,904] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,904] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,928] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:50,928] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:25:53,858] INFO [RaftManager id=6] Completed transition to Unattached(epoch=180, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=179, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=894, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:54,478] INFO [RaftManager id=6] Completed transition to Unattached(epoch=185, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=180, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:54,784] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:55,103] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=185, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=894, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=185, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:55,183] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:55,317] INFO [RaftManager id=6] Completed transition to Unattached(epoch=186, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=185, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=894, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:55,821] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:56,657] INFO [RaftManager id=6] Completed transition to Unattached(epoch=187, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=186, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:56,868] INFO [RaftManager id=6] Completed transition to Unattached(epoch=188, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=187, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:59,434] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:26:00,292] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:00,577] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=188, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=894, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=188, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:26:03,284] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:03,357] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:03,498] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:03,521] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:03,610] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:03,641] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:03,646] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:03,697] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:03,718] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:03,725] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:04,035] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:04,361] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:04,410] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:04,463] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:04,530] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:04,535] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:04,672] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:04,673] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:04,725] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:04,741] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:04,743] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:04,784] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 897 (kafka.log.UnifiedLog)
[2026-01-16 08:26:04,812] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:05,078] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:05,517] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:05,535] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:26:05,812] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:05,835] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:05,887] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:05,921] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:05,945] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:06,224] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:06,244] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:06,245] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:06,296] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:06,381] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:06,384] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:06,444] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:06,535] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:06,536] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:06,598] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:06,652] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:06,661] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:06,718] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:07,274] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:07,279] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:07,361] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:07,495] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:07,531] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:07,581] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:07,800] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:07,801] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:07,860] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:07,943] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:07,945] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:07,996] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:08,049] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:08,059] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:08,121] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:08,169] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:08,193] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:08,200] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 897 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:26:08,206] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 897 (kafka.log.UnifiedLog$)
[2026-01-16 08:26:08,248] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:08,308] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:08,340] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 897 with 0 producer ids in 18 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 08:26:08,341] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 122ms for segment recovery from offset 897 (kafka.log.UnifiedLog$)
[2026-01-16 08:26:08,342] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:08,346] INFO [RaftManager id=6] Truncated to offset 897 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:26:08,419] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:08,519] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:08,536] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:08,589] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:08,646] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:08,662] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:08,671] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=189, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=894, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=188, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=894, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:26:08,719] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:08,837] INFO [Broker id=6] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:26:08,907] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:08,910] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:08,921] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:08,922] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:08,931] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:08,937] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:08,940] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:08,943] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:08,946] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:08,956] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:08,960] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:08,972] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:08,991] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,007] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,030] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:09,032] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:09,042] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,043] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,043] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:09,052] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,060] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,068] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,083] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,095] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:09,111] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:09,138] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,146] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,153] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,154] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,154] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,162] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,195] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,197] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,199] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,199] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:09,205] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,225] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,275] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:09,299] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,333] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,337] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,340] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:09,359] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,359] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,359] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,360] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:09,360] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,360] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:09,368] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,369] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:09,369] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:09,390] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,396] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,403] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,404] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,409] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,410] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,410] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,411] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,412] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,412] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,416] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,417] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,417] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,420] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,421] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,421] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,421] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,423] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,422] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,429] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,432] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,432] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,435] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,436] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,437] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,437] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,446] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,447] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,446] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,449] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,449] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,453] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,457] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,457] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,457] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,457] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,459] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,464] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,470] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,470] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,474] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,474] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,474] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,474] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,475] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,475] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,475] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,475] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,476] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,476] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,476] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,476] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,513] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,516] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,516] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,524] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,525] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,525] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,526] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,528] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,528] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,528] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,529] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,529] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,530] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,530] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,530] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,530] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,530] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,530] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,530] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,542] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,545] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,543] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,547] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,546] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,553] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,554] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,554] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,554] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,554] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,556] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,581] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,583] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,584] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,590] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,593] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,596] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,595] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,603] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,601] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,605] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,607] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,611] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,612] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,612] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,612] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,612] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,613] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,612] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,613] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,613] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,614] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,614] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,614] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,615] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,617] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,618] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,629] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,622] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,629] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,629] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,629] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,630] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,630] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,630] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,630] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,630] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,630] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,631] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,631] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,632] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,636] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,637] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,645] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,646] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,647] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,647] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,648] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,648] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,648] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,649] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,650] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,649] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,650] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,650] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,650] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,650] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,650] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,650] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,651] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,657] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,651] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,659] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,658] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:09,659] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:09,665] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:12,121] INFO [Broker id=6] Transitioning 25 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:26:12,391] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,400] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,420] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,422] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:12,431] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,432] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:12,456] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,471] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,479] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,495] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,509] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,521] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:12,556] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:12,617] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,641] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,684] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:12,755] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,761] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,770] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:12,774] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,883] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,892] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,935] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:12,944] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:12,963] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:13,057] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,064] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,082] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,083] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,091] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,093] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,097] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,100] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,111] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,119] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,127] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,128] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,129] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,141] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,144] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,145] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,151] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,151] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,151] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,152] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,152] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,162] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,163] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,170] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,165] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,174] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,176] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,176] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,181] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,188] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,188] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,188] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,188] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,188] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,190] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,202] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,203] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,208] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,231] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,240] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,242] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,258] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,250] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,265] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,265] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,266] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,268] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,266] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,271] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,271] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,274] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,274] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,274] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,295] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,295] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,296] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,298] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:13,296] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,302] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,302] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,306] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,309] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,704] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,709] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,724] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,745] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,749] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,760] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,779] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,812] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,813] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,824] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:13,901] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:26:14,053] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,121] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,123] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,141] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,179] INFO [Broker id=6] Transitioning 23 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:26:14,189] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,193] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,202] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,204] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,218] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,225] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,243] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,254] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,261] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,275] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,276] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,292] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,299] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,301] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,301] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,304] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,317] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,324] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,340] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,350] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,683] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,712] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,722] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,819] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,831] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,833] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,852] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,858] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,895] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,897] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,898] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,899] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,901] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,904] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,929] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,929] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,929] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,835] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,930] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,930] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,930] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,930] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,937] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,958] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,930] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,960] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,961] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,962] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,966] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,969] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,972] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,001] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,961] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,071] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,075] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,072] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,089] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,089] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,090] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,090] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,100] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,100] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,100] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,136] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,139] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,141] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,140] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,150] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,146] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,166] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,167] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,177] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,174] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,182] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,179] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,184] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,184] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,185] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,189] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,191] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,215] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,252] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,252] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,252] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,258] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,263] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,203] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,275] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,280] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,304] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,268] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,313] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,592] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:26:15,646] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:15,715] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,729] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,756] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,803] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:26:15,864] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:15,868] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:15,897] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:15,898] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:28:04,302] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:28:04,751] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1284 due to node 3 being disconnected (elapsed time since creation: 2071ms, elapsed time since send: 2047ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:28:05,494] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:28:05,536] INFO [RaftManager id=6] Completed transition to Unattached(epoch=190, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=189, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:28:05,541] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:28:06,115] INFO [RaftManager id=6] Completed transition to Unattached(epoch=191, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=190, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:28:06,831] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=191, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=191, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:28:06,890] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:28:07,052] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:28:07,057] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:28:07,117] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:01,091] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:01,186] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1388 due to node 1 being disconnected (elapsed time since creation: 2504ms, elapsed time since send: 2501ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:01,262] WARN [BrokerLifecycleManager id=6] Broker 6 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:29:01,444] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:01,474] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:01,647] INFO [RaftManager id=6] Completed transition to Unattached(epoch=192, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=191, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1377, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:29:01,964] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:02,085] INFO [RaftManager id=6] Completed transition to Unattached(epoch=193, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=192, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:29:04,438] INFO [RaftManager id=6] Completed transition to Unattached(epoch=194, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=193, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:29:04,513] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=194, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1377, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=194, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:29:04,532] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:31:59,521] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:00,884] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1757 due to node 1 being disconnected (elapsed time since creation: 2053ms, elapsed time since send: 2052ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:07,368] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:09,288] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 497 due to node 1 being disconnected (elapsed time since creation: 6624ms, elapsed time since send: 6624ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:09,596] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:32:09,824] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:12,844] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:13,079] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1760 due to node 3 being disconnected (elapsed time since creation: 11182ms, elapsed time since send: 2158ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:16,436] WARN [BrokerLifecycleManager id=6] Broker 6 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:16,994] INFO [RaftManager id=6] Completed transition to Unattached(epoch=200, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=194, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1710, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:19,703] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:20,296] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:20,343] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1765 due to node 1 being disconnected (elapsed time since creation: 2099ms, elapsed time since send: 2026ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:22,769] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:23,082] INFO [RaftManager id=6] Completed transition to Unattached(epoch=203, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=200, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:28,689] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:32,664] INFO [RaftManager id=6] Completed transition to Unattached(epoch=205, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=203, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:33,886] INFO [RaftManager id=6] Completed transition to Unattached(epoch=207, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=205, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:34,544] INFO [RaftManager id=6] Completed transition to Unattached(epoch=208, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=207, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:38,094] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:39,058] INFO [RaftManager id=6] Completed transition to Unattached(epoch=209, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=208, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:39,245] INFO [RaftManager id=6] Completed transition to Unattached(epoch=211, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=209, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:41,148] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=211, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1710, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=211, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:41,149] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:32:47,116] INFO [RaftManager id=6] Completed transition to Unattached(epoch=213, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=211, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1710, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:47,138] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:50,346] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:53,864] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:54,581] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1783 due to node 3 being disconnected (elapsed time since creation: 3032ms, elapsed time since send: 2119ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:56,031] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:59,426] INFO [RaftManager id=6] Completed transition to Unattached(epoch=220, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=213, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:00,997] INFO [RaftManager id=6] Completed transition to Unattached(epoch=222, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=220, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:01,869] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:33:02,207] INFO [RaftManager id=6] Completed transition to Unattached(epoch=224, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=222, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:05,394] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:05,691] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1797 due to node 3 being disconnected (elapsed time since creation: 2109ms, elapsed time since send: 2109ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:06,994] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:33:07,003] INFO [RaftManager id=6] Completed transition to Unattached(epoch=225, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=224, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:12,528] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:33:13,984] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:14,868] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1801 due to node 1 being disconnected (elapsed time since creation: 2464ms, elapsed time since send: 2345ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:17,716] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:18,171] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 1804 due to node 1 being disconnected (elapsed time since creation: 2303ms, elapsed time since send: 2303ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:18,462] INFO [RaftManager id=6] Completed transition to Unattached(epoch=230, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=225, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:20,205] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:33:23,182] INFO [RaftManager id=6] Completed transition to Unattached(epoch=234, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=230, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:25,054] INFO [RaftManager id=6] Completed transition to Unattached(epoch=236, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=234, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:25,437] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=238, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1710, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=236, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:25,581] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:31,309] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:31,725] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:31,768] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:33:33,663] INFO [RaftManager id=6] Completed transition to Unattached(epoch=241, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=238, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1718, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:35,061] INFO [RaftManager id=6] Completed transition to Unattached(epoch=242, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=241, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:35,567] INFO [RaftManager id=6] Completed transition to Unattached(epoch=243, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=242, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:36,746] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=243, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1718, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=243, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:40,689] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:40,813] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 1817 due to node 3 being disconnected (elapsed time since creation: 2067ms, elapsed time since send: 2028ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:41,523] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:41,563] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:41,967] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:42,011] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:42,092] INFO [RaftManager id=6] Completed transition to Unattached(epoch=246, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=243, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1720, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:42,318] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:42,553] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=246, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1720, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=246, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:42,651] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:43,319] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:43,446] INFO [RaftManager id=6] Completed transition to Unattached(epoch=247, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=246, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1720, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:43,644] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:33:44,273] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:44,325] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=247, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1720, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=247, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:45,500] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 1720 (kafka.log.UnifiedLog)
[2026-01-16 08:33:47,050] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 1720 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:33:47,136] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 1720 (kafka.log.UnifiedLog$)
[2026-01-16 08:33:47,150] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=897, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000000897.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 08:33:49,056] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 1720 with 0 producer ids in 150 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 08:33:49,153] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1456ms for snapshot load and 557ms for segment recovery from offset 1720 (kafka.log.UnifiedLog$)
[2026-01-16 08:33:49,196] INFO [RaftManager id=6] Truncated to offset 1720 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:33:55,929] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2026-01-16 08:34:01,206] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-39, __consumer_offsets-5, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:34:01,654] INFO [Broker id=6] Leader __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:02,299] INFO [Broker id=6] Leader __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:34:02,437] INFO [Broker id=6] Leader __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:02,479] INFO [Broker id=6] Leader __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:34:02,672] INFO [Broker id=6] Leader __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:34:03,099] INFO [Broker id=6] Leader __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:34:03,294] INFO [Broker id=6] Leader __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:03,914] INFO [Broker id=6] Leader __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:34:04,105] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-15 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,127] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-15 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,128] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-48 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,129] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-48 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,129] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-46 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,130] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-46 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,131] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-10 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,132] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-10 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,132] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-24 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,133] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-24 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,133] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-40 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,133] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-40 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,133] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-38 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,133] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-38 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,133] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-33 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,133] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-33 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,133] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-2 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,133] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-2 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,133] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-29 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,134] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-29 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:04,261] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:04,773] INFO [Broker id=6] Leader __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:06,178] INFO [Broker id=6] Leader __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:34:06,438] INFO [Broker id=6] Leader __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:06,606] INFO [Broker id=6] Leader __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:34:06,680] INFO [Broker id=6] Leader __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:06,728] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:06,885] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:06,937] INFO [Broker id=6] Transitioning 35 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:34:06,972] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:06,991] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:07,017] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,058] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 1 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:07,059] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,061] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,066] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 6], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:07,066] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,066] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:07,067] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:07,067] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 6], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:07,067] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,067] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,067] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,068] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,069] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,071] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 6], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:07,079] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 6], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:07,080] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,086] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:07,089] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:07,090] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,090] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 6], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:07,090] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,090] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,091] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 6], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:07,094] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,101] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 1 from offset 1 with partition epoch 5 and high watermark 1. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:07,106] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,215] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,219] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 6], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:07,227] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,250] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 6], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:07,256] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:07,256] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 1 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:08,545] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-29, __consumer_offsets-46, __consumer_offsets-10, __consumer_offsets-40, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:34:08,569] INFO [Broker id=6] Stopped fetchers as part of become-follower for 10 partitions (state.change.logger)
[2026-01-16 08:34:09,754] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-48 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-29 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,1), __consumer_offsets-46 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-10 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-40 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-24 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-38 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-33 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-2 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:34:09,799] INFO [Broker id=6] Started fetchers as part of become-follower for 10 partitions (state.change.logger)
[2026-01-16 08:34:09,761] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:09,957] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:34:10,033] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:10,034] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:34:10,036] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:10,039] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:34:10,040] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:10,040] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:34:10,041] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:10,042] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:34:10,044] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:10,044] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:34:10,045] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:10,047] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:34:10,056] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:10,057] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:34:10,068] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:10,072] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:34:10,449] INFO [ReplicaFetcherThread-0-4]: Shutting down (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:10,953] INFO [ReplicaFetcherThread-0-4]: Stopped (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:11,107] INFO [ReplicaFetcherThread-0-4]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:15,646] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:34:15,952] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:34:16,027] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:34:16,164] INFO [RaftManager id=6] Completed transition to Unattached(epoch=251, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=247, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1876, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:34:16,817] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:34:17,454] INFO [RaftManager id=6] Completed transition to Unattached(epoch=252, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=251, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:34:18,774] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=252, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1876, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=252, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:34:18,807] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:34:19,426] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 45 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,499] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,551] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 14 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,558] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,559] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 9 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,560] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,570] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 42 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,570] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,577] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 23 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,579] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,584] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 19 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,586] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,596] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 17 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,602] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,603] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 30 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,603] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,610] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,617] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,617] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 26 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,618] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,624] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 7 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,626] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,636] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 39 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,637] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,646] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 5 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,646] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,650] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 37 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,660] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,660] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,660] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,661] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,661] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,663] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,715] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,760] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,762] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,768] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,770] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,771] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,771] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,775] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,786] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,788] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,796] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,797] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,797] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,803] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,803] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,805] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,806] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,830] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,840] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,843] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,843] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,843] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,843] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,843] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,843] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,873] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,876] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,879] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,883] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,887] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,887] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,887] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,888] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,888] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,889] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,893] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,894] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,899] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,907] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,908] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,909] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,910] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,911] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,912] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,913] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,913] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,913] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,913] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,913] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,913] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,913] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,914] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,914] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,914] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,914] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,914] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,914] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,917] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,923] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,924] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,943] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,925] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-45 in 409 milliseconds for epoch 2, of which 132 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,944] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-14 in 385 milliseconds for epoch 1, of which 385 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,945] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-9 in 375 milliseconds for epoch 2, of which 375 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,945] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,947] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,947] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-42 in 371 milliseconds for epoch 1, of which 370 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,948] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,949] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,949] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-23 in 365 milliseconds for epoch 1, of which 364 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,951] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:19,952] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,952] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-19 in 356 milliseconds for epoch 1, of which 355 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,955] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-17 in 352 milliseconds for epoch 2, of which 351 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,965] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-30 in 355 milliseconds for epoch 1, of which 354 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,968] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 350 milliseconds for epoch 2, of which 350 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,984] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-26 in 360 milliseconds for epoch 2, of which 347 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,987] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-7 in 351 milliseconds for epoch 1, of which 351 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,989] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-39 in 343 milliseconds for epoch 2, of which 342 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,992] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-5 in 342 milliseconds for epoch 1, of which 340 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,995] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-37 in 335 milliseconds for epoch 2, of which 334 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,996] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 335 milliseconds for epoch 2, of which 334 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,997] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 335 milliseconds for epoch 2, of which 334 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,998] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,999] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:19,999] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,000] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,000] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,001] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,001] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,002] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,002] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,003] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,004] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,008] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,008] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,009] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,009] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,009] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,012] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,013] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,017] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,030] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,033] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,040] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,054] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,056] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,058] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,060] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,062] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,085] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,097] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,107] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,115] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,118] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,121] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:20,134] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:21,179] INFO [Broker id=6] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:34:21,481] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:21,548] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,133] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:22,174] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:22,194] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,258] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:22,303] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:22,309] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:22,318] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:22,319] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,322] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:22,328] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,334] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,337] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,338] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:22,341] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,347] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,349] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,350] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,350] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,352] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,357] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,358] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,360] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,362] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:22,376] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,382] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,409] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,435] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,451] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,459] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,471] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:22,530] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:22,538] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,643] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:22,703] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:22,717] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,724] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,747] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,782] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,835] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:22,883] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,885] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,887] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,891] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,921] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:22,923] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,936] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:22,947] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:34:22,954] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:22,965] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:34:23,192] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-39, __consumer_offsets-7, __consumer_offsets-37, __consumer_offsets-5, __consumer_offsets-35, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:34:23,200] INFO [Broker id=6] Stopped fetchers as part of become-follower for 16 partitions (state.change.logger)
[2026-01-16 08:34:23,471] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-45 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-14 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-9 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-42 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-23 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-19 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-17 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-30 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-28 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-26 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-39 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-7 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-37 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-5 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-35 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-1 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:34:23,494] INFO [Broker id=6] Started fetchers as part of become-follower for 16 partitions (state.change.logger)
[2026-01-16 08:34:25,175] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:25,211] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,267] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:25,276] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,311] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:25,315] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,331] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:25,341] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,349] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:25,319] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,356] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,358] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:25,360] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,368] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,369] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:25,395] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,399] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:25,438] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,442] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:25,446] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,482] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:25,372] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,514] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:25,486] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,045] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,049] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,052] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,136] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,149] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,167] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,125] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,175] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,177] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,179] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,192] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,193] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,194] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,195] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,198] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,169] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,217] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,221] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,229] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,233] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,246] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,248] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,250] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,221] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,255] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,258] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,264] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,257] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,283] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,286] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,311] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,312] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,312] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,287] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,315] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,316] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,314] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,317] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,317] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,318] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,318] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,318] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,318] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,318] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,318] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,323] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,317] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,325] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,325] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,326] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,324] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,328] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,328] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,331] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,330] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,333] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,333] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,337] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,400] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,430] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,467] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,484] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,486] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,487] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,518] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,591] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,631] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,467] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,634] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,634] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,634] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,636] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,638] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,645] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,644] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,650] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,655] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,659] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,665] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,664] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,669] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,669] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,978] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:26,982] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,997] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,007] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:27,009] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,012] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:26,983] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,018] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,028] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,014] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,030] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:27,032] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,039] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:27,040] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,031] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,046] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,055] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,040] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:27,074] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,103] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,121] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:27,123] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,125] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:27,126] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,125] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,129] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:27,129] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,129] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,131] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,131] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:27,132] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,132] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:27,133] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,154] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,212] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,212] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:27,214] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,215] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,215] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:27,215] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:27,215] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:29,821] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:34:29,862] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 1209 due to node 5 being disconnected (elapsed time since creation: 7413ms, elapsed time since send: 7413ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:34:29,865] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:34:29,864] WARN [BrokerLifecycleManager id=6] Broker 6 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:34:29,990] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=800976800, epoch=1209) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:34:30,478] INFO [Broker id=6] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:34:31,222] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,235] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,236] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:31,246] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:31,246] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,248] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:31,252] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:31,255] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:31,258] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:31,259] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,260] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:31,264] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,269] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 1 from offset 2 with partition epoch 7 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,274] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,274] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:31,275] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,275] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,275] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,276] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,277] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,277] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,279] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 1 with partition epoch 7 and high watermark 1. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,281] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,284] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,290] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:31,290] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,290] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,290] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,290] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,291] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,291] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,291] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:31,291] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:31,292] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,292] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:31,293] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:31,294] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,294] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,296] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,297] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,300] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:31,303] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,303] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,304] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,305] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,307] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,309] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,311] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:31,311] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:31,311] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:31,311] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:31,409] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:34:31,411] INFO [Broker id=6] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2026-01-16 08:34:31,481] INFO [Broker id=6] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2026-01-16 08:34:31,499] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:32,149] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=800976800, epoch=1209), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:34:32,162] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:32,164] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:32,313] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,341] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,348] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,353] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,361] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,362] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,362] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,362] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,363] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,363] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,368] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,371] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,385] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,409] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,416] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,416] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,437] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,438] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,439] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,439] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,439] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,351] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,440] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,439] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,440] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,441] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,444] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,509] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,511] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,548] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,551] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,553] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,554] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,556] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,556] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,556] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,557] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,440] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,562] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,564] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,566] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,575] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,576] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,576] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,578] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,579] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,579] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,581] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,560] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,582] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,583] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,592] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,595] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,595] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,596] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,581] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,596] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,599] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,597] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,607] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,606] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,613] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,613] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,619] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,619] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,619] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,619] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,620] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,620] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,620] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,620] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,622] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,620] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,622] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,622] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,622] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,622] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,622] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,622] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,623] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,622] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,623] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,623] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,623] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,623] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,623] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,623] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,623] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,623] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,624] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,628] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,632] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,632] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,632] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,632] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,633] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,632] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,638] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,644] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,644] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,645] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,646] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,646] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,646] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,649] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,655] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,654] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,657] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,657] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,663] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,663] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,663] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,663] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,664] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,663] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,664] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,664] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,664] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,664] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,664] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,664] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,666] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,666] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:32,666] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,666] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,668] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:32,667] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:33,017] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,035] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:33,036] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,036] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:33,044] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,047] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:33,050] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,079] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:33,083] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,084] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:33,084] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,084] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:33,036] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,085] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,085] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,086] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,090] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,104] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,084] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,107] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:33,107] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,122] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:33,136] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:36:37,968] INFO [RaftManager id=6] Completed transition to Unattached(epoch=253, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=252, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2183, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:36:38,277] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:36:38,343] INFO [RaftManager id=6] Completed transition to Unattached(epoch=256, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=253, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:36:38,718] INFO [RaftManager id=6] Completed transition to Unattached(epoch=257, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=256, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:36:40,030] INFO [RaftManager id=6] Completed transition to Unattached(epoch=258, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=257, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:36:40,489] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=258, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2183, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=258, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:36:40,571] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:36:53,196] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:36:53,674] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2171 due to node 1 being disconnected (elapsed time since creation: 3445ms, elapsed time since send: 2690ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:36:56,774] INFO [RaftManager id=6] Completed transition to Unattached(epoch=259, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=258, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:36:56,896] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:36:57,702] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 591 due to node 1 being disconnected (elapsed time since creation: 5258ms, elapsed time since send: 5258ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:36:58,020] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:00,005] INFO [RaftManager id=6] Completed transition to Unattached(epoch=261, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=259, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:02,785] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:04,224] INFO [RaftManager id=6] Completed transition to Unattached(epoch=264, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=261, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:09,257] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:09,443] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:09,866] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2176 due to node 3 being disconnected (elapsed time since creation: 4004ms, elapsed time since send: 3430ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:13,133] INFO [RaftManager id=6] Completed transition to Unattached(epoch=268, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=264, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:14,925] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:19,435] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:21,734] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:22,579] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2179 due to node 1 being disconnected (elapsed time since creation: 4023ms, elapsed time since send: 2881ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:24,970] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:26,042] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2181 due to node 2 being disconnected (elapsed time since creation: 3176ms, elapsed time since send: 2018ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:29,153] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:34,634] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:35,485] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2184 due to node 1 being disconnected (elapsed time since creation: 4065ms, elapsed time since send: 2811ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:41,027] INFO [RaftManager id=6] Completed transition to Unattached(epoch=276, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=268, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:41,554] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:41,594] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2191 due to node 3 being disconnected (elapsed time since creation: 3738ms, elapsed time since send: 2872ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:42,794] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:43,397] INFO [RaftManager id=6] Completed transition to Unattached(epoch=277, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=276, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:45,798] INFO [RaftManager id=6] Completed transition to Unattached(epoch=279, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=277, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:49,654] INFO [RaftManager id=6] Completed transition to Unattached(epoch=280, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=279, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:54,447] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:55,880] INFO [RaftManager id=6] Completed transition to Unattached(epoch=282, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=280, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:57,759] INFO [RaftManager id=6] Completed transition to Unattached(epoch=283, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=282, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:38:04,217] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:04,253] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2202 due to node 2 being disconnected (elapsed time since creation: 2394ms, elapsed time since send: 2163ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:08,212] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:09,126] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:38:09,313] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2205 due to node 3 being disconnected (elapsed time since creation: 2321ms, elapsed time since send: 2176ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:13,277] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:14,341] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2206 due to node 1 being disconnected (elapsed time since creation: 4590ms, elapsed time since send: 2641ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:15,137] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:15,229] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2208 due to node 3 being disconnected (elapsed time since creation: 2789ms, elapsed time since send: 2789ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:24,011] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:24,577] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:38:25,380] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2211 due to node 1 being disconnected (elapsed time since creation: 2849ms, elapsed time since send: 2849ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:38,912] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:40,169] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:38:40,699] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2219 due to node 2 being disconnected (elapsed time since creation: 4701ms, elapsed time since send: 3524ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:41,151] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:41,229] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2218 due to node 3 being disconnected (elapsed time since creation: 10458ms, elapsed time since send: 3524ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:52,546] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:52,835] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2223 due to node 2 being disconnected (elapsed time since creation: 2471ms, elapsed time since send: 2471ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:57,657] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:38:57,846] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:58,578] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2231 due to node 1 being disconnected (elapsed time since creation: 2129ms, elapsed time since send: 2100ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:04,956] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:05,277] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2233 due to node 1 being disconnected (elapsed time since creation: 3168ms, elapsed time since send: 3168ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:07,205] INFO [RaftManager id=6] Completed transition to Unattached(epoch=300, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=283, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:39:10,902] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:11,157] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2235 due to node 3 being disconnected (elapsed time since creation: 2502ms, elapsed time since send: 2286ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:14,229] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:39:14,574] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:14,784] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2236 due to node 1 being disconnected (elapsed time since creation: 3755ms, elapsed time since send: 2478ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:30,028] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:30,529] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2242 due to node 1 being disconnected (elapsed time since creation: 4851ms, elapsed time since send: 4851ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:32,792] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:33,683] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:39:34,302] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2244 due to node 2 being disconnected (elapsed time since creation: 4071ms, elapsed time since send: 2176ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:36,953] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:37,174] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2248 due to node 1 being disconnected (elapsed time since creation: 2126ms, elapsed time since send: 2042ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:39,072] INFO [RaftManager id=6] Completed transition to Unattached(epoch=310, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=300, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:39:42,366] INFO [RaftManager id=6] Completed transition to Unattached(epoch=311, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=310, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:39:45,667] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:46,465] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2252 due to node 3 being disconnected (elapsed time since creation: 2582ms, elapsed time since send: 2480ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:47,741] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:39:51,692] INFO [RaftManager id=6] Completed transition to Unattached(epoch=318, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=311, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:39:54,150] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:54,316] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2259 due to node 3 being disconnected (elapsed time since creation: 2136ms, elapsed time since send: 2104ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:54,687] INFO [RaftManager id=6] Completed transition to Unattached(epoch=320, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=318, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:39:59,267] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:59,626] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2263 due to node 1 being disconnected (elapsed time since creation: 2099ms, elapsed time since send: 2068ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:02,470] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:40:03,332] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:03,386] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2264 due to node 3 being disconnected (elapsed time since creation: 4007ms, elapsed time since send: 2040ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:04,919] INFO [RaftManager id=6] Completed transition to Unattached(epoch=325, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=320, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:05,168] INFO [RaftManager id=6] Completed transition to Unattached(epoch=326, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=325, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:06,045] INFO [RaftManager id=6] Completed transition to Unattached(epoch=327, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=326, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:07,512] INFO [RaftManager id=6] Completed transition to Unattached(epoch=328, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=327, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:10,126] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:10,254] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2275 due to node 3 being disconnected (elapsed time since creation: 2205ms, elapsed time since send: 2204ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:16,970] INFO [RaftManager id=6] Completed transition to Unattached(epoch=333, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=328, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:18,881] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:40:19,334] INFO [RaftManager id=6] Completed transition to Unattached(epoch=334, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=333, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:19,762] INFO [RaftManager id=6] Completed transition to Unattached(epoch=335, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=334, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:20,904] INFO [RaftManager id=6] Completed transition to Unattached(epoch=336, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=335, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:32,358] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:40:32,463] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=338, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=336, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:35,423] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=342, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=338, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:35,960] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:35,976] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:40:36,103] INFO [RaftManager id=6] Completed transition to Unattached(epoch=343, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=342, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:36,516] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:36,573] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:40:37,960] INFO [RaftManager id=6] Completed transition to Unattached(epoch=344, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=343, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:38,175] INFO [RaftManager id=6] Completed transition to Unattached(epoch=345, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=344, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:41,361] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:40:41,452] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=345, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=345, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:46,074] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:47,766] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2295 due to node 3 being disconnected (elapsed time since creation: 3156ms, elapsed time since send: 2507ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:48,483] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:48,740] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:40:48,957] INFO [RaftManager id=6] Completed transition to Unattached(epoch=349, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=345, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:49,092] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:50,767] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:40:51,200] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:51,284] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2299 due to node 1 being disconnected (elapsed time since creation: 2121ms, elapsed time since send: 2103ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:52,584] INFO [RaftManager id=6] Completed transition to Unattached(epoch=350, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=349, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:54,357] INFO [RaftManager id=6] Completed transition to Unattached(epoch=352, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=350, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:41:03,393] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:03,607] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2302 due to node 3 being disconnected (elapsed time since creation: 2286ms, elapsed time since send: 2074ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:05,427] INFO [RaftManager id=6] Completed transition to Unattached(epoch=357, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=352, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:41:08,508] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:41:08,699] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:09,213] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2304 due to node 2 being disconnected (elapsed time since creation: 2990ms, elapsed time since send: 2954ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:25,489] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=365, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=357, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:41:25,597] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:41:26,197] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:41:28,793] INFO [RaftManager id=6] Completed transition to Unattached(epoch=366, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=365, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:41:36,134] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:37,707] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2316 due to node 1 being disconnected (elapsed time since creation: 4047ms, elapsed time since send: 3913ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:41,357] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:41:44,032] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:45,865] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2320 due to node 3 being disconnected (elapsed time since creation: 2975ms, elapsed time since send: 2809ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:49,134] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:50,854] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2321 due to node 2 being disconnected (elapsed time since creation: 4948ms, elapsed time since send: 2221ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:56,317] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:42:02,659] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:05,015] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2322 due to node 1 being disconnected (elapsed time since creation: 11057ms, elapsed time since send: 10377ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:08,279] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:08,504] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2325 due to node 2 being disconnected (elapsed time since creation: 10723ms, elapsed time since send: 10723ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:11,266] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:42:16,372] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:16,759] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2333 due to node 1 being disconnected (elapsed time since creation: 4026ms, elapsed time since send: 3504ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:26,423] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:27,345] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2340 due to node 3 being disconnected (elapsed time since creation: 3414ms, elapsed time since send: 3414ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:27,371] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:42:36,775] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:38,287] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2345 due to node 1 being disconnected (elapsed time since creation: 5225ms, elapsed time since send: 2249ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:38,633] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:38,653] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2343 due to node 2 being disconnected (elapsed time since creation: 2689ms, elapsed time since send: 2689ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:38,717] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:38,780] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2344 due to node 3 being disconnected (elapsed time since creation: 2689ms, elapsed time since send: 2689ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:43,233] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:42:44,707] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:44,923] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2350 due to node 2 being disconnected (elapsed time since creation: 2484ms, elapsed time since send: 2484ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:48,832] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:54,212] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2354 due to node 2 being disconnected (elapsed time since creation: 2091ms, elapsed time since send: 2091ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:54,839] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:54,930] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2352 due to node 3 being disconnected (elapsed time since creation: 6168ms, elapsed time since send: 3358ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:57,194] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:57,555] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2358 due to node 2 being disconnected (elapsed time since creation: 2118ms, elapsed time since send: 2118ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:01,974] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:43:09,384] INFO [RaftManager id=6] Completed transition to Unattached(epoch=389, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=366, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:43:16,807] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:43:17,403] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:17,633] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2368 due to node 3 being disconnected (elapsed time since creation: 2536ms, elapsed time since send: 2064ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:21,176] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:22,378] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2370 due to node 1 being disconnected (elapsed time since creation: 2126ms, elapsed time since send: 2076ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:22,568] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:22,611] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2369 due to node 2 being disconnected (elapsed time since creation: 4223ms, elapsed time since send: 2896ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:31,050] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:31,158] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:43:31,887] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2377 due to node 3 being disconnected (elapsed time since creation: 3339ms, elapsed time since send: 3339ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:35,824] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:37,150] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2379 due to node 1 being disconnected (elapsed time since creation: 4040ms, elapsed time since send: 3001ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:45,781] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:43:50,688] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:51,434] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2383 due to node 1 being disconnected (elapsed time since creation: 4131ms, elapsed time since send: 4131ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:51,505] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:51,512] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2384 due to node 3 being disconnected (elapsed time since creation: 4131ms, elapsed time since send: 4131ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:51,925] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:57,057] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2385 due to node 2 being disconnected (elapsed time since creation: 12628ms, elapsed time since send: 2958ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:05,388] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:44:07,297] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:07,479] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2394 due to node 3 being disconnected (elapsed time since creation: 2918ms, elapsed time since send: 2384ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:16,792] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:17,700] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2400 due to node 1 being disconnected (elapsed time since creation: 2578ms, elapsed time since send: 2389ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:17,908] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:17,927] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2401 due to node 2 being disconnected (elapsed time since creation: 2393ms, elapsed time since send: 2323ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:21,456] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:44:24,948] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:25,090] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2403 due to node 3 being disconnected (elapsed time since creation: 4408ms, elapsed time since send: 4408ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:26,551] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:26,842] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2405 due to node 1 being disconnected (elapsed time since creation: 2712ms, elapsed time since send: 2712ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:28,587] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:28,767] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2407 due to node 3 being disconnected (elapsed time since creation: 2024ms, elapsed time since send: 2024ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:32,971] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:33,487] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2410 due to node 1 being disconnected (elapsed time since creation: 4656ms, elapsed time since send: 3670ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:35,994] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:36,221] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2413 due to node 2 being disconnected (elapsed time since creation: 4139ms, elapsed time since send: 2300ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:37,967] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:44:42,912] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:42,963] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2417 due to node 3 being disconnected (elapsed time since creation: 4970ms, elapsed time since send: 3879ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:48,570] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:49,541] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2421 due to node 3 being disconnected (elapsed time since creation: 2490ms, elapsed time since send: 2490ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:54,988] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:45:01,558] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:04,854] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2425 due to node 2 being disconnected (elapsed time since creation: 2146ms, elapsed time since send: 2146ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:05,418] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:05,462] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2428 due to node 1 being disconnected (elapsed time since creation: 6635ms, elapsed time since send: 4582ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:08,881] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:09,151] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2431 due to node 3 being disconnected (elapsed time since creation: 2799ms, elapsed time since send: 2799ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:11,024] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:45:17,641] INFO [RaftManager id=6] Completed transition to Unattached(epoch=411, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=389, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:45:21,205] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:21,394] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2437 due to node 3 being disconnected (elapsed time since creation: 2302ms, elapsed time since send: 2129ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:27,419] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:45:36,685] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:38,059] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2440 due to node 1 being disconnected (elapsed time since creation: 4114ms, elapsed time since send: 2734ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:41,990] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:45:51,605] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:51,623] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2445 due to node 1 being disconnected (elapsed time since creation: 3412ms, elapsed time since send: 3412ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:51,644] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:51,645] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2446 due to node 2 being disconnected (elapsed time since creation: 3412ms, elapsed time since send: 3412ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:55,988] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:56,714] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2453 due to node 3 being disconnected (elapsed time since creation: 3829ms, elapsed time since send: 3085ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:57,339] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:46:04,539] INFO [RaftManager id=6] Completed transition to Unattached(epoch=420, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=411, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:07,019] INFO [RaftManager id=6] Completed transition to Unattached(epoch=423, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=420, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:09,388] INFO [RaftManager id=6] Completed transition to Unattached(epoch=426, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=423, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:11,300] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:46:12,033] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:12,083] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2462 due to node 3 being disconnected (elapsed time since creation: 2391ms, elapsed time since send: 2319ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:14,228] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:14,272] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2463 due to node 2 being disconnected (elapsed time since creation: 2147ms, elapsed time since send: 2056ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:14,938] INFO [RaftManager id=6] Completed transition to Unattached(epoch=429, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=426, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:17,943] INFO [RaftManager id=6] Completed transition to Unattached(epoch=431, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=429, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:20,639] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:20,888] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2475 due to node 2 being disconnected (elapsed time since creation: 2290ms, elapsed time since send: 2027ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:25,282] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:46:27,323] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:27,738] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2479 due to node 2 being disconnected (elapsed time since creation: 4097ms, elapsed time since send: 3996ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:29,457] INFO [RaftManager id=6] Completed transition to Unattached(epoch=435, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=431, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:32,100] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:32,426] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 2483 due to node 2 being disconnected (elapsed time since creation: 2137ms, elapsed time since send: 2137ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:33,794] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:33,932] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2484 due to node 3 being disconnected (elapsed time since creation: 2233ms, elapsed time since send: 2015ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:36,044] INFO [RaftManager id=6] Completed transition to Unattached(epoch=440, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=435, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:36,441] INFO [RaftManager id=6] Completed transition to Unattached(epoch=441, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=440, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:36,651] INFO [RaftManager id=6] Completed transition to Unattached(epoch=442, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=441, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:39,331] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:46:39,404] INFO [RaftManager id=6] Completed transition to Unattached(epoch=443, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=442, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:41,388] INFO [RaftManager id=6] Completed transition to Unattached(epoch=444, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=443, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:44,364] INFO [RaftManager id=6] Completed transition to Unattached(epoch=446, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=444, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:45,743] INFO [RaftManager id=6] Completed transition to Unattached(epoch=447, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=446, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:46,121] INFO [RaftManager id=6] Completed transition to Unattached(epoch=448, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=447, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:46,980] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=448, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=448, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:47,112] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:46:50,533] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:51,284] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:46:54,344] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:54,545] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:46:54,561] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:46:56,149] INFO [RaftManager id=6] Completed transition to Unattached(epoch=452, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=448, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:56,536] INFO [RaftManager id=6] Completed transition to Unattached(epoch=453, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=452, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:00,613] INFO [RaftManager id=6] Completed transition to Unattached(epoch=454, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=453, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:07,759] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:09,231] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:47:09,554] INFO [RaftManager id=6] Completed transition to Unattached(epoch=458, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=454, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:13,461] INFO [RaftManager id=6] Completed transition to Unattached(epoch=463, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=458, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:14,372] INFO [RaftManager id=6] Completed transition to Unattached(epoch=464, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=463, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:17,219] INFO [RaftManager id=6] Completed transition to Unattached(epoch=465, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=464, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:18,118] INFO [RaftManager id=6] Completed transition to Unattached(epoch=468, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=465, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:20,607] INFO [RaftManager id=6] Completed transition to Unattached(epoch=469, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=468, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:20,898] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=469, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=469, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:20,923] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:22,832] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:23,519] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:23,573] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:47:28,037] INFO [RaftManager id=6] Completed transition to Unattached(epoch=470, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=469, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:33,555] INFO [RaftManager id=6] Completed transition to Unattached(epoch=474, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=470, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:33,748] INFO [RaftManager id=6] Completed transition to Unattached(epoch=478, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=474, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:33,980] INFO [RaftManager id=6] Completed transition to Unattached(epoch=479, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=478, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:34,232] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:34,515] INFO [RaftManager id=6] Completed transition to Unattached(epoch=480, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=479, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:35,864] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=480, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=480, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:36,209] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:41,879] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:45,138] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:46,804] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:47:46,911] INFO [RaftManager id=6] Completed transition to Unattached(epoch=483, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=480, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:49,267] INFO [RaftManager id=6] Completed transition to Unattached(epoch=487, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=483, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:51,841] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:52,060] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:47:53,990] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:54,255] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=488, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=487, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:59,812] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=491, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=488, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:00,890] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:01,047] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:01,289] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:48:01,357] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2534 due to node 3 being disconnected (elapsed time since creation: 5646ms, elapsed time since send: 2165ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:01,420] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:48:05,472] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:05,585] INFO [RaftManager id=6] Completed transition to Unattached(epoch=493, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=491, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:10,356] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:48:12,377] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:48:12,784] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=496, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=493, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:20,473] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:48:20,652] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:48:27,840] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:29,226] INFO [RaftManager id=6] Completed transition to Unattached(epoch=501, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=496, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:30,794] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:48:39,936] INFO [RaftManager id=6] Completed transition to Unattached(epoch=511, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=501, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:41,725] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:48:42,938] INFO [RaftManager id=6] Completed transition to Unattached(epoch=512, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=511, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:43,508] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=513, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=512, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:43,590] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:48:44,109] INFO [RaftManager id=6] Completed transition to Unattached(epoch=514, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=513, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:44,971] INFO [RaftManager id=6] Completed transition to Unattached(epoch=515, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=514, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:48,993] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:49,632] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:48:49,639] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=517, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2196, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=515, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:06,572] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:06,986] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:07,032] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:07,444] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2566 due to node 3 being disconnected (elapsed time since creation: 2522ms, elapsed time since send: 2216ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:11,321] INFO [RaftManager id=6] Completed transition to Unattached(epoch=520, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=517, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2257, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:11,330] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:12,214] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:49:17,127] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:49:18,507] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:19,059] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2570 due to node 3 being disconnected (elapsed time since creation: 2698ms, elapsed time since send: 2116ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:23,328] INFO [RaftManager id=6] Completed transition to Unattached(epoch=526, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=520, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:23,441] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:49:24,754] INFO [RaftManager id=6] Completed transition to Unattached(epoch=527, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=526, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:26,743] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=527, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2257, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=527, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:26,849] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:38,266] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:38,560] INFO [RaftManager id=6] Completed transition to Unattached(epoch=529, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=527, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2274, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:41,970] INFO [RaftManager id=6] Completed transition to Unattached(epoch=530, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=529, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:43,412] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:49:45,247] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:45,526] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=531, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2274, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=530, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:50,760] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:54,480] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:49:58,122] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:58,315] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:58,671] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight API_VERSIONS request with correlation id 637 due to node 3 being disconnected (elapsed time since creation: 7812ms, elapsed time since send: 7812ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:58,689] INFO [Broker id=6] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:50:01,180] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2586 due to node 2 being disconnected (elapsed time since creation: 3171ms, elapsed time since send: 2993ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:03,353] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:03,688] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:03,746] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:03,959] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,004] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:04,047] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:04,050] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,053] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:04,062] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:04,062] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:04,063] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:04,063] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,063] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:04,063] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,079] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 2 from offset 2 with partition epoch 8 and high watermark 2. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,079] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,082] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:04,083] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,091] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,093] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,093] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,093] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,094] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,097] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 3 from offset 1 with partition epoch 8 and high watermark 1. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,100] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,101] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,104] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:04,107] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,110] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,113] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,118] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,124] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,126] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,127] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:04,127] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:04,127] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,136] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:04,139] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:04,139] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,141] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,141] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,143] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,150] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:04,152] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,153] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,154] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,154] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,155] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,155] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,157] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:04,159] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:04,168] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:04,169] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:04,208] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:50:04,227] INFO [Broker id=6] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2026-01-16 08:50:04,676] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:06,323] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:07,984] INFO [RaftManager id=6] Completed transition to Unattached(epoch=535, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=531, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2353, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:08,493] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:09,269] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:15,511] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:15,914] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:16,782] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2593 due to node 1 being disconnected (elapsed time since creation: 3121ms, elapsed time since send: 2699ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:17,185] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:17,231] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-13 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-46 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-9 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-42 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-21 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-17 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-30 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-26 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-5 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-38 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-1 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-34 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-16 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), _schemas-0 -> InitialFetchState(Some(UjN16IrSRlipZQeCwkGrdQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,2), __consumer_offsets-45 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-12 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-41 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-24 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-20 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-49 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-0 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-29 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,1), __consumer_offsets-25 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-8 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-37 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-4 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-33 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-15 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-48 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-11 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-44 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-23 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-19 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-32 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-28 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-7 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-40 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-3 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-36 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-47 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-14 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-43 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-10 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-22 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-18 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-31 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-27 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-39 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-6 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-35 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-2 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:50:17,314] INFO [Broker id=6] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2026-01-16 08:50:17,247] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:17,997] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,278] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,369] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,409] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,462] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,488] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,491] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,492] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,505] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,512] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,518] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,529] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,530] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,533] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,563] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,602] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,615] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,658] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,726] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,728] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,730] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,730] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,731] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,731] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,731] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,731] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,731] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,731] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,731] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,733] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,735] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,744] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,846] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,847] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,847] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,848] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,883] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,891] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,896] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,934] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,943] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,950] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,955] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,958] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,962] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,987] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,988] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,989] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,989] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:18,990] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:50:18,990] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:50:19,494] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:19,749] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:19,650] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:19,812] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Connection to node 5 (kafka-broker-2/172.24.0.16:19092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:19,857] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:19,862] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:19,854] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,036] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:19,948] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,041] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={__consumer_offsets-13=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-46=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-9=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-42=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-21=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-17=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-30=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-26=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-5=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-38=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-1=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-34=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-16=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-45=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-12=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-41=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-24=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-20=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-49=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-0=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-29=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=1, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-25=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-8=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-37=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-4=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-33=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-15=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-48=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-11=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-44=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-23=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-19=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-32=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-28=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-7=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-40=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-3=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-36=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-47=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-14=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-43=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-10=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-22=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-18=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-31=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-27=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-39=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-6=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-35=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-2=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), _schemas-0=PartitionData(topicId=UjN16IrSRlipZQeCwkGrdQ, fetchOffset=2, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0])}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=INVALID, epoch=INITIAL), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:20,077] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,103] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,105] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,103] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,111] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,124] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,140] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,140] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,141] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,142] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,142] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,168] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,172] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,173] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,174] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,177] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,178] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,178] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,178] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,174] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,179] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,182] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,178] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,183] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,184] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,185] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,183] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,193] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,198] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,186] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,226] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,231] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,263] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,341] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,367] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,226] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,372] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,373] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,376] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,374] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,378] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,378] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,383] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,383] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,384] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,384] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,386] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,385] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,390] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,388] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,395] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,395] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,395] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,395] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,396] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,396] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,397] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,398] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,398] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,399] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,400] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,402] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,400] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,406] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,406] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,413] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,413] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,414] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,414] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,416] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,418] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,420] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,424] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,425] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,425] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,426] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,426] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,428] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,432] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,436] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,437] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,437] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,438] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,440] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,441] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,443] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,446] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,449] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,450] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,453] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,453] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,453] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,453] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,453] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,455] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,455] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,457] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,408] INFO [RaftManager id=6] Completed transition to Unattached(epoch=543, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=535, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:20,459] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,526] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,528] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,531] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:20,589] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:20,414] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,317] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:21,414] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,425] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:21,425] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,439] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:21,442] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,454] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:21,458] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,458] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,462] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,463] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,466] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,462] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:21,475] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,476] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:21,477] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,483] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:21,503] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,504] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:50:21,504] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,476] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,504] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,505] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,505] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,506] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,507] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,509] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,533] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,551] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,559] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,583] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,588] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,513] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:21,597] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Connection to node 5 (kafka-broker-2/172.24.0.16:19092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:21,612] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:21,613] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:21,597] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,673] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,705] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,710] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,723] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,731] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,682] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={__consumer_offsets-13=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-46=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-9=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-42=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-21=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-17=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-30=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-26=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-5=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-38=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-1=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-34=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-16=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), _schemas-0=PartitionData(topicId=UjN16IrSRlipZQeCwkGrdQ, fetchOffset=2, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-45=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-12=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-41=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-24=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-20=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-49=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-0=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-29=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=1, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-25=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-8=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-37=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-4=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-33=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-15=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-48=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-11=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-44=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-23=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-19=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-32=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-28=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-7=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-40=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-3=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-36=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-47=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-14=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-43=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-10=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-22=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-18=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-31=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-27=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-39=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-6=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-35=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-2=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty)}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=INVALID, epoch=INITIAL), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:21,740] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,786] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:21,786] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:50:22,244] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=543, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2353, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=543, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:22,492] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:22,905] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:22,916] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Connection to node 5 (kafka-broker-2/172.24.0.16:19092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:22,917] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:22,918] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:23,016] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={__consumer_offsets-13=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-46=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-9=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-42=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-21=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-17=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-30=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-26=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-5=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-38=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-1=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-34=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-16=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), _schemas-0=PartitionData(topicId=UjN16IrSRlipZQeCwkGrdQ, fetchOffset=2, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-45=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-12=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-41=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-24=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-20=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-49=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-0=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-29=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=1, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-25=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-8=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-37=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-4=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-33=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-15=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-48=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-11=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-44=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-23=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-19=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-32=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-28=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-7=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-40=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-3=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-36=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-47=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-14=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-43=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-10=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-22=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-18=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-31=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-27=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-39=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-6=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-35=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-2=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty)}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=INVALID, epoch=INITIAL), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:23,231] INFO [RaftManager id=6] Completed transition to Unattached(epoch=544, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=543, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2368, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:24,804] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:24,857] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:24,875] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Connection to node 5 (kafka-broker-2/172.24.0.16:19092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:24,886] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:24,894] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:24,900] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:25,072] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={__consumer_offsets-13=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-46=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-9=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-42=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-21=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-17=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-30=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-26=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-5=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-38=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-1=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-34=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-16=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), _schemas-0=PartitionData(topicId=UjN16IrSRlipZQeCwkGrdQ, fetchOffset=2, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-45=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-12=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-41=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-24=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-20=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-49=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-0=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-29=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=1, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-25=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-8=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-37=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-4=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-33=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-15=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-48=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-11=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-44=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-23=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-19=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-32=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-28=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-7=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-40=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-3=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-36=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-47=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-14=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-43=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-10=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-22=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-18=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-31=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-27=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-39=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-6=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-35=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-2=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty)}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=INVALID, epoch=INITIAL), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:27,501] INFO [RaftManager id=6] Completed transition to Unattached(epoch=547, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=544, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:28,544] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:29,090] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Connection to node 5 (kafka-broker-2/172.24.0.16:19092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:31,510] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:32,544] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:32,808] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={__consumer_offsets-13=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-46=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-9=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-42=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-21=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-17=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-30=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-26=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-5=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-38=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-1=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-34=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-16=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), _schemas-0=PartitionData(topicId=UjN16IrSRlipZQeCwkGrdQ, fetchOffset=2, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-45=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-12=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-41=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-24=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-20=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-49=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-0=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-29=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=1, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-25=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-8=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-37=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-4=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-33=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-15=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-48=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-11=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-44=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-23=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-19=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-32=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-28=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-7=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-40=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-3=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-36=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-47=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-14=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-43=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-10=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-22=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-18=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-31=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-27=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-39=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-6=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-35=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-2=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty)}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=INVALID, epoch=INITIAL), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:33,590] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:35,060] INFO [RaftManager id=6] Completed transition to Unattached(epoch=550, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=547, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:35,534] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:35,539] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Connection to node 5 (kafka-broker-2/172.24.0.16:19092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:35,545] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:35,562] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:36,125] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={__consumer_offsets-13=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-46=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-9=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-42=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-21=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-17=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-30=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-26=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-5=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-38=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-1=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-34=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-16=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), _schemas-0=PartitionData(topicId=UjN16IrSRlipZQeCwkGrdQ, fetchOffset=2, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-45=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-12=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-41=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-24=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-20=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-49=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-0=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-29=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=1, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-25=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-8=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-37=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-4=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-33=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-15=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-48=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-11=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-44=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-23=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-19=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-32=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-28=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-7=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-40=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-3=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-36=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-47=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-14=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-43=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-10=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-22=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-18=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-31=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-27=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-39=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-6=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-35=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-2=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty)}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=INVALID, epoch=INITIAL), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:50:39,027] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:39,271] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=551, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2368, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=550, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:40,952] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:41,372] INFO [RaftManager id=6] Completed transition to Unattached(epoch=553, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=551, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2368, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:44,560] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:45,364] INFO [RaftManager id=6] Completed transition to Unattached(epoch=556, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=553, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:46,964] INFO [RaftManager id=6] Completed transition to Unattached(epoch=557, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=556, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:48,178] INFO [RaftManager id=6] Completed transition to Unattached(epoch=558, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=557, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:48,815] INFO [RaftManager id=6] Completed transition to Unattached(epoch=559, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=558, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:49,009] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=559, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2368, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=559, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:49,046] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:51:10,167] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Disconnecting from node 5 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:51:12,098] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 0 due to node 5 being disconnected (elapsed time since creation: 30932ms, elapsed time since send: 30932ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:51:12,254] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:51:12,792] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:51:13,713] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={__consumer_offsets-13=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-46=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-9=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-42=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-21=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-17=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-30=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-26=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-5=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-38=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-1=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-34=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-16=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), _schemas-0=PartitionData(topicId=UjN16IrSRlipZQeCwkGrdQ, fetchOffset=2, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-45=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-12=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-41=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-24=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-20=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-49=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-0=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-29=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=1, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-25=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-8=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-37=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-4=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-33=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-15=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-48=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-11=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-44=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-23=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-19=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-32=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-28=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-7=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-40=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-3=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-36=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-47=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-14=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[1]), __consumer_offsets-43=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-10=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty), __consumer_offsets-22=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-18=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-31=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[0]), __consumer_offsets-27=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-39=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-6=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional.empty), __consumer_offsets-35=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[2]), __consumer_offsets-2=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional.empty)}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=INVALID, epoch=INITIAL), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:51:27,901] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:51:28,628] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,102] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:29,136] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:29,227] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,009] INFO [Broker id=6] Transitioning 50 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:51:30,033] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,054] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:30,058] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:30,060] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,063] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:30,065] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:30,069] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:30,073] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:30,075] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,082] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:30,098] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,101] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,105] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,111] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:30,117] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,122] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,124] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,130] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,131] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,133] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,133] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,173] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,184] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,193] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:30,218] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,220] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,220] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,242] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,244] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,245] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,264] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:30,265] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:30,267] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,269] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:30,276] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:30,285] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,295] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,313] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,315] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,318] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:30,322] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,323] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,331] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,332] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,335] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,345] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,346] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:30,350] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,356] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:30,357] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,824] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,845] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,846] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,848] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,848] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,849] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,849] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,849] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,850] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,872] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,886] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,934] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,941] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,943] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,872] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,969] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,971] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,991] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,968] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,997] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,997] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,007] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,002] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,023] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:31,023] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,027] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,035] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:31,046] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,046] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,066] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,066] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:31,083] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,084] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:31,161] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,201] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:31,202] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,209] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:31,284] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,084] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,331] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,342] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:31,343] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,356] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:31,384] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,356] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,483] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:31,488] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,014] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,726] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,731] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,789] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,790] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,797] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,018] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,810] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,810] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,811] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,810] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,812] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,817] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,818] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,817] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,821] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,820] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,822] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,823] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,825] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,823] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,839] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,839] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,843] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,844] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,844] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,845] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,845] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,845] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,845] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,846] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,845] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,846] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,849] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,850] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,849] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,851] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,860] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,865] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,858] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,868] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,867] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,868] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,868] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,868] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,869] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,869] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,869] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,869] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,869] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,869] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,870] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,870] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,870] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,871] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,876] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,872] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,879] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,879] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,880] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,880] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,880] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,880] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,880] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,880] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,880] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,881] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,881] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,886] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,887] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,889] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,888] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,894] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,889] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,897] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,900] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,916] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,914] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,920] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,920] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,923] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,923] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,924] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,924] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,925] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,925] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,925] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,928] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,927] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,937] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,938] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,938] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,938] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,939] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,939] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,940] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,941] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,941] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,941] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,943] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,414] INFO [Broker id=6] Transitioning 24 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:51:33,502] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,529] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,562] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,566] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,569] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:33,583] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,602] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:33,660] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,668] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,674] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,682] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:33,685] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,692] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:33,697] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,699] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,706] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,706] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:33,707] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,713] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:33,717] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:33,718] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:33,719] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,720] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:33,726] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,743] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,750] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,755] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,759] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,765] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,781] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,785] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,793] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,798] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,816] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,822] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,823] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,827] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,827] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,828] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,801] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,828] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,829] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,831] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,828] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,837] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,838] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,840] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,838] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,846] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,846] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,852] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,854] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,866] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,867] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,867] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,867] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,877] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,869] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,885] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,890] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,890] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,891] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,891] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,891] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,891] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,891] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,892] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,893] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,908] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,910] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,909] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,920] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,919] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,923] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,923] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,932] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,923] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,946] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,948] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,953] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,955] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,956] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,960] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,958] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,964] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,965] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,979] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,997] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,998] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,000] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,000] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:33,997] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,001] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,001] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,006] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,019] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,148] INFO [Broker id=6] Transitioning 27 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:51:34,165] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:34,174] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:34,214] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:34,903] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:34,969] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:34,985] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:34,987] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:34,992] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,014] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,303] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,322] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,342] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,455] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,481] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,493] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,522] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,526] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,548] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,572] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,604] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:35,630] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,674] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:35,703] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,717] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:35,755] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,780] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,820] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,936] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:35,960] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:35,968] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:35,969] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:35,970] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:35,977] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:35,992] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,009] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,014] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,028] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,011] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,030] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,041] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,045] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,051] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,030] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,052] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,053] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,057] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,058] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,057] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,063] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,059] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,066] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,067] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,066] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,072] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,105] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,118] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,154] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,173] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,184] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,186] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,155] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,189] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,189] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,189] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,189] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,189] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,189] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,189] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,192] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,189] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,198] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,205] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,201] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,215] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,225] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,229] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,238] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,240] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,240] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,241] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,242] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,245] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,244] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,248] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,254] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,259] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,260] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,260] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,260] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,266] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,266] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,268] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,269] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,271] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,273] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,271] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,276] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,275] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,279] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,285] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,286] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,287] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,300] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,303] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,307] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:54:00,183] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Disconnecting from node 5 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:03,392] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:04,154] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:04,175] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 13 due to node 5 being disconnected (elapsed time since creation: 124974ms, elapsed time since send: 124974ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:04,327] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2742 due to node 3 being disconnected (elapsed time since creation: 6881ms, elapsed time since send: 5249ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:10,428] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:54:10,516] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:10,850] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:11,153] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=798713224, epoch=12) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:54:12,224] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=798713224, epoch=12), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:54:13,627] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:14,373] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:54:20,581] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:21,181] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight API_VERSIONS request with correlation id 677 due to node 2 being disconnected (elapsed time since creation: 4610ms, elapsed time since send: 4610ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:21,273] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:54:21,368] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:25,359] INFO [RaftManager id=6] Completed transition to Unattached(epoch=563, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=559, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2597, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:54:28,724] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:33,235] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:34,792] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:35,535] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2748 due to node 1 being disconnected (elapsed time since creation: 4120ms, elapsed time since send: 3625ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:36,321] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:36,432] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2749 due to node 3 being disconnected (elapsed time since creation: 2103ms, elapsed time since send: 2058ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:40,085] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:40,401] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:40,540] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2750 due to node 2 being disconnected (elapsed time since creation: 7122ms, elapsed time since send: 3451ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:47,862] INFO [RaftManager id=6] Completed transition to Unattached(epoch=580, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=563, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:54:48,807] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:50,219] INFO [RaftManager id=6] Completed transition to Unattached(epoch=581, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=580, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:54:51,933] INFO [RaftManager id=6] Completed transition to Unattached(epoch=583, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=581, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:54:53,015] INFO [RaftManager id=6] Completed transition to Unattached(epoch=584, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=583, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:54:55,149] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:55,482] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2762 due to node 2 being disconnected (elapsed time since creation: 2011ms, elapsed time since send: 2010ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:56,886] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:55:00,409] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:00,437] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2763 due to node 2 being disconnected (elapsed time since creation: 4996ms, elapsed time since send: 3884ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:02,989] INFO [RaftManager id=6] Completed transition to Unattached(epoch=589, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=584, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:04,459] INFO [RaftManager id=6] Completed transition to Unattached(epoch=590, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=589, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:06,139] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=590, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2597, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=590, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:06,237] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:09,094] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:09,323] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:09,326] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:55:12,000] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:12,187] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2772 due to node 3 being disconnected (elapsed time since creation: 2283ms, elapsed time since send: 2074ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:14,158] INFO [RaftManager id=6] Completed transition to Unattached(epoch=593, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=590, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2597, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:14,554] INFO [RaftManager id=6] Completed transition to Unattached(epoch=595, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=593, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:18,650] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:18,662] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=595, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2597, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=595, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:18,850] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:19,703] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:19,750] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:19,825] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:19,843] INFO [RaftManager id=6] Completed transition to Unattached(epoch=599, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=595, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2597, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:19,929] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:22,889] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:55:25,179] INFO [RaftManager id=6] Completed transition to Unattached(epoch=600, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=599, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:27,063] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:27,264] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=603, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2597, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=600, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:32,479] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:55:33,692] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5], partitionEpoch=11, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:55:33,850] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:55:34,046] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:55:34,951] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:55:37,516] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:38,330] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:38,443] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:56:02,188] INFO [RaftManager id=6] Completed transition to Unattached(epoch=607, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=603, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2619, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:07,128] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:56:12,174] INFO [RaftManager id=6] Completed transition to Unattached(epoch=612, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=607, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:23,388] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:56:27,153] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:29,377] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2799 due to node 1 being disconnected (elapsed time since creation: 4038ms, elapsed time since send: 3044ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:39,427] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:40,036] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2801 due to node 2 being disconnected (elapsed time since creation: 10737ms, elapsed time since send: 7844ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:42,803] INFO [RaftManager id=6] Completed transition to Unattached(epoch=623, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=612, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:45,665] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:56:50,323] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:50,460] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2803 due to node 3 being disconnected (elapsed time since creation: 4157ms, elapsed time since send: 3833ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:52,673] INFO [RaftManager id=6] Completed transition to Unattached(epoch=630, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=623, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:59,987] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:57:04,846] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:06,048] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2812 due to node 2 being disconnected (elapsed time since creation: 3180ms, elapsed time since send: 2953ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:10,858] INFO [RaftManager id=6] Completed transition to Unattached(epoch=637, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=630, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:14,059] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:14,758] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2817 due to node 3 being disconnected (elapsed time since creation: 2371ms, elapsed time since send: 2320ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:15,699] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:57:17,858] INFO [RaftManager id=6] Completed transition to Unattached(epoch=641, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=637, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:18,716] INFO [RaftManager id=6] Completed transition to Unattached(epoch=642, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=641, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:20,290] INFO [RaftManager id=6] Completed transition to Unattached(epoch=643, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=642, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:21,574] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=644, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2619, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=643, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:21,637] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:57:22,601] INFO [RaftManager id=6] Completed transition to Unattached(epoch=645, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=644, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2619, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:23,046] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=645, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2619, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=645, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:26,008] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:26,223] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:57:26,868] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:26,871] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:57:27,035] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:57:28,878] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=647, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2619, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=645, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2619, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:29,936] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:30,142] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:57:30,204] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:57:32,360] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:32,674] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2829 due to node 1 being disconnected (elapsed time since creation: 2169ms, elapsed time since send: 2159ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:35,038] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:35,436] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 2830 due to node 3 being disconnected (elapsed time since creation: 3071ms, elapsed time since send: 2253ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:37,813] INFO [RaftManager id=6] Completed transition to Unattached(epoch=651, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=647, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2619, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:38,373] INFO [RaftManager id=6] Completed transition to Unattached(epoch=652, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=651, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:39,487] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:39,672] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=652, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2619, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=652, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:40,317] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:57:41,052] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:42,141] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:57:43,246] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:43,386] INFO [RaftManager id=6] Completed transition to Unattached(epoch=656, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=652, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2619, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:44,168] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:57:44,735] INFO [RaftManager id=6] Completed transition to Unattached(epoch=657, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=656, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:45,543] INFO [RaftManager id=6] Completed transition to Unattached(epoch=658, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=657, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:46,209] INFO [RaftManager id=6] Completed transition to Unattached(epoch=659, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=658, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:46,916] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:57:46,938] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=659, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2619, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=659, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:49,710] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:57:50,207] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=12, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:57:50,265] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:57:50,314] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:57:50,736] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:57:50,952] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:57:51,016] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:57:51,055] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:57:51,058] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:57:51,060] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:58:24,422] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:58:25,046] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:58:27,947] INFO [RaftManager id=6] Completed transition to Unattached(epoch=660, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=659, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2698, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:58:29,935] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=663, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2698, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=660, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:58:37,888] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:58:39,835] INFO [RaftManager id=6] Completed transition to Unattached(epoch=664, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=663, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2715, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:58:40,065] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:58:40,332] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=666, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2715, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=664, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:58:40,452] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:59:09,970] INFO [RaftManager id=6] Completed transition to Unattached(epoch=667, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=666, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2761, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:59:10,007] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:59:11,559] INFO [RaftManager id=6] Completed transition to Unattached(epoch=671, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=667, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:59:12,100] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=674, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2761, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=671, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:59:12,142] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:59:14,307] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:59:14,635] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:59:14,678] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:59:19,916] INFO [RaftManager id=6] Completed transition to Unattached(epoch=678, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=674, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2770, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:59:20,321] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:59:21,100] INFO [RaftManager id=6] Completed transition to Unattached(epoch=680, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=678, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:59:21,394] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:59:21,437] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=680, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2770, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=680, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:59:21,528] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:00:59,661] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:05,135] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:05,334] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:05,465] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:01:05,629] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 3101 due to node 1 being disconnected (elapsed time since creation: 6614ms, elapsed time since send: 6572ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:06,965] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:07,176] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:07,214] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:07,459] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:07,476] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:07,537] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:08,981] INFO [RaftManager id=6] Completed transition to Unattached(epoch=685, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=680, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2943, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:09,274] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:09,918] INFO [RaftManager id=6] Completed transition to Unattached(epoch=688, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=685, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:11,012] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:01:13,559] INFO [RaftManager id=6] Completed transition to Unattached(epoch=691, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=688, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:14,571] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=692, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2943, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=691, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:14,682] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:17,040] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=693, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2945, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=692, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2945, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:19,776] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:19,921] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:01:36,678] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:37,389] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:38,364] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:38,397] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:38,455] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:38,485] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:38,595] INFO [RaftManager id=6] Completed transition to Unattached(epoch=695, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=693, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2975, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:39,053] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=697, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2975, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=695, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:39,522] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:28,369] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:28,549] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 3335 due to node 2 being disconnected (elapsed time since creation: 2053ms, elapsed time since send: 2008ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:29,340] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:29,365] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:29,412] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:29,449] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:29,450] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:29,501] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:29,559] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:29,572] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:29,612] INFO [RaftManager id=6] Completed transition to Unattached(epoch=698, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=697, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3182, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:29,808] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=699, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3182, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=698, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:29,827] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:34,657] INFO [RaftManager id=6] Completed transition to Unattached(epoch=701, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=699, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3186, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:34,792] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=702, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3186, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=701, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:35,126] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:35,333] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:39,148] WARN [BrokerLifecycleManager id=6] Broker 6 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:03:39,561] INFO [RaftManager id=6] Completed transition to Unattached(epoch=703, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=702, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3187, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:39,774] INFO [RaftManager id=6] Completed transition to Unattached(epoch=704, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=703, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:40,006] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:40,118] INFO [RaftManager id=6] Completed transition to Unattached(epoch=705, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=704, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:40,484] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=705, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3187, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=705, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:40,494] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:57,383] WARN [BrokerLifecycleManager id=6] Broker 6 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:03:57,785] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:57,838] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:57,879] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:58,357] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:58,375] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:58,431] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:58,505] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:58,520] INFO [RaftManager id=6] Completed transition to Unattached(epoch=706, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=705, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3212, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:59,374] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=707, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3212, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=706, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:59,463] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:06:29,121] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:06:29,248] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 3669 due to node 2 being disconnected (elapsed time since creation: 2019ms, elapsed time since send: 2018ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:06:30,065] INFO [RaftManager id=6] Completed transition to Unattached(epoch=708, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=707, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3488, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:30,107] INFO [RaftManager id=6] Completed transition to Unattached(epoch=709, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=708, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:30,189] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:06:30,497] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=709, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3488, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=709, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:30,508] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:06:31,766] INFO [RaftManager id=6] Completed transition to Unattached(epoch=710, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=709, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3488, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:32,135] INFO [RaftManager id=6] Completed transition to Unattached(epoch=711, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=710, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:32,432] INFO [RaftManager id=6] Completed transition to Unattached(epoch=712, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=711, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:32,927] INFO [RaftManager id=6] Completed transition to Unattached(epoch=713, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=712, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:33,713] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=713, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3488, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=713, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:34,043] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:06:34,046] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:30,444] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:30,528] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:30,583] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:31,481] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:31,662] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 4320 due to node 3 being disconnected (elapsed time since creation: 2001ms, elapsed time since send: 2001ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:31,668] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:31,671] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:31,724] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:31,733] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:31,737] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:31,789] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:31,863] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:31,864] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:31,918] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:31,984] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:31,987] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:32,059] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:32,082] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:32,090] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:32,142] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:32,200] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:32,203] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:32,254] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:32,298] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:32,301] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:32,339] INFO [RaftManager id=6] Completed transition to Unattached(epoch=716, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=713, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4054, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:11:32,367] INFO [RaftManager id=6] Completed transition to Unattached(epoch=717, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=716, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:11:32,974] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=717, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4054, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=717, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:11:33,001] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:33,276] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:33,294] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:33,345] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:33,458] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:33,462] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:33,513] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:33,537] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:33,547] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:33,632] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:14:26,687] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000004384-0000000717 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 09:14:36,425] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000004384-0000000717 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 09:16:34,853] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2026-01-16 09:16:35,235] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-9, __consumer_offsets-21, __consumer_offsets-20, __consumer_offsets-17, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-3, __consumer_offsets-35, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2026-01-16 09:16:35,499] INFO [Broker id=6] Leader __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:35,543] INFO [Broker id=6] Leader __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:35,580] INFO [Broker id=6] Leader __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:35,619] INFO [Broker id=6] Leader __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:35,654] INFO [Broker id=6] Leader __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:35,700] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:35,738] INFO [Broker id=6] Leader __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:35,763] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:35,794] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:35,845] INFO [Broker id=6] Leader __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:35,900] INFO [Broker id=6] Leader __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:35,930] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:36,042] INFO [Broker id=6] Leader __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:36,151] INFO [Broker id=6] Leader __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:36,322] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:36,414] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:36,505] INFO [Broker id=6] Transitioning 17 partition(s) to local followers. (state.change.logger)
[2026-01-16 09:16:36,592] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,664] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,680] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:36,680] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,680] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,680] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:36,680] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:36,681] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,681] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:36,682] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 4 from offset 8 with partition epoch 14 and high watermark 8. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,694] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:36,696] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:36,696] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,697] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 09:16:36,713] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,715] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,727] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,037] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-46, __consumer_offsets-42, __consumer_offsets-10, __consumer_offsets-23, __consumer_offsets-24, __consumer_offsets-19, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 09:16:37,047] INFO [Broker id=6] Stopped fetchers as part of become-follower for 17 partitions (state.change.logger)
[2026-01-16 09:16:38,921] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:39,041] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,0), __consumer_offsets-48 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,0), __consumer_offsets-14 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,0), __consumer_offsets-46 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,0), __consumer_offsets-42 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,0), __consumer_offsets-10 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,0), __consumer_offsets-23 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,0), __consumer_offsets-24 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,0), __consumer_offsets-19 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,0), __consumer_offsets-29 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,8), __consumer_offsets-30 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,0), __consumer_offsets-7 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,0), __consumer_offsets-40 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,0), __consumer_offsets-5 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,0), __consumer_offsets-38 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,0), __consumer_offsets-33 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,0), __consumer_offsets-2 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 09:16:39,150] INFO [Broker id=6] Started fetchers as part of become-follower for 17 partitions (state.change.logger)
[2026-01-16 09:16:39,160] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:39,324] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:39,373] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:39,383] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:39,391] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:39,404] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:39,417] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:39,419] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:39,420] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:39,421] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:39,421] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:39,426] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:39,428] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:39,430] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:39,436] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:39,440] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:39,440] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:39,445] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:39,821] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 45 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,836] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,842] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 43 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,857] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,871] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 12 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,872] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,874] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 9 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,874] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,893] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 21 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,895] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,898] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,899] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,903] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 17 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,912] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,917] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,921] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,927] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,929] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,942] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 26 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,944] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,947] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 39 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,951] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,958] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,968] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,978] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 37 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,979] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,985] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 3 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,986] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,987] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,990] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,991] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,991] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,023] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,036] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,037] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,047] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,048] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,052] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,029] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-45 in 190 milliseconds for epoch 6, of which 63 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,060] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-43 in 189 milliseconds for epoch 4, of which 189 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,064] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-12 in 190 milliseconds for epoch 4, of which 189 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,057] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,067] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,080] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,081] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,082] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,083] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,069] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-9 in 177 milliseconds for epoch 6, of which 176 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,088] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-21 in 190 milliseconds for epoch 4, of which 189 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,087] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,100] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,106] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,108] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,108] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 198 milliseconds for epoch 4, of which 197 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,116] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-17 in 199 milliseconds for epoch 6, of which 199 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,121] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 193 milliseconds for epoch 4, of which 193 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,118] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,129] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,130] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,136] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,136] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,137] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,137] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,137] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,148] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,151] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,152] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,160] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,162] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,171] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,172] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,173] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,174] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:40,175] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,184] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 242 milliseconds for epoch 6, of which 241 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,204] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-26 in 257 milliseconds for epoch 6, of which 252 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,206] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-39 in 248 milliseconds for epoch 6, of which 247 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,207] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 229 milliseconds for epoch 4, of which 228 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,209] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-37 in 224 milliseconds for epoch 6, of which 224 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,213] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-3 in 226 milliseconds for epoch 4, of which 226 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,218] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 228 milliseconds for epoch 6, of which 228 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,220] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 223 milliseconds for epoch 6, of which 222 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,241] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,257] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,272] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,274] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,276] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,283] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,293] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,315] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,325] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,331] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,334] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,343] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,345] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,359] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,361] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,372] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:40,380] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:21:33,546] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:21:33,914] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:21:34,837] INFO [RaftManager id=6] Completed transition to Unattached(epoch=718, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=717, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5238, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:21:34,974] INFO [RaftManager id=6] Completed transition to Unattached(epoch=720, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=718, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:21:35,166] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:21:35,555] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=720, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5238, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=720, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:21:35,606] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:21:36,518] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:21:36,598] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:21:36,676] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:21:36,689] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:21:36,773] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:21:36,950] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:21:36,959] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:21:37,147] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:22:45,929] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:22:46,146] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 5733 due to node 2 being disconnected (elapsed time since creation: 2050ms, elapsed time since send: 2050ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:22:49,685] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:22:49,798] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:22:50,104] INFO [RaftManager id=6] Completed transition to Unattached(epoch=721, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=720, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5368, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:22:50,177] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=722, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5368, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=721, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:22:50,315] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:22:50,317] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:24:44,188] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:10,586] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:10,950] INFO [RaftManager id=6] Completed transition to Unattached(epoch=723, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=722, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5753, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:11,111] INFO [RaftManager id=6] Completed transition to Unattached(epoch=724, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=723, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:12,544] INFO [RaftManager id=6] Completed transition to Unattached(epoch=726, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=724, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:13,114] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:13,116] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=726, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5753, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=726, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:38,139] INFO [RaftManager id=6] Completed transition to Unattached(epoch=727, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=726, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5795, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:38,501] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=729, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5795, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=727, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:40,123] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:40,520] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:42,823] INFO [RaftManager id=6] Completed transition to Unattached(epoch=730, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=729, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5799, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:43,106] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:43,268] INFO [RaftManager id=6] Completed transition to Unattached(epoch=732, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=730, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:43,557] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=732, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5799, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=732, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:43,588] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:30:03,895] INFO [RaftManager id=6] Completed transition to Unattached(epoch=733, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=732, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6184, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:30:03,921] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:30:03,996] INFO [RaftManager id=6] Completed transition to Unattached(epoch=735, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=733, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:30:05,632] INFO [RaftManager id=6] Completed transition to Unattached(epoch=737, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=735, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:30:05,899] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=737, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6184, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=737, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:30:05,946] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:37:53,719] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:37:53,923] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 7595 due to node 2 being disconnected (elapsed time since creation: 2005ms, elapsed time since send: 2005ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:37:54,252] INFO [RaftManager id=6] Completed transition to Unattached(epoch=738, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=737, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=7105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:37:54,772] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:37:54,825] INFO [RaftManager id=6] Completed transition to Unattached(epoch=739, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=738, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:37:55,347] INFO [RaftManager id=6] Completed transition to Unattached(epoch=740, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=739, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:37:55,467] INFO [RaftManager id=6] Completed transition to Unattached(epoch=741, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=740, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:37:56,288] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=741, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=7105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=741, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:37:56,350] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:47:56,053] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:47:56,274] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:14:26,148] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000011440-0000000741 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 10:14:26,665] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000011440-0000000741 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 10:26:36,797] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:26:37,255] INFO [RaftManager id=6] Completed transition to Unattached(epoch=742, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=741, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=12885, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:37,390] INFO [RaftManager id=6] Completed transition to Unattached(epoch=743, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=742, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:37,772] INFO [RaftManager id=6] Completed transition to Unattached(epoch=744, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=743, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:38,059] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=744, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=12885, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=744, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:38,156] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:26:38,276] INFO [RaftManager id=6] Completed transition to Unattached(epoch=745, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=744, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=12885, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:38,535] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:26:39,202] INFO [RaftManager id=6] Completed transition to Unattached(epoch=746, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=745, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:39,767] INFO [RaftManager id=6] Completed transition to Unattached(epoch=747, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=746, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:39,976] INFO [RaftManager id=6] Completed transition to Unattached(epoch=748, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=747, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:40,012] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=748, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=12885, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=748, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:40,034] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:26:40,102] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:26:40,104] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:26:40,169] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:36:40,220] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:36:40,453] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:44:17,221] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:44:17,425] INFO [RaftManager id=6] Completed transition to Unattached(epoch=749, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=748, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14977, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:18,237] INFO [RaftManager id=6] Completed transition to Unattached(epoch=750, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=749, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:18,347] INFO [RaftManager id=6] Completed transition to Unattached(epoch=751, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=750, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:18,474] INFO [RaftManager id=6] Completed transition to Unattached(epoch=752, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=751, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:19,372] INFO [RaftManager id=6] Completed transition to Unattached(epoch=753, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=752, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:19,626] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=753, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14977, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=753, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:19,650] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:54:19,386] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:54:19,559] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:14:26,590] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000018569-0000000753 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 11:14:26,975] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000018569-0000000753 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 11:25:56,517] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:25:56,669] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 21024 due to node 2 being disconnected (elapsed time since creation: 3056ms, elapsed time since send: 3053ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:25:58,470] INFO [RaftManager id=6] Completed transition to Unattached(epoch=755, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=753, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=19938, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:25:58,600] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=755, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=19938, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=755, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:25:58,604] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:25:58,638] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:25:59,034] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:25:59,035] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:25:59,114] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:40,451] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:40,880] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 22092 due to node 1 being disconnected (elapsed time since creation: 2017ms, elapsed time since send: 2017ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:42,067] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:42,131] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:42,594] INFO [RaftManager id=6] Completed transition to Unattached(epoch=759, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=755, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:34:42,923] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=759, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=759, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:34:44,315] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:44,475] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:44,587] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 11:34:54,214] WARN [BrokerLifecycleManager id=6] Broker 6 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 11:34:56,527] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:56,724] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:56,743] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:56,783] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 22117 due to node 2 being disconnected (elapsed time since creation: 2290ms, elapsed time since send: 2290ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:57,609] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:57,645] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:57,727] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:57,770] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:57,773] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:57,833] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:57,899] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:57,900] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:57,954] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,013] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:58,015] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,066] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,094] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:58,104] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,161] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,184] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:58,188] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,251] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,264] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:58,266] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,341] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,490] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:58,495] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,552] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,594] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:58,613] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,673] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,741] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:58,742] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:58,827] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:35:01,937] INFO [RaftManager id=6] Completed transition to Unattached(epoch=762, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=759, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20984, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:04,320] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 11:35:05,348] INFO [RaftManager id=6] Completed transition to Unattached(epoch=763, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=762, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:09,403] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:09,696] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 22123 due to node 3 being disconnected (elapsed time since creation: 2224ms, elapsed time since send: 2049ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:09,990] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 11:35:10,488] INFO [RaftManager id=6] Completed transition to Unattached(epoch=768, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=763, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:13,412] INFO [RaftManager id=6] Completed transition to Unattached(epoch=769, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=768, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:13,587] INFO [RaftManager id=6] Completed transition to Unattached(epoch=770, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=769, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:14,479] INFO [RaftManager id=6] Completed transition to Unattached(epoch=771, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=770, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:14,658] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=771, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20984, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=771, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:14,738] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:35:14,771] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:14,784] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:35:14,835] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:35:19,146] INFO [RaftManager id=6] Completed transition to Unattached(epoch=772, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=771, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20989, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:19,311] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:19,966] INFO [RaftManager id=6] Completed transition to Unattached(epoch=773, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=772, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:20,322] INFO [RaftManager id=6] Completed transition to Unattached(epoch=774, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=773, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:21,175] INFO [RaftManager id=6] Completed transition to Unattached(epoch=775, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=774, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:22,020] INFO [RaftManager id=6] Completed transition to Unattached(epoch=776, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=775, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:22,219] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=776, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20989, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=776, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:22,245] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:45:21,620] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:45:22,282] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:14:26,996] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000025642-0000000776 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 12:14:27,143] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000025642-0000000776 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 12:18:10,337] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:10,510] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:22,617] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 6648 due to node 1 being disconnected (elapsed time since creation: 4540ms, elapsed time since send: 4540ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:22,719] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27427 due to node 1 being disconnected (elapsed time since creation: 2106ms, elapsed time since send: 2106ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:30,693] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:18:31,487] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:18:37,493] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:37,721] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 27430 due to node 1 being disconnected (elapsed time since creation: 2615ms, elapsed time since send: 2615ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:38,820] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:38,835] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27432 due to node 2 being disconnected (elapsed time since creation: 3419ms, elapsed time since send: 2355ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:39,440] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:18:39,472] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:18:42,084] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:42,457] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27433 due to node 1 being disconnected (elapsed time since creation: 4660ms, elapsed time since send: 3074ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:42,836] WARN [BrokerLifecycleManager id=6] Broker 6 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:18:43,476] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:43,516] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:18:43,573] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:18:44,724] INFO [RaftManager id=6] Completed transition to Unattached(epoch=779, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=776, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:46,129] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:46,479] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=780, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=779, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:46,656] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:18:46,757] INFO [RaftManager id=6] Completed transition to Unattached(epoch=782, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=780, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:47,550] INFO [RaftManager id=6] Completed transition to Unattached(epoch=783, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=782, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:48,215] INFO [RaftManager id=6] Completed transition to Unattached(epoch=784, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=783, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:48,370] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:48,376] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:18:50,310] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:50,472] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27453 due to node 3 being disconnected (elapsed time since creation: 2034ms, elapsed time since send: 2034ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:51,075] INFO [RaftManager id=6] Completed transition to Unattached(epoch=786, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=784, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:52,004] INFO [RaftManager id=6] Completed transition to Unattached(epoch=787, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=786, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:53,914] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:18:53,939] INFO [RaftManager id=6] Completed transition to Unattached(epoch=788, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=787, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:56,107] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:56,373] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27460 due to node 1 being disconnected (elapsed time since creation: 2002ms, elapsed time since send: 2001ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:58,756] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:59,184] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27461 due to node 3 being disconnected (elapsed time since creation: 2484ms, elapsed time since send: 2079ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:00,260] INFO [RaftManager id=6] Completed transition to Unattached(epoch=791, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=788, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:00,397] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:19:00,919] INFO [RaftManager id=6] Completed transition to Unattached(epoch=792, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=791, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:03,226] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:03,232] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=793, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=792, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:03,453] INFO [RaftManager id=6] Completed transition to Unattached(epoch=794, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=793, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:04,253] INFO [RaftManager id=6] Completed transition to Unattached(epoch=795, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=794, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:04,349] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:08,167] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:08,314] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=796, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=795, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:08,832] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:09,066] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:09,128] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:19:14,977] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:15,978] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27472 due to node 3 being disconnected (elapsed time since creation: 2578ms, elapsed time since send: 2217ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:16,531] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:16,728] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:16,803] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:16,819] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:16,961] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:17,094] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:17,096] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:17,158] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:17,275] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:17,321] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:17,961] INFO [RaftManager id=6] Completed transition to Unattached(epoch=798, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=796, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26083, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:18,873] INFO [RaftManager id=6] Completed transition to Unattached(epoch=800, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=798, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:19,605] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=800, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26083, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=800, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:19,700] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:22,137] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:22,333] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27478 due to node 1 being disconnected (elapsed time since creation: 2069ms, elapsed time since send: 2058ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:24,979] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:25,255] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:25,398] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:19:25,848] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:26,077] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27479 due to node 3 being disconnected (elapsed time since creation: 4199ms, elapsed time since send: 3056ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:42,604] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:49,721] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:50,950] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:19:55,241] INFO [RaftManager id=6] Completed transition to Unattached(epoch=810, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=800, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26083, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:20:19,554] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:20:22,271] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:23,180] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 27485 due to node 3 being disconnected (elapsed time since creation: 2882ms, elapsed time since send: 2882ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:39,807] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:20:40,060] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:41,867] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 27489 due to node 3 being disconnected (elapsed time since creation: 2733ms, elapsed time since send: 2733ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:46,974] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:47,159] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27493 due to node 1 being disconnected (elapsed time since creation: 4811ms, elapsed time since send: 4265ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:47,161] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:47,166] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 27494 due to node 3 being disconnected (elapsed time since creation: 2914ms, elapsed time since send: 2914ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:55,475] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:20:55,605] INFO [RaftManager id=6] Completed transition to Unattached(epoch=821, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=810, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:21:02,239] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:03,055] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27498 due to node 2 being disconnected (elapsed time since creation: 3287ms, elapsed time since send: 3128ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:12,345] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:21:18,541] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:19,334] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 27500 due to node 1 being disconnected (elapsed time since creation: 6307ms, elapsed time since send: 6307ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:21,582] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:26,295] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 27505 due to node 2 being disconnected (elapsed time since creation: 3255ms, elapsed time since send: 3255ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:26,673] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:26,678] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 27506 due to node 3 being disconnected (elapsed time since creation: 3255ms, elapsed time since send: 3255ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:27,244] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:27,248] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 27510 due to node 1 being disconnected (elapsed time since creation: 6225ms, elapsed time since send: 6225ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:29,347] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:21:30,464] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:30,635] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 27514 due to node 2 being disconnected (elapsed time since creation: 2071ms, elapsed time since send: 2071ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:37,602] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:38,041] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27520 due to node 1 being disconnected (elapsed time since creation: 2113ms, elapsed time since send: 2010ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:38,147] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:38,228] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27519 due to node 2 being disconnected (elapsed time since creation: 4122ms, elapsed time since send: 2760ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:41,415] INFO [RaftManager id=6] Completed transition to Unattached(epoch=829, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=821, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:21:43,137] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:21:50,240] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:50,330] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27528 due to node 2 being disconnected (elapsed time since creation: 2142ms, elapsed time since send: 2003ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:51,417] INFO [RaftManager id=6] Completed transition to Unattached(epoch=834, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=829, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:21:52,297] INFO [RaftManager id=6] Completed transition to Unattached(epoch=835, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=834, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:21:55,699] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:56,124] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27532 due to node 1 being disconnected (elapsed time since creation: 2088ms, elapsed time since send: 2020ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:57,294] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:22:00,402] INFO [RaftManager id=6] Completed transition to Unattached(epoch=839, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=835, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:00,974] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=840, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26083, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=839, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:01,195] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:22:05,199] INFO [RaftManager id=6] Completed transition to Unattached(epoch=841, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=840, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26083, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:07,144] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:08,625] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:08,636] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27542 due to node 3 being disconnected (elapsed time since creation: 2002ms, elapsed time since send: 2001ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:10,899] INFO [RaftManager id=6] Completed transition to Unattached(epoch=843, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=841, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:10,975] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:22:16,667] INFO [RaftManager id=6] Completed transition to Unattached(epoch=846, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=843, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:19,201] INFO [RaftManager id=6] Completed transition to Unattached(epoch=848, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=846, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:24,942] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:22:25,156] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=848, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26083, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=848, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:25,624] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:22:27,436] INFO [RaftManager id=6] Completed transition to Unattached(epoch=852, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=848, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26083, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:28,786] INFO [RaftManager id=6] Completed transition to Unattached(epoch=853, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=852, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:31,351] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:31,889] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27554 due to node 2 being disconnected (elapsed time since creation: 2120ms, elapsed time since send: 2078ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:37,345] INFO [RaftManager id=6] Completed transition to Unattached(epoch=857, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=853, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:38,650] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:39,155] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:22:41,040] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:41,470] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27558 due to node 2 being disconnected (elapsed time since creation: 3484ms, elapsed time since send: 2638ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:41,894] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=861, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26083, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=857, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:41,971] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:22:54,709] WARN [BrokerLifecycleManager id=6] Broker 6 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:23:01,487] INFO [Broker id=6] Transitioning 23 partition(s) to local leaders. (state.change.logger)
[2026-01-16 12:23:09,428] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-46 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:12,169] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-20, __consumer_offsets-17, __consumer_offsets-31, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-5, __consumer_offsets-37, __consumer_offsets-3, __consumer_offsets-35, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2026-01-16 12:23:12,719] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-46 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:12,791] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-38 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:13,261] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-38 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:17,462] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-15 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,053] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-15 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,082] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-48 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,414] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-48 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,416] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-10 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,592] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-10 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,593] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-24 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,667] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-24 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,671] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-40 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,677] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-40 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,709] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-33 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,766] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-33 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,797] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-2 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,810] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-2 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,811] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-29 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:18,817] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Partition __consumer_offsets-29 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:20,056] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:20,284] INFO [Broker id=6] Leader __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 12, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:21,488] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:21,535] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:21,573] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:21,672] INFO [Broker id=6] Leader __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 12, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:23,024] INFO [Broker id=6] Leader __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 12, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:23,370] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:23,395] INFO [Broker id=6] Leader __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 12, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:24,005] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:24,008] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:24,019] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:24,029] INFO [Broker id=6] Leader __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 12, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:24,079] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:24,081] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:24,083] INFO [Broker id=6] Leader __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 12, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:24,297] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:24,362] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:24,385] INFO [Broker id=6] Leader __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 12, high watermark 0, ISR [5,6], adding replicas [] and removing replicas [] . Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:25,245] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:25,301] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:25,339] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:25,342] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:25,357] INFO [Broker id=6] Transitioning 28 partition(s) to local followers. (state.change.logger)
[2026-01-16 12:23:25,367] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:25,368] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:25,369] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,369] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:25,369] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,370] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,370] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,372] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:25,373] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:25,374] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,375] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,382] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,385] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,394] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,398] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,401] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:25,403] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:25,430] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,431] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,434] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,436] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,442] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 5 from offset 12 with partition epoch 15 and high watermark 12. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:25,451] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,452] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,453] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,454] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:25,455] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:25,456] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:25,492] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-29, __consumer_offsets-46, __consumer_offsets-10, __consumer_offsets-40, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 12:23:25,499] INFO [Broker id=6] Stopped fetchers as part of become-follower for 10 partitions (state.change.logger)
[2026-01-16 12:23:26,342] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-48 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-29 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,12), __consumer_offsets-46 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-10 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-40 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-24 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-38 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-33 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-2 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 12:23:26,344] INFO [Broker id=6] Started fetchers as part of become-follower for 10 partitions (state.change.logger)
[2026-01-16 12:23:26,857] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:26,919] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:23:26,935] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:26,936] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:23:26,936] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:26,937] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:23:26,938] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:26,938] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:23:26,938] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:26,939] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:23:26,940] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:26,940] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:23:26,941] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:26,941] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:23:26,941] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:26,942] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:23:26,942] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:26,942] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:23:27,155] INFO [ReplicaFetcherThread-0-4]: Shutting down (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:27,198] INFO [ReplicaFetcherThread-0-4]: Stopped (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:27,204] INFO [ReplicaFetcherThread-0-4]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:23:29,391] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 14 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:29,521] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:30,935] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 30 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,097] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,108] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 42 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,118] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,122] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 7 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,147] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,162] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 23 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,163] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,168] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 5 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,175] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,176] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 19 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,191] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,347] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,351] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,362] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,362] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,375] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,462] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,475] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,482] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,577] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,588] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,593] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,596] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,848] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,849] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,849] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,850] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,851] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,851] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,851] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,851] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,851] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,851] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,852] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,852] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,853] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,853] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,853] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,853] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,853] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,853] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,853] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,853] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,853] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,854] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,854] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,855] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,863] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,863] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,863] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,864] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,865] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,868] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,869] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,871] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,876] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,941] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,942] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,972] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,972] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,973] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,973] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:31,973] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:31,973] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:32,406] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-14 in 1793 milliseconds for epoch 6, of which 603 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:32,591] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-30 in 1483 milliseconds for epoch 6, of which 1481 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:32,978] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-42 in 1856 milliseconds for epoch 6, of which 1856 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:32,981] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-7 in 1820 milliseconds for epoch 6, of which 1819 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:32,981] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-23 in 1813 milliseconds for epoch 6, of which 1813 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:32,981] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-5 in 1805 milliseconds for epoch 6, of which 1805 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:32,981] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-19 in 1711 milliseconds for epoch 6, of which 1711 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:32,986] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:32,994] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,013] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,013] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,013] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,014] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,014] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,035] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,037] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,039] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,048] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,051] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,053] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,054] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,054] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,054] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,054] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,054] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,054] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,054] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,054] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,054] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,056] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,101] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,108] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,110] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:33,113] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:48,969] WARN [BrokerLifecycleManager id=6] Broker 6 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:23:49,773] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2026-01-16 12:23:49,830] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-45) (kafka.server.ReplicaFetcherManager)
[2026-01-16 12:23:49,903] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:52,862] INFO [Partition __consumer_offsets-14 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,032] INFO [Partition __consumer_offsets-43 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,061] INFO [Partition __consumer_offsets-12 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,065] INFO [Partition __consumer_offsets-9 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,074] INFO [Partition __consumer_offsets-42 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,083] INFO [Partition __consumer_offsets-23 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,085] INFO [Partition __consumer_offsets-21 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,090] INFO [Partition __consumer_offsets-19 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,091] INFO [Partition __consumer_offsets-20 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,093] INFO [Partition __consumer_offsets-17 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,096] INFO [Partition __consumer_offsets-31 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,097] INFO [Partition __consumer_offsets-30 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,098] INFO [Partition __consumer_offsets-28 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,099] INFO [Partition __consumer_offsets-26 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,099] INFO [Partition __consumer_offsets-39 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,099] INFO [Partition __consumer_offsets-7 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,099] INFO [Partition __consumer_offsets-8 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,100] INFO [Partition __consumer_offsets-37 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,094] INFO [Broker id=6] Transitioning 22 partition(s) to local leaders. (state.change.logger)
[2026-01-16 12:23:53,100] INFO [Partition __consumer_offsets-5 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,103] INFO [Partition __consumer_offsets-35 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,104] INFO [Partition __consumer_offsets-3 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,111] INFO [Partition __consumer_offsets-1 broker=6] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2026-01-16 12:23:53,238] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-20, __consumer_offsets-17, __consumer_offsets-31, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-39, __consumer_offsets-7, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-5, __consumer_offsets-35, __consumer_offsets-3, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2026-01-16 12:23:53,279] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,283] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,285] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,287] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,289] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,294] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,299] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,302] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,308] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,309] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,310] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,318] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,322] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,327] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,328] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,328] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,328] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,330] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,333] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,334] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,342] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:23:53,351] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 12:24:02,993] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 12:24:03,052] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:03,072] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:03,076] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:03,087] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:04,670] INFO [Broker id=6] Transitioning 27 partition(s) to local followers. (state.change.logger)
[2026-01-16 12:24:04,909] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:04,912] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:04,927] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,930] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:04,936] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,941] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,955] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,957] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:04,959] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:04,962] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,963] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,964] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,964] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,965] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,966] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:04,966] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:04,966] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,967] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,972] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,974] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,975] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=16, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:04,975] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,976] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,977] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,982] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:04,983] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:04,991] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:05,020] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,025] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,032] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,044] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,032] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,048] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,047] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,049] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,057] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,187] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,057] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,187] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,187] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,187] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,187] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,188] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,188] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,190] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,189] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,192] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,192] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,193] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,193] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,193] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,193] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,194] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,194] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,195] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,194] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,196] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,196] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,197] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,197] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,197] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,198] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,199] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,200] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,200] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,201] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,202] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,203] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,204] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,204] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,205] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,206] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,206] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,206] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,206] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,207] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,206] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,207] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,207] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,207] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,207] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,207] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,207] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,207] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,207] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,207] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,207] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,207] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,208] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,208] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,208] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,209] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,209] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,209] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,209] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,209] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,209] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,210] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,210] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,210] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,210] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,210] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,210] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,210] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,210] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:41,867] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:24:44,283] INFO [RaftManager id=6] Completed transition to Unattached(epoch=862, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=861, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26409, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:44,547] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:24:46,674] INFO [RaftManager id=6] Completed transition to Unattached(epoch=867, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=862, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:46,925] INFO [RaftManager id=6] Completed transition to Unattached(epoch=868, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=867, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:46,946] INFO [RaftManager id=6] Completed transition to Unattached(epoch=869, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=868, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:47,424] INFO [RaftManager id=6] Completed transition to Unattached(epoch=870, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=869, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:49,053] INFO [RaftManager id=6] Completed transition to Unattached(epoch=872, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=870, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:49,256] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:24:50,089] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=872, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26409, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=872, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:50,137] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:24:50,462] INFO [RaftManager id=6] Completed transition to Unattached(epoch=873, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=872, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26409, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:50,991] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:24:52,808] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:24:53,424] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 27812 due to node 1 being disconnected (elapsed time since creation: 2228ms, elapsed time since send: 2228ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:24:54,082] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=875, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26409, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=873, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:54,157] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:24:57,993] INFO [RaftManager id=6] Completed transition to Unattached(epoch=876, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=875, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26416, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:58,179] INFO [RaftManager id=6] Completed transition to Unattached(epoch=877, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=876, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:58,720] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:25:00,358] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=879, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26416, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=877, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:25:00,398] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:30:02,369] WARN [BrokerLifecycleManager id=6] Broker 6 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:30:04,782] INFO [Broker id=6] Transitioning 17 partition(s) to local followers. (state.change.logger)
[2026-01-16 12:30:04,883] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-15 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,020] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:05,033] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:05,033] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:05,034] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 7. (state.change.logger)
[2026-01-16 12:30:05,044] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 7. (state.change.logger)
[2026-01-16 12:30:05,045] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:05,050] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 7. (state.change.logger)
[2026-01-16 12:30:05,052] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:05,052] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 7. (state.change.logger)
[2026-01-16 12:30:05,053] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 6 from offset 12 with partition epoch 17 and high watermark 12. Current leader is 4. Previous leader Some(4) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:05,061] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 7. (state.change.logger)
[2026-01-16 12:30:05,061] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 7. (state.change.logger)
[2026-01-16 12:30:05,071] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-15 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,079] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-48 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,090] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-48 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,091] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-46 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,092] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-46 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,101] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-10 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,097] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:05,107] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-10 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,119] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-40 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,118] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 7. (state.change.logger)
[2026-01-16 12:30:05,120] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:05,131] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:05,132] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:05,122] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-40 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,134] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-24 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,178] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-24 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,188] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-38 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,202] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-38 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,224] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-29 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,248] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-29 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,249] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-33 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,261] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-33 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,268] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-2 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,270] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-2 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:05,534] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-46, __consumer_offsets-42, __consumer_offsets-10, __consumer_offsets-23, __consumer_offsets-24, __consumer_offsets-19, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 12:30:05,540] INFO [Broker id=6] Stopped fetchers as part of become-follower for 17 partitions (state.change.logger)
[2026-01-16 12:30:06,848] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),6,0), __consumer_offsets-48 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),6,0), __consumer_offsets-14 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),7,0), __consumer_offsets-46 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),6,0), __consumer_offsets-42 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),7,0), __consumer_offsets-10 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),6,0), __consumer_offsets-23 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),7,0), __consumer_offsets-24 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),6,0), __consumer_offsets-19 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),7,0), __consumer_offsets-29 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),6,12), __consumer_offsets-30 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),7,0), __consumer_offsets-7 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),7,0), __consumer_offsets-40 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),6,0), __consumer_offsets-5 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),7,0), __consumer_offsets-38 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),6,0), __consumer_offsets-33 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),6,0), __consumer_offsets-2 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),6,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 12:30:06,859] INFO [Broker id=6] Started fetchers as part of become-follower for 17 partitions (state.change.logger)
[2026-01-16 12:30:06,864] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:06,877] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:06,896] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:30:06,916] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:06,918] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:30:06,918] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:06,932] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:30:06,932] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:06,933] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:30:06,934] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:06,934] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:30:06,938] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:06,939] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:30:06,940] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:06,951] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:30:06,971] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:06,973] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:30:06,974] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:06,974] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 12:30:07,084] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,119] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,147] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,163] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,165] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,181] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,193] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,195] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,196] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,199] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,201] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,221] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,236] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,244] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,263] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,159] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,263] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,264] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,270] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,263] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,282] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,291] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,271] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,303] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,313] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,331] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,339] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,297] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,346] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,348] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,349] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,349] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,399] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,375] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,403] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,419] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,443] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,482] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,483] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,503] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,419] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,504] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,503] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,505] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,507] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,506] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,508] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,508] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,508] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,514] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,515] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:11,045] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:30:11,500] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:30:12,274] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:30:12,298] INFO [RaftManager id=6] Completed transition to Unattached(epoch=880, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=879, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:30:13,821] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:30:13,830] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=883, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=880, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:30:15,130] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=884, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=883, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:30:15,183] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:30:15,220] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:30:15,275] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:33:54,104] INFO [NodeToControllerChannelManager id=6 name=alter-partition] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:42,600] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:42,843] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 29025 due to node 3 being disconnected (elapsed time since creation: 2658ms, elapsed time since send: 2657ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:43,140] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:43,252] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:43,939] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:43,948] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:44,072] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:44,275] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:44,284] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:44,492] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:44,649] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:44,652] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:44,702] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:44,728] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:44,736] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:44,790] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:44,825] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:44,838] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:44,889] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:44,963] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:44,965] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:45,015] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:45,078] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:45,080] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:45,131] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:45,273] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:45,331] INFO [RaftManager id=6] Completed transition to Unattached(epoch=887, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=884, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27521, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:45,531] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:34:45,685] INFO [RaftManager id=6] Completed transition to Unattached(epoch=888, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=887, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:45,762] INFO [RaftManager id=6] Completed transition to Unattached(epoch=889, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=888, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:48,226] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:48,233] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=889, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27521, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=889, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:48,539] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:48,634] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:48,757] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:48,904] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:48,937] INFO [RaftManager id=6] Completed transition to Unattached(epoch=891, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=889, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27521, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:49,241] INFO [RaftManager id=6] Completed transition to Unattached(epoch=892, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=891, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:50,229] INFO [RaftManager id=6] Completed transition to Unattached(epoch=893, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=892, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:50,373] INFO [RaftManager id=6] Completed transition to Unattached(epoch=894, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=893, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:50,445] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:34:51,394] INFO [RaftManager id=6] Completed transition to Unattached(epoch=895, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=894, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:52,144] INFO [RaftManager id=6] Completed transition to Unattached(epoch=896, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=895, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:53,348] INFO [RaftManager id=6] Completed transition to Unattached(epoch=897, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=896, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:53,951] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=897, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27521, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=897, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:54,023] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:54,385] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:55,424] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:55,491] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:35:02,901] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:03,305] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 29068 due to node 3 being disconnected (elapsed time since creation: 4404ms, elapsed time since send: 4023ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:03,626] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:03,661] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:03,687] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:03,823] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:03,824] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:04,384] INFO [RaftManager id=6] Completed transition to Unattached(epoch=898, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=897, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27528, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:35:04,638] INFO [RaftManager id=6] Completed transition to Unattached(epoch=899, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=898, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:35:06,237] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:35:06,962] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:06,979] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 29074 due to node 3 being disconnected (elapsed time since creation: 2063ms, elapsed time since send: 2062ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:07,358] INFO [RaftManager id=6] Completed transition to Unattached(epoch=900, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=899, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:35:08,379] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=901, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27528, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=900, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:35:08,393] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,081] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:10,192] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,219] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,301] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:10,319] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,402] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,544] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:10,560] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,614] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,661] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:10,667] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,719] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,777] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:10,779] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,849] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:45:04,663] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:14:27,419] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000032230-0000000901 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 13:14:27,795] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000032230-0000000901 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 13:28:50,249] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:28:54,942] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:29:47,446] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 35626 due to node 2 being disconnected (elapsed time since creation: 22780ms, elapsed time since send: 22780ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:29:47,518] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 8658 due to node 2 being disconnected (elapsed time since creation: 4670ms, elapsed time since send: 4670ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:29:47,874] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:29:50,686] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 13:29:58,523] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:29:58,674] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:29:58,735] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 13:29:58,780] INFO [RaftManager id=6] Cancelled in-flight API_VERSIONS request with correlation id 35632 due to node 3 being disconnected (elapsed time since creation: 2263ms, elapsed time since send: 2263ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:00,465] WARN [BrokerLifecycleManager id=6] Broker 6 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 13:30:00,572] INFO [RaftManager id=6] Completed transition to Unattached(epoch=903, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=901, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33899, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 13:30:00,697] INFO [RaftManager id=6] Completed transition to Unattached(epoch=904, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=903, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 13:30:00,931] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:02,842] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=904, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33899, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=904, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 13:30:02,880] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:30:03,142] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:03,143] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 35637 due to node 3 being disconnected (elapsed time since creation: 2436ms, elapsed time since send: 2099ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:06,375] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:06,383] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:30:06,385] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=908, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33903, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=904, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33903, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 13:30:06,454] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:40:04,935] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 14:02:41,478] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 14:02:43,736] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 14:15:20,097] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000039303-0000000908 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 14:15:20,207] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000039303-0000000908 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 15:01:16,769] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:01:18,078] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 46931 due to node 1 being disconnected (elapsed time since creation: 2024ms, elapsed time since send: 2024ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:01:19,811] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:01:19,914] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 11388 due to node 1 being disconnected (elapsed time since creation: 4510ms, elapsed time since send: 4510ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:01:19,946] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:01:20,058] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:01:21,295] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:01:21,345] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:01:21,418] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:01:21,578] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:01:21,604] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:01:21,655] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:01:21,809] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:01:21,811] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:01:21,864] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:01:21,925] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:01:21,930] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:01:21,981] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:01:22,061] INFO [RaftManager id=6] Completed transition to Unattached(epoch=910, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=908, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44791, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:01:22,079] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:01:22,459] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=911, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44791, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=910, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:01:22,486] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:01:22,642] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 44791 (kafka.log.UnifiedLog)
[2026-01-16 15:01:22,882] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 44791 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 15:01:22,886] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 44791 (kafka.log.UnifiedLog$)
[2026-01-16 15:01:22,903] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=1720, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000001720.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 15:01:23,435] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 44791 with 0 producer ids in 79 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 15:01:23,437] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 87ms for snapshot load and 454ms for segment recovery from offset 44791 (kafka.log.UnifiedLog$)
[2026-01-16 15:01:23,440] INFO [RaftManager id=6] Truncated to offset 44791 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 15:02:43,502] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:02:44,002] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:02:45,444] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:02:45,568] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 47106 due to node 3 being disconnected (elapsed time since creation: 2006ms, elapsed time since send: 2006ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:02:48,264] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:02:49,205] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:02:49,530] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:02:50,964] INFO [RaftManager id=6] Completed transition to Unattached(epoch=914, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=911, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44941, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:02:52,441] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:02:53,181] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:02:53,278] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 47108 due to node 1 being disconnected (elapsed time since creation: 2190ms, elapsed time since send: 2127ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:02:54,321] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:02:57,087] INFO [RaftManager id=6] Completed transition to Unattached(epoch=918, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=914, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:02:58,423] INFO [RaftManager id=6] Completed transition to Unattached(epoch=920, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=918, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:02:58,782] INFO [RaftManager id=6] Completed transition to Unattached(epoch=921, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=920, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:02:58,942] INFO [RaftManager id=6] Completed transition to Unattached(epoch=922, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=921, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:02,177] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:03:04,693] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:03:05,245] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 47117 due to node 1 being disconnected (elapsed time since creation: 3986ms, elapsed time since send: 2014ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:03:07,710] INFO [RaftManager id=6] Completed transition to Unattached(epoch=923, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=922, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:07,801] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:03:08,524] INFO [RaftManager id=6] Completed transition to Unattached(epoch=926, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=923, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:08,977] INFO [RaftManager id=6] Completed transition to Unattached(epoch=928, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=926, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:12,214] INFO [RaftManager id=6] Completed transition to Unattached(epoch=929, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=928, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:12,784] INFO [RaftManager id=6] Completed transition to Unattached(epoch=930, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=929, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:14,592] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:03:16,769] INFO [RaftManager id=6] Completed transition to Unattached(epoch=931, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=930, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:18,541] INFO [RaftManager id=6] Completed transition to Unattached(epoch=934, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=931, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:22,321] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:03:22,422] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:03:22,753] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 47129 due to node 3 being disconnected (elapsed time since creation: 3489ms, elapsed time since send: 3480ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:03:24,146] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:03:24,162] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=937, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44941, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=934, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:24,412] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 44941 (kafka.log.UnifiedLog)
[2026-01-16 15:03:25,976] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 44941 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 15:03:28,835] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 44941 (kafka.log.UnifiedLog$)
[2026-01-16 15:03:29,817] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=44791, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000044791.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 15:03:30,539] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:03:30,810] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:03:31,514] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 44941 with 0 producer ids in 118 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 15:03:31,540] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 934ms for snapshot load and 1008ms for segment recovery from offset 44941 (kafka.log.UnifiedLog$)
[2026-01-16 15:03:31,582] INFO [RaftManager id=6] Truncated to offset 44941 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 15:03:32,277] INFO [RaftManager id=6] Completed transition to Unattached(epoch=940, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=937, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44941, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:33,228] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=940, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44941, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=940, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:39,912] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:03:40,907] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:03:41,275] INFO [RaftManager id=6] Completed transition to Unattached(epoch=941, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=940, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44945, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:41,527] INFO [RaftManager id=6] Completed transition to Unattached(epoch=943, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=941, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:41,543] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:03:44,152] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:03:46,057] INFO [RaftManager id=6] Completed transition to Unattached(epoch=944, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=943, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:03:46,110] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:03:46,730] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 47138 due to node 3 being disconnected (elapsed time since creation: 4290ms, elapsed time since send: 3838ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:03:49,727] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:03:51,806] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 47141 due to node 1 being disconnected (elapsed time since creation: 2110ms, elapsed time since send: 2026ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:03:59,821] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:04:02,006] INFO [RaftManager id=6] Completed transition to Unattached(epoch=948, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=944, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:04:06,621] INFO [RaftManager id=6] Completed transition to Unattached(epoch=949, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=948, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:04:08,562] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=950, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44945, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=949, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:04:08,663] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:04:12,954] INFO [RaftManager id=6] Completed transition to Unattached(epoch=952, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=950, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44945, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:04:16,580] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:04:16,632] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:04:17,544] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 47153 due to node 3 being disconnected (elapsed time since creation: 3490ms, elapsed time since send: 3400ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:04:18,286] INFO [RaftManager id=6] Completed transition to Unattached(epoch=957, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=952, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:04:19,001] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:04:20,877] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=957, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44945, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=957, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:04:21,395] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:20:43,488] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:20:44,197] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:20:44,868] INFO [RaftManager id=6] Completed transition to Unattached(epoch=959, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=957, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44952, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:20:45,034] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:20:47,625] INFO [RaftManager id=6] Completed transition to Unattached(epoch=961, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=959, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:20:47,744] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:20:50,541] INFO [RaftManager id=6] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:20:50,789] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 47164 due to node 1 being disconnected (elapsed time since creation: 2567ms, elapsed time since send: 2565ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:20:52,800] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:20:53,301] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:20:53,311] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=963, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44952, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=961, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:20:53,762] INFO [RaftManager id=6] Completed transition to Unattached(epoch=964, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=963, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44952, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:20:54,007] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:20:54,207] INFO [RaftManager id=6] Completed transition to Unattached(epoch=965, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=964, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:20:55,414] INFO [RaftManager id=6] Completed transition to Unattached(epoch=966, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=965, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:20:55,935] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=966, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44952, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=966, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:20:55,981] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:20:56,382] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:20:56,501] INFO [RaftManager id=6] Completed transition to Unattached(epoch=967, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=966, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44952, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:20:57,643] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:20:58,787] INFO [RaftManager id=6] Completed transition to Unattached(epoch=968, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=967, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:20:59,410] INFO [RaftManager id=6] Completed transition to Unattached(epoch=969, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=968, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:20:59,685] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=969, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44952, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=969, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:20:59,686] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:21:01,792] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:21:01,967] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:21:02,503] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:21:02,538] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:21:02,591] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:21:02,783] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:21:02,796] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:21:02,811] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:21:04,124] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:21:04,207] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:21:04,280] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:21:04,405] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:21:04,426] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:21:04,489] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:21:04,863] INFO [RaftManager id=6] Completed transition to Unattached(epoch=970, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=969, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44952, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:21:05,563] INFO [RaftManager id=6] Completed transition to Unattached(epoch=972, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=970, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:21:05,786] INFO [RaftManager id=6] Completed transition to Unattached(epoch=973, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=972, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:21:07,318] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:21:07,825] INFO [RaftManager id=6] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:21:08,196] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 47201 due to node 3 being disconnected (elapsed time since creation: 2016ms, elapsed time since send: 2011ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:21:09,198] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=974, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44952, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=973, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:21:09,242] INFO [RaftManager id=6] Completed transition to Unattached(epoch=975, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=974, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44952, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:21:11,321] INFO [RaftManager id=6] Completed transition to Unattached(epoch=977, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=975, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:21:11,710] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=977, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=44952, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=977, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:21:11,742] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:21:11,781] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 15:40:38,081] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:40:38,160] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:40:38,211] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:40:38,440] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:40:38,444] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:40:38,496] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:40:38,520] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:40:38,525] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:40:38,549] INFO [RaftManager id=6] Completed transition to Unattached(epoch=978, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=977, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45071, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:40:38,600] INFO [RaftManager id=6] Completed transition to Unattached(epoch=979, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=978, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:40:40,092] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=979, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45071, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=979, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 15:40:40,099] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:40:40,396] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 15:40:40,405] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 15:40:40,457] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:22,845] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:25:22,905] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:22,948] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:23,636] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:25:23,642] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:23,699] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:23,825] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:25:23,837] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:23,887] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:23,970] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:25:23,975] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:24,025] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:24,059] INFO [RaftManager id=6] Completed transition to Unattached(epoch=980, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=979, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45183, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:25:24,061] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:25:25,945] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=980, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45183, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=980, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:25:26,020] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:26,404] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:25:26,405] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:26,465] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:26,479] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=981, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45183, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=980, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45183, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:25:26,565] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 45184 (kafka.log.UnifiedLog)
[2026-01-16 16:25:26,744] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 45184 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 16:25:26,762] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 45184 (kafka.log.UnifiedLog$)
[2026-01-16 16:25:26,773] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=44941, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000044941.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 16:25:26,975] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 45184 with 0 producer ids in 49 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 16:25:26,988] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 51ms for snapshot load and 169ms for segment recovery from offset 45184 (kafka.log.UnifiedLog$)
[2026-01-16 16:25:26,989] INFO [RaftManager id=6] Truncated to offset 45184 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 16:25:27,066] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:25:27,073] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:25:27,093] INFO [BrokerLifecycleManager id=6] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 16:43:04,375] INFO [RaftManager id=6] Completed transition to Unattached(epoch=982, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=981, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45234, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:43:04,604] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=982, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45234, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=982, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:43:05,702] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:43:05,706] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:43:44,539] INFO [RaftManager id=6] Completed transition to Unattached(epoch=983, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=982, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45292, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:43:44,551] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:43:44,611] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=983, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45292, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=983, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:43:44,656] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:22,951] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:44:23,174] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:23,313] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:25,289] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:44:25,403] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 47826 due to node 2 being disconnected (elapsed time since creation: 2151ms, elapsed time since send: 2143ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:44:25,596] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:44:25,608] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:25,658] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:25,740] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:44:25,799] INFO [RaftManager id=6] Completed transition to Unattached(epoch=985, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=983, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:26,222] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=985, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=985, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:26,275] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:26,936] INFO [RaftManager id=6] Completed transition to Unattached(epoch=986, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=985, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:27,086] INFO [RaftManager id=6] Completed transition to Unattached(epoch=987, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=986, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:27,340] INFO [RaftManager id=6] Completed transition to Unattached(epoch=988, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=987, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:27,501] INFO [RaftManager id=6] Completed transition to Unattached(epoch=989, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=988, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:28,266] INFO [RaftManager id=6] Completed transition to Unattached(epoch=990, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=989, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:29,002] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:44:30,396] INFO [RaftManager id=6] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:44:30,413] INFO [RaftManager id=6] Cancelled in-flight FETCH request with correlation id 47883 due to node 2 being disconnected (elapsed time since creation: 2007ms, elapsed time since send: 2005ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:44:30,487] INFO [RaftManager id=6] Completed transition to Unattached(epoch=992, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=990, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:31,138] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=992, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=992, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:31,186] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:40,692] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:44:40,741] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:40,783] INFO [RaftManager id=6] Completed transition to Unattached(epoch=993, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=992, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45367, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:41,154] INFO [RaftManager id=6] Completed transition to Unattached(epoch=995, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=993, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:42,366] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:42,371] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=995, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45367, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=995, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:44:42,557] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:44:42,581] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:42,647] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:42,702] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:44:42,713] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:44:42,764] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:45:34,525] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:45:34,590] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:45:34,641] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:45:35,142] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:45:35,148] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:45:35,199] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:45:35,327] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:45:35,329] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:45:35,381] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:45:35,403] INFO [RaftManager id=6] Completed transition to Unattached(epoch=996, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=995, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45386, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:45:35,503] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:45:35,757] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=996, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45386, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=996, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:45:35,821] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:45:49,259] INFO [RaftManager id=6] Completed transition to Unattached(epoch=997, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=996, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45411, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:45:49,656] INFO [RaftManager id=6] Completed transition to Unattached(epoch=998, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=997, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:45:49,808] INFO [RaftManager id=6] Completed transition to Unattached(epoch=999, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=998, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:45:50,084] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:45:52,007] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1000, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=999, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:45:52,963] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1001, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=1000, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:45:53,442] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1001, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45411, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=1001, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:45:53,500] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:47:11,237] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1002, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1001, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:47:11,565] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1002, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=1002, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:47:11,600] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1003, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1002, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:47:11,640] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1003, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=1003, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:47:11,657] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 45430 (kafka.log.UnifiedLog)
[2026-01-16 16:47:11,696] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 45430 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 16:47:11,698] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 45430 (kafka.log.UnifiedLog$)
[2026-01-16 16:47:11,700] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=45184, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000045184.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 16:47:11,749] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 45430 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 16:47:11,750] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 16ms for snapshot load and 35ms for segment recovery from offset 45430 (kafka.log.UnifiedLog$)
[2026-01-16 16:47:11,750] INFO [RaftManager id=6] Truncated to offset 45430 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 16:47:12,181] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:47:12,187] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:49:15,704] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1004, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1003, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45607, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:49:15,939] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1004, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45607, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=1004, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:49:16,723] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:49:16,734] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:51:33,238] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1005, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1004, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45844, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:51:33,483] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1005, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45844, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=1005, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:51:33,510] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1006, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1005, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45844, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:51:33,587] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1006, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45844, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=1006, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:51:33,606] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 45845 (kafka.log.UnifiedLog)
[2026-01-16 16:51:33,659] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 45845 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 16:51:33,662] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 45845 (kafka.log.UnifiedLog$)
[2026-01-16 16:51:33,665] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=45430, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000045430.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 16:51:33,722] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 45845 with 0 producer ids in 10 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 16:51:33,723] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 26ms for snapshot load and 33ms for segment recovery from offset 45845 (kafka.log.UnifiedLog$)
[2026-01-16 16:51:33,724] INFO [RaftManager id=6] Truncated to offset 45845 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 16:51:33,802] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:51:33,805] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:52:49,494] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1007, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1006, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45962, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:52:49,638] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1007, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=45962, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=1007, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:52:50,444] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:52:50,463] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:54:19,462] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000046135-0000001007 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 16:54:19,609] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000046135-0000001007 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 16:55:00,207] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1008, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1007, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=46200, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:55:00,286] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1008, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=46200, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=1008, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:55:00,425] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:55:00,429] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:56:02,712] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:56:02,760] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:56:02,800] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:56:03,006] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:56:03,016] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:56:03,085] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:56:03,086] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1009, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1008, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=46259, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:56:03,124] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:56:03,367] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1009, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=46259, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=1009, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:56:03,431] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:58:40,273] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1010, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1009, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=46552, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:58:40,383] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1010, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=46552, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=1010, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:58:40,772] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:58:40,796] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 16:59:51,680] INFO [RaftManager id=6] Completed transition to Unattached(epoch=1011, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1010, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=46611, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:59:51,730] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1011, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=46611, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=1011, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 16:59:52,629] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 16:59:52,674] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
