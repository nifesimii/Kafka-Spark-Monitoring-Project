[2025-12-24 09:10:28,316] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 09:10:28,786] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 09:10:28,911] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 09:10:28,924] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,370] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 09:10:32,495] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 09:10:32,502] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,693] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,699] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,714] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 09:10:32,715] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 09:10:32,717] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 09:10:32,720] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,939] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:32,949] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:32,953] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:33,013] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 09:10:33,093] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 09:10:33,096] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,103] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,181] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1320) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:33,191] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 09:10:33,195] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 09:10:33,233] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,241] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-24 09:10:33,273] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:33,323] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1109634719 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,334] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,340] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,341] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,348] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,342] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,362] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,341] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,466] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,544] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 09:10:33,545] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 09:10:33,583] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,589] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:33,590] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,599] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,603] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 09:10:33,688] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,740] INFO [RaftManager id=4] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1320) (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:33,762] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,764] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,774] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,774] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,789] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,891] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,993] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,099] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,200] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,311] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,361] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 09:10:34,413] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,416] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 09:10:34,416] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 09:10:34,421] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 09:10:34,433] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,456] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,501] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,506] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,509] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,576] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,577] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,576] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:34,577] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,577] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,577] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,597] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,599] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,611] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 09:10:34,614] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,614] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,627] INFO [BrokerLifecycleManager id=4] Incarnation _7wCxPPbSfO81U2Voxbubw of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,641] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,668] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,669] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 09:10:34,669] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,673] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 1 (org.apache.kafka.raft.FollowerState)
[2025-12-24 09:10:34,672] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 09:10:34,676] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 09:10:34,683] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,712] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 7 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,721] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,722] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,723] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,724] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=6, epoch=1) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 09:10:34,727] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 09:10:34,735] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 09:10:34,740] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 7 (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,752] INFO Loaded 0 logs in 24ms (kafka.log.LogManager)
[2025-12-24 09:10:34,755] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 09:10:34,760] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 09:10:34,785] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 09:10:34,999] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 09:10:35,011] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 09:10:35,011] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 09:10:35,012] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:35,016] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:35,017] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 09:10:35,020] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 09:10:35,020] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 09:10:35,029] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:35,092] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:35,094] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 09:10:35,094] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 09:10:35,094] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 09:10:35,096] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 09:10:35,099] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:35,106] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 09:10:35,145] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:35,187] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:35,188] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 09:10:35,190] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 09:10:35,190] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 09:10:35,191] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 09:10:35,192] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 09:10:35,194] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 09:10:35,198] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 09:10:35,199] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 09:10:35,199] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 09:10:35,200] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 09:10:35,200] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 09:10:35,200] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,201] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,202] INFO Kafka startTimeMs: 1766567435200 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,203] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 09:10:37,818] INFO Sent auto-creation request for Set(_schemas) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 09:10:38,169] INFO [Broker id=4] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-12-24 09:10:38,175] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:38,177] INFO [Broker id=4] Creating new partition _schemas-0 with topic id Im1NE7r1TMiyWVwhbgXZ_g. (state.change.logger)
[2025-12-24 09:10:38,202] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:38,205] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 09:10:38,207] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 09:10:38,209] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:38,210] INFO [Broker id=4] Leader _schemas-0 with topic id Some(Im1NE7r1TMiyWVwhbgXZ_g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:38,223] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 09:10:39,165] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-24 09:10:39,166] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-48, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-0, __consumer_offsets-32, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-5, __consumer_offsets-6, __consumer_offsets-36, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,168] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,172] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,174] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,177] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 09:10:39,177] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,178] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,185] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,197] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,205] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,206] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 09:10:39,207] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,209] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,221] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,231] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,240] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,242] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 09:10:39,243] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,244] INFO [Broker id=4] Leader __consumer_offsets-11 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,266] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,279] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,284] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,284] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 09:10:39,285] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,285] INFO [Broker id=4] Leader __consumer_offsets-12 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,292] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,298] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,299] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,299] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 09:10:39,299] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,300] INFO [Broker id=4] Leader __consumer_offsets-41 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,308] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,311] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,312] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,312] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 09:10:39,313] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,313] INFO [Broker id=4] Leader __consumer_offsets-42 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,318] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,321] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,322] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,322] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 09:10:39,322] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,323] INFO [Broker id=4] Leader __consumer_offsets-21 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,327] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,330] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,331] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,331] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 09:10:39,331] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,332] INFO [Broker id=4] Leader __consumer_offsets-19 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,337] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,341] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,342] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,344] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 09:10:39,345] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,345] INFO [Broker id=4] Leader __consumer_offsets-17 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,352] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,355] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,356] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,357] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,357] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,357] INFO [Broker id=4] Leader __consumer_offsets-0 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,361] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,370] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,371] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,373] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 09:10:39,375] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,375] INFO [Broker id=4] Leader __consumer_offsets-32 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,380] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,384] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,385] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,386] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 09:10:39,387] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,388] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,394] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,397] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,400] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,400] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 09:10:39,401] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,402] INFO [Broker id=4] Leader __consumer_offsets-25 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,406] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,409] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,411] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,412] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 09:10:39,413] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,414] INFO [Broker id=4] Leader __consumer_offsets-5 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,419] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,421] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,423] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,423] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 09:10:39,424] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,424] INFO [Broker id=4] Leader __consumer_offsets-6 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,433] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,440] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,441] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,441] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 09:10:39,441] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,441] INFO [Broker id=4] Leader __consumer_offsets-36 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,448] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,452] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,452] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,453] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 09:10:39,453] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,453] INFO [Broker id=4] Leader __consumer_offsets-34 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,457] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-24 09:10:39,457] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,462] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,465] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,467] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 09:10:39,467] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,469] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,470] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,474] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,475] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,475] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 09:10:39,475] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,475] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,476] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,481] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,482] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,484] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 09:10:39,484] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,485] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,485] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,490] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,491] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,491] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 09:10:39,491] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,491] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,492] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,494] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,495] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,496] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 09:10:39,496] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,496] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,496] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,499] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,499] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,500] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 09:10:39,500] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,501] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,501] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,505] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,507] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,508] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 09:10:39,508] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,508] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,509] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,513] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,513] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,514] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 09:10:39,515] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,516] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,516] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,521] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,522] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,523] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 09:10:39,524] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,525] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,526] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,529] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,530] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,531] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 09:10:39,531] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,532] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,532] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,535] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,536] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,536] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 09:10:39,537] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,538] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,538] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,540] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,541] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,542] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 09:10:39,542] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,542] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,543] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,546] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,547] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,547] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 09:10:39,547] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,547] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,548] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,552] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,554] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,554] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 09:10:39,555] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,555] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,556] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,559] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,562] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,563] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 09:10:39,563] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,564] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,564] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,569] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,569] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,571] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 09:10:39,571] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,571] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,572] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,581] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,583] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,584] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 09:10:39,584] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,585] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,585] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,591] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,592] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,593] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 09:10:39,593] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,594] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,594] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,597] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,597] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,598] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 09:10:39,598] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,598] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,599] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,601] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,602] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,602] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 09:10:39,603] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,603] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,603] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,607] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,609] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,609] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 09:10:39,610] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,610] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,610] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,616] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,617] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,617] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 09:10:39,618] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,618] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,618] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,621] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,622] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,622] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 09:10:39,622] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,622] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,623] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,626] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,626] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,627] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 09:10:39,627] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,627] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,628] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,634] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,639] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,640] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 09:10:39,641] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,645] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,647] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,653] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,654] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,654] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 09:10:39,654] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,654] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,655] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,659] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,660] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,660] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 09:10:39,661] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,661] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,661] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,665] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,668] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,668] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 09:10:39,669] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,669] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,670] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,673] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,675] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,675] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 09:10:39,676] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,676] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,676] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,682] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,682] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,683] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 09:10:39,683] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,683] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,684] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,687] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,690] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,690] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 09:10:39,690] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,691] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,691] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,694] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,696] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,696] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 09:10:39,697] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,697] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,697] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,702] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,704] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,704] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 09:10:39,704] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,705] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,706] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-13, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-1, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,706] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 09:10:39,724] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,728] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,728] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,729] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,732] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,734] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,734] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,734] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,735] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,734] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,735] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,735] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,735] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,736] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 09:10:39,735] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,736] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,736] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,737] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,736] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,737] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,737] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,737] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,737] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,738] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,738] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,738] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,739] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,738] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,739] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,739] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,739] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,740] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,740] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,739] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,740] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,740] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,740] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,741] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,741] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,741] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,741] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,742] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,742] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,742] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,743] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,743] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,743] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,743] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,743] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,744] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,743] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,744] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,744] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,744] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,745] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,744] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,745] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,745] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,745] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,746] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,746] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,746] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,746] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,746] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,746] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,747] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,747] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,747] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,748] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,748] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,748] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,755] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,756] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,757] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,757] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,757] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,758] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,758] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,758] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,760] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,760] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,760] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,760] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,760] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,761] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,761] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,762] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,762] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,762] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,762] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,762] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,762] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,763] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,763] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,762] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-11 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,763] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,763] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,763] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,763] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,763] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,764] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,764] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,763] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-41 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,764] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,764] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,764] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,764] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,765] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,765] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-21 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,765] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,765] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,765] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,765] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,765] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,766] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,766] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,766] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,766] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,766] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,765] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,766] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,766] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,767] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,766] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-17 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,767] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-0 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,767] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-32 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-25 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-5 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,769] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,769] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-6 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,769] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-36 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,770] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,770] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,770] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-34 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,770] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,770] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,770] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,770] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,770] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,771] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,771] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,771] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,771] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,771] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,771] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,771] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,899] INFO [GroupCoordinator 4]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-d97cbd36-3521-4579-b6fd-05fee9ad2175 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,903] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-d97cbd36-3521-4579-b6fd-05fee9ad2175 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:42,911] INFO [GroupCoordinator 4]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:42,934] INFO [GroupCoordinator 4]: Assignment received from leader sr-1-d97cbd36-3521-4579-b6fd-05fee9ad2175 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:20:34,330] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:20:37,843] INFO [NodeToControllerChannelManager id=4 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:07:11,891] INFO [RaftManager id=4] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6779, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:07:11,966] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6779, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:07:12,205] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:07:12,206] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:15:24,164] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6898, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:15:24,183] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6898, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:15:24,671] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:15:24,672] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:18:15,406] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000007240-0000000003 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 10:18:15,536] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000007240-0000000003 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 10:24:59,108] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-d97cbd36-3521-4579-b6fd-05fee9ad2175 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:24:59,112] INFO [GroupCoordinator 4]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:24:59,123] INFO [GroupCoordinator 4]: Member MemberMetadata(memberId=sr-1-d97cbd36-3521-4579-b6fd-05fee9ad2175, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.14, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:26:37,722] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 10:26:38,488] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 10:26:38,552] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 10:26:38,558] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:42,422] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 10:26:42,745] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 10:26:42,779] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:43,143] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:43,167] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:43,264] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 10:26:43,302] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 10:26:43,306] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 10:26:43,310] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:43,930] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:43,962] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:43,994] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:44,381] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 10:26:44,525] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 10:26:44,576] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:44,582] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:44,789] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1702) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:44,801] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 10:26:44,807] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 10:26:44,833] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:44,840] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-24 10:26:44,871] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:44,944] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,056] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,068] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,107] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,131] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,137] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,150] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1583505174 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:45,165] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,223] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,240] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,272] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,374] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,404] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 10:26:45,411] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 10:26:45,418] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,423] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,451] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,457] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,461] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:45,466] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,467] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,477] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,880] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,478] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,895] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,898] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,912] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,940] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,918] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 10:26:45,996] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,035] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,048] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,071] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,091] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,100] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,114] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,126] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,213] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,323] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,429] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,694] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,712] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,736] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,812] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,940] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,866] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,991] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,003] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,064] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,072] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,128] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,185] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,287] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,301] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,295] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,447] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,504] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,577] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,712] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,830] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,017] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,022] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,043] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,075] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,089] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,139] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,240] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,343] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,593] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,629] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,628] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 10:26:48,602] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,819] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,928] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 10:26:48,932] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,944] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 10:26:49,047] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 10:26:49,079] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,183] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,224] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:49,295] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,342] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:49,404] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,409] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,431] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,450] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,456] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,461] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,510] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,511] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,512] INFO [RaftManager id=4] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1702) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:49,518] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,682] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,904] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,963] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 10:26:49,981] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:49,989] INFO [BrokerLifecycleManager id=4] Incarnation Hc3RSEWMR_meyc8bhUfATw of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:50,008] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:50,017] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:50,036] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:50,053] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,156] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 10:26:50,159] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,161] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,161] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 10:26:50,164] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 10:26:50,262] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,365] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,467] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,566] INFO [RaftManager id=4] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:50,569] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,677] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,784] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,829] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:50,845] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,885] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,886] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,909] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,917] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,388] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,518] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,664] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,868] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:51,885] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,888] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,928] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,983] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:51,987] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,987] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:52,040] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:52,106] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,214] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,427] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-24 10:26:52,436] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,572] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,625] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,681] INFO [MetadataLoader id=4] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,685] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,696] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,704] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,711] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=2) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 10:26:52,749] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 10:26:52,886] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 10:26:53,046] INFO Loaded 0 logs in 257ms (kafka.log.LogManager)
[2025-12-24 10:26:53,054] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 10:26:53,052] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:53,067] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 10:26:53,109] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 10:26:53,757] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 10:26:53,782] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 10:26:53,783] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 10:26:53,786] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:26:53,789] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:26:53,790] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 10:26:53,796] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 10:26:53,797] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 10:26:53,860] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:53,956] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:53,957] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 10:26:53,957] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 10:26:53,958] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 10:26:53,976] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 10:26:53,987] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:53,981] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:54,004] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 10:26:54,083] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:54,089] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 10:26:54,113] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 10:26:54,117] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 10:26:54,127] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 10:26:54,154] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 10:26:54,192] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 10:26:54,230] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 10:26:54,232] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 10:26:54,234] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 10:26:54,235] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 10:26:54,235] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 10:26:54,245] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,249] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,251] INFO Kafka startTimeMs: 1766572014237 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,256] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 10:26:59,281] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-24 10:26:59,285] INFO [Broker id=4] Creating new partition _schemas-0 with topic id 4gVnUI_IQ3yRWz4RgXc--Q. (state.change.logger)
[2025-12-24 10:26:59,343] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:59,348] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 10:26:59,354] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 10:26:59,358] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:26:59,373] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:26:59,375] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:26:59,377] INFO [Broker id=4] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 10:26:59,407] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:26:59,411] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(4gVnUI_IQ3yRWz4RgXc--Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:26:59,413] INFO [Broker id=4] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 10:26:59,411] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:26:59,421] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:26:59,440] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 10:27:00,789] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-24 10:27:00,792] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-42, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-35, __consumer_offsets-36) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:00,792] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,803] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,806] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,807] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 10:27:00,810] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,813] INFO [Broker id=4] Leader __consumer_offsets-15 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,824] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,829] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,831] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,832] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 10:27:00,832] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,833] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,839] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,843] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,844] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,844] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 10:27:00,845] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,846] INFO [Broker id=4] Leader __consumer_offsets-14 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,851] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,857] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,860] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,861] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 10:27:00,861] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,862] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,867] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,873] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,878] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,879] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 10:27:00,881] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,884] INFO [Broker id=4] Leader __consumer_offsets-11 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,890] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,895] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,896] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,896] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 10:27:00,896] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,897] INFO [Broker id=4] Leader __consumer_offsets-42 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,900] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,915] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,919] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,920] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 10:27:00,920] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,921] INFO [Broker id=4] Leader __consumer_offsets-24 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,926] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,929] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,930] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,931] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 10:27:00,932] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,932] INFO [Broker id=4] Leader __consumer_offsets-21 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,936] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,940] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,943] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,944] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 10:27:00,946] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,946] INFO [Broker id=4] Leader __consumer_offsets-18 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,952] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,955] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,956] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,957] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,957] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,958] INFO [Broker id=4] Leader __consumer_offsets-0 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,965] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,968] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,969] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,970] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 10:27:00,971] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,971] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,980] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,988] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,995] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,996] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 10:27:00,997] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,999] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,006] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,009] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,010] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,011] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 10:27:01,011] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,012] INFO [Broker id=4] Leader __consumer_offsets-8 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,017] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,022] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,024] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,024] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 10:27:01,024] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,026] INFO [Broker id=4] Leader __consumer_offsets-40 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,033] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,035] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,036] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,036] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 10:27:01,036] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,037] INFO [Broker id=4] Leader __consumer_offsets-3 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,039] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,042] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,043] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,043] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 10:27:01,044] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,044] INFO [Broker id=4] Leader __consumer_offsets-35 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,048] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,052] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,052] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,052] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 10:27:01,053] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,053] INFO [Broker id=4] Leader __consumer_offsets-36 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,057] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-24 10:27:01,057] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,060] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,061] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,061] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 10:27:01,061] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,061] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,061] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,064] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,065] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,065] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 10:27:01,066] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,066] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,067] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,078] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,079] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,079] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 10:27:01,080] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,080] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,081] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,085] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,087] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,088] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 10:27:01,089] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,089] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,090] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,094] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,095] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,095] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 10:27:01,095] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,096] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,096] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,100] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,101] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,101] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 10:27:01,102] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,102] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,103] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,110] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,111] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,112] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 10:27:01,112] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,112] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,112] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,120] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,123] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,124] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 10:27:01,124] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,125] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,125] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,133] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,135] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,135] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 10:27:01,136] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,136] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,136] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,139] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,142] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,142] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 10:27:01,142] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,143] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,143] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,146] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,148] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,149] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 10:27:01,149] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,150] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,150] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,154] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,155] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,155] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 10:27:01,156] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,156] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,156] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,161] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,162] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,162] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 10:27:01,162] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,163] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,163] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,166] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,168] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,168] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 10:27:01,169] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,169] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,169] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,173] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,177] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,178] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 10:27:01,178] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,178] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,179] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,183] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,184] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,184] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 10:27:01,185] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,185] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,186] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,188] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,189] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,189] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 10:27:01,189] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,190] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,190] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,196] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,197] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,198] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 10:27:01,199] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,199] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,200] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,204] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,205] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,205] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 10:27:01,206] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,206] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,206] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,210] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,210] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,211] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 10:27:01,211] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,211] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,212] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,217] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,218] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,218] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 10:27:01,218] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,218] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,219] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,223] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,226] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,227] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 10:27:01,227] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,228] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,230] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,233] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,235] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,235] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 10:27:01,235] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,236] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,237] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,242] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,243] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,243] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 10:27:01,243] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,244] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,244] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,250] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,251] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,251] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 10:27:01,252] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,252] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,252] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,256] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,258] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,258] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 10:27:01,259] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,259] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,259] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,263] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,265] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,265] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 10:27:01,266] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,266] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,267] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,271] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,278] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,280] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 10:27:01,281] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,283] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,284] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,291] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,295] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,295] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 10:27:01,295] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,296] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,297] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,304] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,305] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,306] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 10:27:01,306] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,306] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,306] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,322] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,326] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,338] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 10:27:01,340] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,345] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,350] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,371] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,373] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,375] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 10:27:01,375] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,389] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,391] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,406] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,409] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,413] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 10:27:01,413] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,414] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,415] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-37, __consumer_offsets-6, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,415] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 10:27:01,417] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-16 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,436] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,437] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,437] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 10:27:01,453] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,454] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,454] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,455] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,455] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,455] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,458] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,457] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-15 in 3 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,457] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,459] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,458] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,459] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,459] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,460] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,460] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,460] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,461] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,460] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,461] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,461] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,461] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,461] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,462] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,462] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,460] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,462] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,462] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,461] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,463] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,462] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,465] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,462] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,466] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,465] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,467] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,468] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,466] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,469] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,469] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,470] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,470] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,471] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,471] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,471] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,471] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,471] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,471] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,471] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,472] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,472] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,473] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,473] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,475] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,468] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,478] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,477] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,479] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,470] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-42 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,479] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-24 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,479] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,480] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,479] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,482] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,481] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,483] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,480] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-21 in 18 milliseconds for epoch 0, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,483] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-18 in 18 milliseconds for epoch 0, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,483] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,487] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,487] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,488] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,489] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,490] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,487] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-0 in 19 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,494] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,496] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,496] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 18 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,497] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,499] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,499] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 16 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,501] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-8 in 14 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,501] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,507] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-40 in 18 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,509] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-3 in 15 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,508] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,511] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-35 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,514] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,515] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,515] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-36 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,516] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,518] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,517] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,519] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,519] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,520] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,520] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,522] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,522] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,522] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,523] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,524] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,524] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,524] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,524] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,524] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,526] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,526] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,526] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,526] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,528] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,528] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,528] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,530] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,530] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,530] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,532] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,532] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,532] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,533] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,533] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,532] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,533] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,534] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,533] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,534] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,534] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,534] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,538] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,538] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,542] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,542] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,542] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,542] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,543] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,542] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,543] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,543] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,544] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,544] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,545] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,545] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,546] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,547] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 10:27:01,547] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,661] INFO [GroupCoordinator 4]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-9e0d337e-78aa-4c00-8b29-54e9167b0117 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,667] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-9e0d337e-78aa-4c00-8b29-54e9167b0117 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,902] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,903] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,903] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,903] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,904] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,904] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,904] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,904] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,904] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,904] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,905] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,905] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,905] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,905] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,905] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,905] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,906] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,906] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,906] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,906] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,906] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,906] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,907] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,907] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,907] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,907] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,907] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,907] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,908] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,908] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,908] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,908] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:04,672] INFO [GroupCoordinator 4]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:04,704] INFO [GroupCoordinator 4]: Assignment received from leader sr-1-9e0d337e-78aa-4c00-8b29-54e9167b0117 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:36:50,474] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:36:50,902] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:04:30,902] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:04:30,943] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:04:30,956] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:04:30,957] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:14:30,942] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:14:30,947] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:31:29,361] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000007243-0000000003 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 11:31:29,516] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000007243-0000000003 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 11:32:28,041] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-9e0d337e-78aa-4c00-8b29-54e9167b0117 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:32:28,044] INFO [GroupCoordinator 4]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:32:28,056] INFO [GroupCoordinator 4]: Member MemberMetadata(memberId=sr-1-9e0d337e-78aa-4c00-8b29-54e9167b0117, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.14, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:27,309] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 11:34:27,770] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 11:34:27,783] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 11:34:27,793] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,186] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 11:34:32,450] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 11:34:32,460] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,673] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,687] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,726] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 11:34:32,736] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 11:34:32,744] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 11:34:32,755] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,876] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,877] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,878] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,905] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 11:34:32,953] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 11:34:32,964] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:32,974] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:33,060] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1969) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:33,061] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 11:34:33,061] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 11:34:33,082] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,085] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-24 11:34:33,112] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:33,158] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1103525925 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:33,185] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,188] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:33,191] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:33,209] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:33,211] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:33,210] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:33,213] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:33,245] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 11:34:33,252] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 11:34:33,276] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:33,295] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,303] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 11:34:33,402] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,504] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,612] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,687] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 11:34:33,723] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,746] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 11:34:33,747] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 11:34:33,769] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 11:34:33,808] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:33,848] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,886] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:33,893] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:33,899] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:33,901] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:33,914] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:33,915] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,083] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,083] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,090] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,122] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 11:34:34,127] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,130] INFO [RaftManager id=4] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1969) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:34,132] INFO [BrokerLifecycleManager id=4] Incarnation 566eob1CRuyU1_jVO7b8lw of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,187] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,199] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,200] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:34,234] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,279] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,281] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,285] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,286] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 11:34:34,287] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,291] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,294] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:34,308] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,292] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,343] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 1 (org.apache.kafka.raft.FollowerState)
[2025-12-24 11:34:34,346] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,360] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,384] INFO [MetadataLoader id=4] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,391] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,395] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,402] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,410] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=1) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 11:34:34,422] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 11:34:34,436] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 11:34:34,437] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,455] INFO Loaded 0 logs in 28ms (kafka.log.LogManager)
[2025-12-24 11:34:34,459] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 11:34:34,462] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 11:34:34,478] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 11:34:34,675] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 11:34:34,683] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 11:34:34,684] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 11:34:34,685] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:34,687] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:34,689] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 11:34:34,694] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 11:34:34,695] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 11:34:34,722] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,763] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,767] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 11:34:34,768] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 11:34:34,768] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 11:34:34,770] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 11:34:34,775] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:34,784] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,791] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 11:34:34,854] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,864] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 11:34:34,867] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 11:34:34,868] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 11:34:34,870] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 11:34:34,874] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 11:34:34,879] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 11:34:34,889] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,889] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,889] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 11:34:34,890] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 11:34:34,890] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 11:34:34,891] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:34,893] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:34,893] INFO Kafka startTimeMs: 1766576074891 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:34,895] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 11:34:38,978] INFO Sent auto-creation request for Set(_schemas) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 11:34:39,365] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-24 11:34:39,369] INFO [Broker id=4] Creating new partition _schemas-0 with topic id 0IctBlPsRtCyKBZU8rL83Q. (state.change.logger)
[2025-12-24 11:34:39,385] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:39,389] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 11:34:39,394] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 11:34:39,399] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:39,401] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:39,405] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:39,408] INFO [Broker id=4] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 11:34:39,426] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:39,429] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(0IctBlPsRtCyKBZU8rL83Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:39,431] INFO [Broker id=4] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 11:34:39,433] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:39,435] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:39,450] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 11:34:40,346] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-24 11:34:40,353] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-46, __consumer_offsets-43, __consumer_offsets-9, __consumer_offsets-23, __consumer_offsets-17, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-38, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:40,356] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,370] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,375] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,376] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 11:34:40,377] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,379] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,390] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,393] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,395] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,397] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 11:34:40,398] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,398] INFO [Broker id=4] Leader __consumer_offsets-14 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,406] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,413] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,415] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,415] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 11:34:40,416] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,417] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,424] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,428] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,430] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,434] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 11:34:40,434] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,434] INFO [Broker id=4] Leader __consumer_offsets-43 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,439] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,446] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,447] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,448] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 11:34:40,448] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,448] INFO [Broker id=4] Leader __consumer_offsets-9 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,456] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,463] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,471] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,473] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 11:34:40,476] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,478] INFO [Broker id=4] Leader __consumer_offsets-23 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,488] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,498] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,499] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,500] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 11:34:40,501] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,502] INFO [Broker id=4] Leader __consumer_offsets-17 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,508] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,512] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,516] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,517] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 11:34:40,517] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,518] INFO [Broker id=4] Leader __consumer_offsets-18 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,532] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,534] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,540] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,541] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 11:34:40,542] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,544] INFO [Broker id=4] Leader __consumer_offsets-31 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,549] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,559] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,560] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,560] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 11:34:40,561] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,561] INFO [Broker id=4] Leader __consumer_offsets-28 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,575] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,584] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,585] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,586] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 11:34:40,586] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,587] INFO [Broker id=4] Leader __consumer_offsets-25 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,596] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,603] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,606] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,608] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 11:34:40,608] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,609] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,622] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,628] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,628] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,629] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 11:34:40,630] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,631] INFO [Broker id=4] Leader __consumer_offsets-6 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,638] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,641] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,642] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,643] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 11:34:40,643] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,645] INFO [Broker id=4] Leader __consumer_offsets-38 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,662] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,670] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,675] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,676] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 11:34:40,676] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,677] INFO [Broker id=4] Leader __consumer_offsets-35 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,684] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,688] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,690] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,691] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 11:34:40,691] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,692] INFO [Broker id=4] Leader __consumer_offsets-4 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,700] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,706] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,707] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,707] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 11:34:40,707] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,708] INFO [Broker id=4] Leader __consumer_offsets-1 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,722] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-24 11:34:40,723] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,725] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,730] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,732] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 11:34:40,733] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,734] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,734] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,746] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,748] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,748] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 11:34:40,750] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,750] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,750] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,759] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,762] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,763] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 11:34:40,763] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,763] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,763] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,768] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,776] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,777] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 11:34:40,777] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,777] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,778] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,859] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,887] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,890] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 11:34:40,900] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,903] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,905] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,921] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,922] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,922] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 11:34:40,923] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,923] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,924] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,930] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,932] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,932] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 11:34:40,933] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,933] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,934] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,937] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,939] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,939] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 11:34:40,940] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,942] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,943] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,948] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,949] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,949] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 11:34:40,950] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,952] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,953] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,956] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,958] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,958] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 11:34:40,958] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,959] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,959] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,962] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,963] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,964] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 11:34:40,964] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,964] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,965] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,969] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,973] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,974] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 11:34:40,974] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,974] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,975] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,982] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,983] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,984] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 11:34:40,985] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,985] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,986] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,993] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,997] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,997] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 11:34:40,997] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,998] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,998] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,008] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,009] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,011] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 11:34:41,012] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,013] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,013] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,018] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,024] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,024] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 11:34:41,025] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,025] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,027] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,030] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,032] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,033] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 11:34:41,033] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,034] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,034] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,039] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,042] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,042] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 11:34:41,043] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,044] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,045] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,051] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,053] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,053] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 11:34:41,053] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,054] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,054] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,057] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,058] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,059] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 11:34:41,059] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,059] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,060] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,066] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,067] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,068] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 11:34:41,069] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,069] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,069] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,072] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,074] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,077] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 11:34:41,080] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,081] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,081] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,091] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,092] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,092] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 11:34:41,093] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,094] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,096] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,105] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,107] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,108] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 11:34:41,108] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,108] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,108] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,119] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,120] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,122] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 11:34:41,123] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,124] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,124] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,134] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,136] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,137] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 11:34:41,137] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,137] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,138] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,141] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,142] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,142] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,142] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,143] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,143] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,145] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,146] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,147] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 11:34:41,147] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,148] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,148] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,156] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,161] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,161] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 11:34:41,162] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,162] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,162] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,166] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,167] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,168] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 11:34:41,168] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,168] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,169] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,171] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,172] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,172] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 11:34:41,172] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,172] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,173] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,178] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,179] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,179] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 11:34:41,179] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,179] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,179] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,182] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,183] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,184] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 11:34:41,184] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,184] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,185] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-13, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,185] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 11:34:41,186] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,190] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,191] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,191] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 11:34:41,191] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,192] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,192] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,192] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,193] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,193] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,193] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,193] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,193] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,194] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,194] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,194] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,194] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,194] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,194] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,195] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,195] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,195] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,196] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,197] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,197] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,198] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,198] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,198] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,199] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,199] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,199] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,199] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,199] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,199] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,200] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,200] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,200] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,200] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,203] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,204] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,205] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,205] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,205] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,205] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,206] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,206] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,206] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,207] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,207] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,207] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,207] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,207] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,207] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,208] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,208] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,208] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 4 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,208] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,208] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-14 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,209] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,209] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,209] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,210] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,209] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,210] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,211] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,211] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,211] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,211] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-43 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,211] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,212] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,212] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,212] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,212] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,213] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,213] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,213] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,214] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,213] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,215] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-17 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,215] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-18 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,214] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,216] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,216] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,217] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-28 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,217] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-25 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,218] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,218] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,218] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,219] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,218] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,220] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,220] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,220] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,220] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-6 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,221] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,221] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,221] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,222] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,222] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,222] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,222] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,221] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-38 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,222] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,223] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,223] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-35 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,223] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,224] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,223] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-4 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,225] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-1 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,224] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,226] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,227] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,227] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,227] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,227] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,227] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,227] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,228] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,228] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,228] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,228] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,229] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,229] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,229] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,229] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,229] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,229] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,229] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,229] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,230] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,230] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,230] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,230] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,230] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,231] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,231] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,231] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,231] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,231] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,232] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,232] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,232] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,232] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,233] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,233] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,233] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,233] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,233] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,234] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,234] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,234] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,234] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,234] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,234] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,234] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,235] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,235] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 11:34:41,233] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,236] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,237] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,237] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,237] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,237] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,239] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,239] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,239] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,239] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,239] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,242] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,683] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,684] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,684] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,684] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,684] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,685] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,685] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,685] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,685] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,686] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,686] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,686] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,686] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,686] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,686] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,686] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,687] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,687] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,687] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,688] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,688] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,689] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,689] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,690] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,690] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,690] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,691] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,691] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,691] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,691] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,692] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,692] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:44:34,193] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:44:34,195] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:44:39,281] INFO [NodeToControllerChannelManager id=4 name=forwarding] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:25:13,599] INFO [RaftManager id=4] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5986, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:25:13,628] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:25:13,687] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5986, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:25:13,734] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:30:26,434] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:26,438] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 6553 due to node 2 being disconnected (elapsed time since creation: 253453ms, elapsed time since send: 253452ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:26,481] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:26,522] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:26,533] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:26,558] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 6106 (kafka.log.UnifiedLog)
[2025-12-24 12:30:26,569] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 6106 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 12:30:26,569] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6106 (kafka.log.UnifiedLog$)
[2025-12-24 12:30:26,574] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 6106 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:30:26,574] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 5ms for segment recovery from offset 6106 (kafka.log.UnifiedLog$)
[2025-12-24 12:30:26,575] INFO [RaftManager id=4] Truncated to offset 6106 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 12:30:27,047] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:27,049] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:30:59,001] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6165, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6165, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:59,878] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:59,884] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:35:22,273] INFO [RaftManager id=4] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,289] INFO [RaftManager id=4] Completed transition to Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,318] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,892] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:35:22,896] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:40:53,626] INFO [RaftManager id=4] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,648] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,663] INFO [RaftManager id=4] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,675] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,680] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 6405 (kafka.log.UnifiedLog)
[2025-12-24 12:40:53,694] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 6405 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,695] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6405 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,698] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=6106, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000006106.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:40:53,712] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 6405 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:40:53,713] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 14ms for snapshot load and 3ms for segment recovery from offset 6405 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,713] INFO [RaftManager id=4] Truncated to offset 6405 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 12:40:54,491] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:40:54,494] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:47:56,427] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000007247-0000000009 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 12:47:56,607] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000007247-0000000009 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 12:50:53,843] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:50:53,844] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:28,598] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Disconnecting from node 6 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:28,624] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Cancelled in-flight FETCH request with correlation id 13168 due to node 6 being disconnected (elapsed time since creation: 844804ms, elapsed time since send: 844804ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:28,632] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Client requested connection close from node 6 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:28,713] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Error sending fetch request (sessionId=94211298, epoch=13168) to node 6: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-12-24 13:52:28,817] INFO [RaftManager id=4] Completed transition to Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13281, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 13:52:28,884] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=94211298, epoch=13168), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-12-24 13:52:28,914] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13281, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 13:52:29,601] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:29,603] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:01:51,132] INFO [RaftManager id=4] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14357, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:01:51,184] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=11, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14357, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:01:51,600] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:01:51,601] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:02:23,346] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000014422-0000000011 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 14:02:23,399] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000014422-0000000011 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 14:06:56,202] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 14:06:56,465] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 14:06:56,504] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 14:06:56,513] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,342] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 14:07:02,546] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 14:07:02,564] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,878] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,923] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,978] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 14:07:02,991] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 14:07:02,995] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 14:07:03,009] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:03,136] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:03,138] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:03,140] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:03,167] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 14:07:03,211] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 14:07:03,218] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,226] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,302] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1809) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:03,305] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 14:07:03,337] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 14:07:03,361] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,362] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-24 14:07:03,372] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:03,426] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,427] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,428] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,443] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@441974924 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,449] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,460] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,446] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,462] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 14:07:03,465] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,465] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 14:07:03,509] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:03,551] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 14:07:03,583] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,687] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,710] INFO [RaftManager id=4] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1809) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:03,794] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,900] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,958] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 14:07:04,001] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,021] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 14:07:04,029] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 14:07:04,053] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 14:07:04,073] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,089] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,103] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,104] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,114] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,115] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,118] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,270] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,271] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,273] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,277] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,299] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,303] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,335] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 14:07:04,348] INFO [BrokerLifecycleManager id=4] Incarnation bqKrz4WdQiOi3Ubjgw8YXQ of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:04,353] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,370] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,374] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,442] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 14:07:04,446] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 14:07:04,445] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,448] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 14:07:04,456] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,465] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,550] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,644] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:04,652] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,674] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,675] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,678] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,678] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,685] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,687] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,721] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-24 14:07:04,723] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,738] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,754] INFO [MetadataLoader id=4] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,759] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,760] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,760] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,760] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=2) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 14:07:04,761] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 14:07:04,764] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 14:07:04,771] INFO Loaded 0 logs in 9ms (kafka.log.LogManager)
[2025-12-24 14:07:04,773] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 14:07:04,774] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 14:07:04,777] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 14:07:04,801] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 5 (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,092] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 14:07:05,095] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 14:07:05,100] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 14:07:05,101] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:05,107] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:05,109] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 14:07:05,110] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 14:07:05,110] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 14:07:05,119] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:05,139] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,140] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 14:07:05,140] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 14:07:05,140] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 14:07:05,142] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 14:07:05,147] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:05,154] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,155] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 14:07:05,204] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,205] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 14:07:05,207] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 14:07:05,207] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 14:07:05,208] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 14:07:05,209] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 14:07:05,211] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 14:07:05,218] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 14:07:05,218] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 14:07:05,219] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 14:07:05,219] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 14:07:05,220] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 14:07:05,222] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,223] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,227] INFO Kafka startTimeMs: 1766585225220 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,237] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 14:07:09,271] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-24 14:07:09,276] INFO [Broker id=4] Creating new partition _schemas-0 with topic id zuwcO5jNTBGzR0UELwhRXw. (state.change.logger)
[2025-12-24 14:07:09,307] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:09,318] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 14:07:09,323] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 14:07:09,326] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:09,328] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:09,333] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:09,337] INFO [Broker id=4] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 14:07:09,433] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:09,435] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(zuwcO5jNTBGzR0UELwhRXw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:09,443] INFO [Broker id=4] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 14:07:09,449] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:09,452] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:09,481] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 14:07:10,521] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 14:07:10,622] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 14:07:10,725] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 14:07:10,774] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-24 14:07:10,777] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-47, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-22, __consumer_offsets-17, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:10,778] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,790] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,792] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,793] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 14:07:10,795] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,797] INFO [Broker id=4] Leader __consumer_offsets-47 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,806] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,810] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,811] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,811] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 14:07:10,812] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,813] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,821] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,828] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,830] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,831] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 14:07:10,832] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,833] INFO [Broker id=4] Leader __consumer_offsets-13 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,840] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,845] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,846] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,846] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 14:07:10,848] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,848] INFO [Broker id=4] Leader __consumer_offsets-11 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,861] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,867] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,869] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,872] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 14:07:10,872] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,873] INFO [Broker id=4] Leader __consumer_offsets-44 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,878] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,880] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,881] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,882] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 14:07:10,883] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,884] INFO [Broker id=4] Leader __consumer_offsets-22 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,889] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,892] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,893] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,893] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 14:07:10,893] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,894] INFO [Broker id=4] Leader __consumer_offsets-17 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,897] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,901] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,902] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,903] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 14:07:10,903] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,903] INFO [Broker id=4] Leader __consumer_offsets-18 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,908] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,910] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,912] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,913] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,913] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,914] INFO [Broker id=4] Leader __consumer_offsets-0 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,916] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,919] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,923] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,923] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 14:07:10,923] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,923] INFO [Broker id=4] Leader __consumer_offsets-32 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,929] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,934] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,936] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,936] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 14:07:10,937] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,937] INFO [Broker id=4] Leader __consumer_offsets-28 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,942] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,945] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,946] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,947] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 14:07:10,947] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,948] INFO [Broker id=4] Leader __consumer_offsets-26 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,953] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,956] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,957] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,957] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 14:07:10,958] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,958] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,963] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,966] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,967] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,967] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 14:07:10,967] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,968] INFO [Broker id=4] Leader __consumer_offsets-8 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,971] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,974] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,975] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,975] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 14:07:10,976] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,977] INFO [Broker id=4] Leader __consumer_offsets-37 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,981] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,985] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,986] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,986] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 14:07:10,987] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,987] INFO [Broker id=4] Leader __consumer_offsets-35 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,990] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,993] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,993] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,994] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 14:07:10,994] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,994] INFO [Broker id=4] Leader __consumer_offsets-4 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,998] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-24 14:07:10,998] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,001] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,002] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,002] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 14:07:11,002] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,002] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,003] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,006] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,007] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,007] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 14:07:11,007] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,008] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,008] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,012] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,013] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,013] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 14:07:11,014] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,014] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,014] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,017] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,019] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,020] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 14:07:11,020] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,022] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,022] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,025] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,025] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,026] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 14:07:11,026] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,027] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,027] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,031] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,032] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,032] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 14:07:11,033] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,033] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,033] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,042] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,043] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,043] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 14:07:11,044] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,044] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,044] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,047] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,050] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,050] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 14:07:11,050] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,051] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,052] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,056] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,057] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,057] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 14:07:11,058] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,058] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,059] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,061] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,063] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,065] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 14:07:11,065] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,066] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,066] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,071] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,072] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,072] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 14:07:11,073] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,073] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,074] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,080] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,081] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,082] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 14:07:11,084] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,085] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,086] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,092] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,094] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,094] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 14:07:11,095] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,096] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,096] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,099] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,100] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,100] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 14:07:11,101] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,101] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,102] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,108] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,109] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,112] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 14:07:11,113] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,113] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,113] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,116] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,116] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,117] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 14:07:11,117] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,117] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,118] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,121] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,122] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,123] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 14:07:11,123] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,124] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,124] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,128] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,129] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,129] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 14:07:11,129] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,130] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,130] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,135] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,136] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,136] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 14:07:11,136] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,137] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,137] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,139] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,140] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,141] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 14:07:11,141] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,141] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,142] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,145] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,147] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,147] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 14:07:11,148] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,148] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,149] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,154] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,154] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,155] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 14:07:11,155] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,156] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,156] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,161] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,162] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,163] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 14:07:11,163] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,164] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,164] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,166] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,168] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,169] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 14:07:11,169] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,169] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,170] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,174] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,176] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,176] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 14:07:11,176] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,176] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,177] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,179] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,180] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,180] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 14:07:11,181] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,181] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,181] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,183] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,184] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,184] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 14:07:11,185] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,185] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,185] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,189] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,190] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,191] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 14:07:11,191] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,192] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,192] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,196] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,198] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,198] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 14:07:11,199] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,199] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,199] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,201] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,202] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,202] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 14:07:11,202] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,203] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,203] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,205] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,207] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,207] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 14:07:11,208] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,208] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,208] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,211] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,211] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,211] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 14:07:11,212] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,212] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,212] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,217] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,217] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,217] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 14:07:11,218] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,218] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,218] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-30, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-31, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-6, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,218] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 14:07:11,219] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-16 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,222] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,223] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 14:07:11,223] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,225] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,226] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,227] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,227] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,227] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,228] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,228] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,229] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,229] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,229] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,230] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,230] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,230] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,230] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,231] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,231] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,232] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,232] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,233] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,233] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,233] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,234] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,234] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,234] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,235] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,236] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,236] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,237] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,237] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,237] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,237] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,237] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,238] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,238] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,242] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,244] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,245] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,245] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,245] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,245] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,246] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,248] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,249] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,249] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,250] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,250] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,250] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,251] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,251] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,251] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,251] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-47 in 6 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,252] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,251] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,252] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,252] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-13 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,254] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,253] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,254] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-44 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,254] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,255] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,255] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-22 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,256] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,255] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,256] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,257] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,257] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,258] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,258] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,259] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,258] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-32 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,259] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-28 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,259] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,260] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,260] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-26 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,260] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,260] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,261] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,260] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,261] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,261] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,261] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,261] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,262] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,262] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,263] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,264] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,265] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,266] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,266] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,266] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,267] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,267] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,267] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,267] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,268] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,268] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,268] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,268] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,269] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,269] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,269] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,269] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,269] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,269] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,270] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,270] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,270] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,270] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,271] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,271] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,271] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,271] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,272] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,272] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,272] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,272] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,272] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,273] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,273] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,273] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,273] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,274] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,273] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,274] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,275] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,275] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,275] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,276] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,275] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,276] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,277] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,278] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,277] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,279] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,280] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,280] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,281] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,281] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,281] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,281] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,281] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,282] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,283] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,283] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,283] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,283] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,283] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,283] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,284] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,284] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,286] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,286] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,286] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,287] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,287] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,287] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,287] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,287] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,288] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,288] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,288] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,288] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,288] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,289] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,289] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,289] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,289] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,289] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,290] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,290] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,291] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,291] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,292] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,291] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,292] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,292] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,292] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,293] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 14:07:11,293] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,370] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,370] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,370] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,370] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,370] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,370] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,371] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,371] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,371] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,371] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,371] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,372] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,372] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,373] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,374] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,374] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,375] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,375] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,375] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,375] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,375] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,376] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,376] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,376] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,376] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,376] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,377] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,377] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,377] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,377] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,377] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,377] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:17:04,494] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:10,844] INFO [NodeToControllerChannelManager id=4 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:24,710] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1298, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:17:24,811] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1298, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:17:26,418] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:26,420] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,547] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:26:55,611] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 2429 (kafka.log.UnifiedLog)
[2025-12-24 14:26:55,658] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 2429 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,662] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2429 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,676] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2429 with 0 producer ids in 7 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:26:55,677] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 13ms for segment recovery from offset 2429 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,678] INFO [RaftManager id=4] Truncated to offset 2429 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:26:56,972] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:26:56,976] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:35:24,681] INFO [Broker id=4] Transitioning 2 partition(s) to local leaders. (state.change.logger)
[2025-12-24 14:35:24,692] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-1, financial_transactions-4) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,693] INFO [Broker id=4] Creating new partition financial_transactions-1 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,730] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,736] INFO Created log for partition financial_transactions-1 in /tmp/kafka-logs/financial_transactions-1 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,739] INFO [Partition financial_transactions-1 broker=4] No checkpointed highwatermark is found for partition financial_transactions-1 (kafka.cluster.Partition)
[2025-12-24 14:35:24,740] INFO [Partition financial_transactions-1 broker=4] Log loaded for partition financial_transactions-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,741] INFO [Broker id=4] Leader financial_transactions-1 with topic id Some(EMZmT9B4Rquk5_L89orVRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:35:24,753] INFO [Broker id=4] Creating new partition financial_transactions-4 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,760] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,762] INFO Created log for partition financial_transactions-4 in /tmp/kafka-logs/financial_transactions-4 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,763] INFO [Partition financial_transactions-4 broker=4] No checkpointed highwatermark is found for partition financial_transactions-4 (kafka.cluster.Partition)
[2025-12-24 14:35:24,764] INFO [Partition financial_transactions-4 broker=4] Log loaded for partition financial_transactions-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,764] INFO [Broker id=4] Leader financial_transactions-4 with topic id Some(EMZmT9B4Rquk5_L89orVRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:35:24,769] INFO [Broker id=4] Transitioning 3 partition(s) to local followers. (state.change.logger)
[2025-12-24 14:35:24,770] INFO [Broker id=4] Creating new partition financial_transactions-2 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,779] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,780] INFO Created log for partition financial_transactions-2 in /tmp/kafka-logs/financial_transactions-2 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,780] INFO [Partition financial_transactions-2 broker=4] No checkpointed highwatermark is found for partition financial_transactions-2 (kafka.cluster.Partition)
[2025-12-24 14:35:24,781] INFO [Partition financial_transactions-2 broker=4] Log loaded for partition financial_transactions-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,781] INFO [Broker id=4] Follower financial_transactions-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,781] INFO [Broker id=4] Creating new partition financial_transactions-3 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,786] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,788] INFO Created log for partition financial_transactions-3 in /tmp/kafka-logs/financial_transactions-3 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,788] INFO [Partition financial_transactions-3 broker=4] No checkpointed highwatermark is found for partition financial_transactions-3 (kafka.cluster.Partition)
[2025-12-24 14:35:24,788] INFO [Partition financial_transactions-3 broker=4] Log loaded for partition financial_transactions-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,788] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,788] INFO [Broker id=4] Creating new partition financial_transactions-0 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,794] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,795] INFO Created log for partition financial_transactions-0 in /tmp/kafka-logs/financial_transactions-0 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,795] INFO [Partition financial_transactions-0 broker=4] No checkpointed highwatermark is found for partition financial_transactions-0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,795] INFO [Partition financial_transactions-0 broker=4] Log loaded for partition financial_transactions-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,795] INFO [Broker id=4] Follower financial_transactions-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,796] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-0, financial_transactions-2, financial_transactions-3) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,796] INFO [Broker id=4] Stopped fetchers as part of become-follower for 3 partitions (state.change.logger)
[2025-12-24 14:35:24,805] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(financial_transactions-0 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,805] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(financial_transactions-2 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-3 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,806] INFO [Broker id=4] Started fetchers as part of become-follower for 3 partitions (state.change.logger)
[2025-12-24 14:35:24,992] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:24,995] INFO [UnifiedLog partition=financial_transactions-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:35:24,995] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:24,998] INFO [UnifiedLog partition=financial_transactions-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:35:25,048] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition financial_transactions-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:25,058] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:36:55,576] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:53:22,438] INFO [RaftManager id=4] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,555] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,606] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,608] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 5421 (kafka.log.UnifiedLog)
[2025-12-24 14:53:22,626] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 5421 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,627] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 5421 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,627] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=2429, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000002429.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:53:22,639] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 5421 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:53:22,640] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 4ms for segment recovery from offset 5421 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,640] INFO [RaftManager id=4] Truncated to offset 5421 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:53:23,557] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:53:23,558] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 15:03:22,491] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 15:03:22,814] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 15:08:34,714] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000007238-0000000007 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 15:08:34,978] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000007238-0000000007 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 16:18:13,871] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:13,902] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:13,950] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,049] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:14,050] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,054] INFO [RaftManager id=4] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:18:14,098] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:18:14,100] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,157] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:14,159] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,210] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:21:54,930] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000014409-0000000008 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 16:21:55,008] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000014409-0000000008 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 16:28:13,783] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:28:14,195] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,424] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,435] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 17043 due to node 2 being disconnected (elapsed time since creation: 1021269ms, elapsed time since send: 1021268ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,535] INFO [RaftManager id=4] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=15282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:46:14,564] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=15282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:46:15,126] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:15,131] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:56:14,842] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,422] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,444] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,494] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,560] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,562] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,613] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,618] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,619] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,623] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=21100, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=21100, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 17:34:58,641] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 21100 (kafka.log.UnifiedLog)
[2025-12-24 17:34:58,661] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 21100 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 17:34:58,661] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 21100 (kafka.log.UnifiedLog$)
[2025-12-24 17:34:58,662] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=5421, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000005421.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 17:34:58,670] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,679] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 21100 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 17:34:58,680] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 6ms for snapshot load and 12ms for segment recovery from offset 21100 (kafka.log.UnifiedLog$)
[2025-12-24 17:34:58,680] INFO [RaftManager id=4] Truncated to offset 21100 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 17:38:55,956] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000021573-0000000013 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 17:38:56,016] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000021573-0000000013 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 17:44:58,579] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,032] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,153] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 31197 due to node 2 being disconnected (elapsed time since creation: 2118ms, elapsed time since send: 2050ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,480] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,482] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 7119 due to node 2 being disconnected (elapsed time since creation: 4508ms, elapsed time since send: 4508ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,503] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,661] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-24 18:35:43,799] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,802] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,853] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,885] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,887] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,922] INFO [RaftManager id=4] Completed transition to Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:44,058] INFO [RaftManager id=4] Completed transition to Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:44,325] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:44,353] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:44,400] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:44,404] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:44,455] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:38:56,077] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000028734-0000000016 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 18:38:56,159] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000028734-0000000016 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 18:45:44,043] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:45:44,297] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,667] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,703] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 20:32:35,753] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 20:32:35,802] INFO [RaftManager id=4] Completed transition to Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33975, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 20:32:35,823] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,865] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33975, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 20:32:35,928] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 21:41:15,984] INFO [RaftManager id=4] Completed transition to Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,074] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,090] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=19, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,095] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 34034 (kafka.log.UnifiedLog)
[2025-12-24 21:41:16,117] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 34034 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 21:41:16,119] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 34034 (kafka.log.UnifiedLog$)
[2025-12-24 21:41:16,120] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=21100, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000021100.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 21:41:16,145] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 34034 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 21:41:16,145] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 10ms for snapshot load and 15ms for segment recovery from offset 34034 (kafka.log.UnifiedLog$)
[2025-12-24 21:41:16,145] INFO [RaftManager id=4] Truncated to offset 34034 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 21:41:16,939] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 21:41:16,942] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 22:57:16,683] INFO [RaftManager id=4] Completed transition to Unattached(epoch=20, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=19, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34091, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 22:57:16,730] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=20, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34091, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=20, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 22:57:17,689] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 22:57:17,695] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 23:41:42,715] INFO [RaftManager id=4] Completed transition to Unattached(epoch=21, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=20, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 23:41:42,729] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=21, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=21, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 23:41:44,254] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 23:41:44,298] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 00:44:16,911] INFO [RaftManager id=4] Completed transition to Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=21, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:16,921] INFO [RaftManager id=4] Completed transition to Unattached(epoch=23, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:16,962] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=23, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=23, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:18,671] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 00:44:18,673] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:19:59,867] INFO [RaftManager id=4] Completed transition to Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=23, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:19:59,889] INFO [RaftManager id=4] Completed transition to Unattached(epoch=25, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:19:59,914] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=25, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=25, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:20:01,376] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:20:01,381] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:29:59,999] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:30:00,006] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:06,036] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:06,154] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:06,980] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:07,088] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:07,531] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:07,594] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:07,945] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:07,961] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:08,154] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:08,159] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 39507 due to node 2 being disconnected (elapsed time since creation: 2027ms, elapsed time since send: 2018ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:08,184] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 01:31:08,231] INFO [RaftManager id=4] Completed transition to Unattached(epoch=29, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=25, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=35590, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,254] INFO [RaftManager id=4] Completed transition to Unattached(epoch=30, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=29, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,293] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:08,312] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=30, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=35590, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=30, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,321] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 35590 (kafka.log.UnifiedLog)
[2025-12-25 01:31:08,344] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 35590 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,345] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 35590 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,346] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=34034, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000034034.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:31:08,392] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 35590 with 0 producer ids in 10 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:31:08,393] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 12ms for snapshot load and 36ms for segment recovery from offset 35590 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,393] INFO [RaftManager id=4] Truncated to offset 35590 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 01:31:08,395] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:33:39,378] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000035891-0000000030 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 01:33:39,431] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000035891-0000000030 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 01:54:59,170] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,180] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 40726 due to node 1 being disconnected (elapsed time since creation: 899745ms, elapsed time since send: 899745ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,264] INFO [RaftManager id=4] Completed transition to Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=30, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=36648, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:54:59,341] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=31, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=36648, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:54:59,682] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,683] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 02:04:59,450] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 02:04:59,502] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 02:48:38,286] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000043059-0000000031 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 02:48:38,459] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000043059-0000000031 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 03:46:47,763] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:47,853] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 55200 due to node 3 being disconnected (elapsed time since creation: 2138ms, elapsed time since send: 2138ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,139] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,175] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 12574 due to node 3 being disconnected (elapsed time since creation: 4564ms, elapsed time since send: 4564ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,448] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:46:49,858] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 03:46:53,050] INFO [RaftManager id=4] Completed transition to Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=31, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=50007, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 03:46:53,466] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=34, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=50007, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 03:46:53,495] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 50008 (kafka.log.UnifiedLog)
[2025-12-25 03:46:53,497] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2025-12-25 03:46:53,615] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 50008 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 03:46:53,638] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 50008 (kafka.log.UnifiedLog$)
[2025-12-25 03:46:53,646] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=35590, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000035590.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 03:46:53,696] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 50008 with 0 producer ids in 18 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 03:46:53,698] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 11ms for snapshot load and 44ms for segment recovery from offset 50008 (kafka.log.UnifiedLog$)
[2025-12-25 03:46:53,702] INFO [RaftManager id=4] Truncated to offset 50008 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 03:46:53,703] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:53,725] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:48:38,637] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000050217-0000000034 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 03:48:38,701] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000050217-0000000034 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 03:56:51,370] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:56:53,550] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 04:48:39,115] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000057390-0000000034 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 04:48:39,253] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000057390-0000000034 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 04:54:06,174] INFO [RaftManager id=4] Completed transition to Unattached(epoch=35, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=34, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 04:54:06,399] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=35, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=35, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 04:54:07,066] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 04:54:07,081] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:10:03,417] INFO [RaftManager id=4] Completed transition to Unattached(epoch=36, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=35, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,480] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=36, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=36, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,495] INFO [RaftManager id=4] Completed transition to Unattached(epoch=37, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=36, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,506] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=37, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=37, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,509] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 58151 (kafka.log.UnifiedLog)
[2025-12-25 05:10:03,528] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 58151 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,529] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 58151 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,530] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=50008, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000050008.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:10:03,582] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 58151 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:10:03,583] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 6ms for snapshot load and 47ms for segment recovery from offset 58151 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,583] INFO [RaftManager id=4] Truncated to offset 58151 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 05:10:05,270] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:10:05,273] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:27:01,627] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,632] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 64349 due to node 3 being disconnected (elapsed time since creation: 898558ms, elapsed time since send: 898558ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,687] INFO [RaftManager id=4] Completed transition to Unattached(epoch=38, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=37, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,752] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=38, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=38, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,768] INFO [RaftManager id=4] Completed transition to Unattached(epoch=39, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=38, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,785] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=39, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=39, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,787] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 58390 (kafka.log.UnifiedLog)
[2025-12-25 05:27:01,792] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 58390 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,793] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 58390 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,794] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=58151, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000058151.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:27:01,802] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 58390 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:27:01,803] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 7ms for segment recovery from offset 58390 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,804] INFO [RaftManager id=4] Truncated to offset 58390 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 05:27:01,903] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,904] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:37:01,991] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:37:02,002] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 06:18:38,659] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000064553-0000000039 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 06:18:38,757] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000064553-0000000039 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 07:18:38,954] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000071718-0000000039 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 07:18:39,018] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000071718-0000000039 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 07:58:34,614] INFO [RaftManager id=4] Completed transition to Unattached(epoch=40, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=39, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=75411, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 07:58:34,707] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=40, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=75411, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=40, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 07:58:35,597] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 07:58:35,600] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:01:41,996] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:01:44,716] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:01:44,770] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:01:44,789] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:52,330] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:01:52,633] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:01:52,653] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,112] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,119] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,169] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-25 12:01:53,175] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-25 12:01:53,176] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-25 12:01:53,183] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,268] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:53,270] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:53,282] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:53,299] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-25 12:01:53,321] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-25 12:01:53,330] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:53,343] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:53,423] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1671) from null (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:01:53,432] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-25 12:01:53,432] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-25 12:01:53,458] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-25 12:01:53,459] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:53,477] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,577] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:53,584] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:53,581] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:53,581] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:53,581] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:53,727] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:01:53,745] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:01:53,770] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:53,805] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:01:53,809] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-25 12:01:53,809] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1146069111 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:53,835] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:53,884] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:53,889] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:53,974] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:53,985] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,082] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,129] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,143] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,251] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,278] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,287] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,296] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,299] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,308] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,309] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,370] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,423] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,432] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,437] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,449] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,475] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,536] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,539] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,622] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,729] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,773] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,775] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,778] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,780] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,794] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,795] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,849] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,001] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,219] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,406] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,526] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,562] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,545] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,685] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,701] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,700] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,727] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,739] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,834] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,962] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,022] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:01:56,159] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,255] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:56,260] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:56,281] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:56,293] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:56,290] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,427] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,543] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,671] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,988] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,001] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,045] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,185] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,186] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,273] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,712] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,716] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-25 12:01:57,778] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:01:57,847] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,869] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,889] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:58,052] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,166] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,274] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,385] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,540] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,737] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-25 12:01:59,102] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,209] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,272] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:01:59,315] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,417] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,437] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:01:59,569] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,762] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,796] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:01:59,871] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,916] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:01:59,948] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:01:59,980] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,005] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:00,092] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,111] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:00,252] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,359] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,482] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,589] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:00,590] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:00,621] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,729] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,347] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,413] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-25 12:02:01,435] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:01,440] INFO [RaftManager id=4] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1671) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:01,461] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,474] INFO [BrokerLifecycleManager id=4] Incarnation G1MGr_z5SLS9tyr-oTCwng of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:01,483] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:01,516] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,531] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,552] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,554] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,568] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:02:01,574] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:02:01,575] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:02:01,568] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,683] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,754] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:01,789] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,940] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,052] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,111] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,112] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,210] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,277] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,292] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,321] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,422] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,526] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,638] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,759] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,872] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,920] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,929] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,953] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,964] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,991] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,114] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,225] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,343] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,452] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,455] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,456] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,565] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,666] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,834] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,940] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,010] INFO [RaftManager id=4] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:04,040] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,046] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:04,047] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:04,141] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,244] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,347] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,449] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,560] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,577] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:04,643] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,657] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,658] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,665] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,680] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,792] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,852] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 7 (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:04,894] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,997] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,010] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=6, metadata=Optional.empty)] for the first time for epoch 4 (org.apache.kafka.raft.FollowerState)
[2025-12-25 12:02:05,105] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,145] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,166] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,178] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,195] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,196] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,203] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=4) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-25 12:02:05,205] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-25 12:02:05,216] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-25 12:02:05,222] INFO Loaded 0 logs in 16ms (kafka.log.LogManager)
[2025-12-25 12:02:05,223] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-25 12:02:05,236] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-25 12:02:05,287] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-25 12:02:05,759] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-25 12:02:05,997] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-25 12:02:06,040] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-25 12:02:06,110] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:06,146] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:06,336] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:02:06,368] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:02:06,389] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-25 12:02:06,456] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:06,767] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:06,774] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:02:06,780] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:02:06,787] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:02:06,812] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:06,827] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:02:06,866] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:02:06,889] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:02:07,092] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:07,097] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:02:07,120] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:02:07,124] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:02:07,127] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-25 12:02:07,137] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:02:07,141] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:02:07,154] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:02:07,156] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:02:07,157] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:02:07,157] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:02:07,160] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-25 12:02:07,166] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:07,174] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:07,176] INFO Kafka startTimeMs: 1766664127165 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:07,180] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-25 12:02:21,308] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:02:21,342] INFO [Broker id=4] Creating new partition _schemas-0 with topic id 5ynfWlfWR8GWI1r7sCBFzA. (state.change.logger)
[2025-12-25 12:02:21,439] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:21,456] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-25 12:02:21,470] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-25 12:02:21,476] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:21,491] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:21,494] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:21,496] INFO [Broker id=4] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:02:21,579] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:21,592] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(5ynfWlfWR8GWI1r7sCBFzA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:21,595] INFO [Broker id=4] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:02:21,601] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:21,622] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:21,651] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:02:22,764] INFO [Broker id=4] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-12-25 12:02:22,812] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-9, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-36, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:22,815] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:22,896] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:22,920] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:22,920] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-25 12:02:22,922] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:22,929] INFO [Broker id=4] Leader __consumer_offsets-45 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:22,942] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:22,962] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:22,966] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:22,967] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-25 12:02:22,970] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:22,974] INFO [Broker id=4] Leader __consumer_offsets-14 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:22,995] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,001] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,006] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,007] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-25 12:02:23,008] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,009] INFO [Broker id=4] Leader __consumer_offsets-43 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,027] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,044] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,046] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,046] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-25 12:02:23,047] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,047] INFO [Broker id=4] Leader __consumer_offsets-9 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,076] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,087] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,089] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,090] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-25 12:02:23,091] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,091] INFO [Broker id=4] Leader __consumer_offsets-24 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,116] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,125] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,136] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,137] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-25 12:02:23,143] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,145] INFO [Broker id=4] Leader __consumer_offsets-21 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,156] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,163] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,165] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,165] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-25 12:02:23,166] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,167] INFO [Broker id=4] Leader __consumer_offsets-19 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,177] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,189] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,193] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,195] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-25 12:02:23,197] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,198] INFO [Broker id=4] Leader __consumer_offsets-17 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,206] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,217] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,218] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,219] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-25 12:02:23,219] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,220] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,234] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,244] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,245] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,246] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-25 12:02:23,246] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,248] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,256] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,265] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,269] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,271] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-25 12:02:23,272] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,272] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,296] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,302] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,304] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,305] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-25 12:02:23,305] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,305] INFO [Broker id=4] Leader __consumer_offsets-8 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,313] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,317] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,319] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,321] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-25 12:02:23,322] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,323] INFO [Broker id=4] Leader __consumer_offsets-35 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,331] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,338] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,339] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,339] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-25 12:02:23,339] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,339] INFO [Broker id=4] Leader __consumer_offsets-4 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,344] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,350] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,351] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,351] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-25 12:02:23,352] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,352] INFO [Broker id=4] Leader __consumer_offsets-36 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,358] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,367] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,369] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,369] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-25 12:02:23,370] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,371] INFO [Broker id=4] Leader __consumer_offsets-2 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,376] INFO [Broker id=4] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:02:23,377] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,386] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,388] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,389] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-25 12:02:23,390] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,391] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,392] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,397] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,399] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,401] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-25 12:02:23,402] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,403] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,403] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,413] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,415] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,416] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-25 12:02:23,417] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,421] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,423] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,434] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,436] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,438] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-25 12:02:23,442] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,445] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,445] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,473] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,475] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,476] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-25 12:02:23,476] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,477] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,477] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,489] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,491] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,492] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-25 12:02:23,492] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,493] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,494] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,502] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,504] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,507] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-25 12:02:23,507] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,508] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,508] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,514] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,517] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,518] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-25 12:02:23,518] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,519] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,519] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,525] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,527] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,530] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-25 12:02:23,532] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,532] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,533] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,536] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,541] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,542] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-25 12:02:23,542] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,543] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,544] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,562] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,564] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,565] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-25 12:02:23,566] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,567] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,567] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,580] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,581] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,582] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-25 12:02:23,592] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,598] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,601] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,622] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,625] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,625] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-25 12:02:23,626] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,626] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,627] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,632] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,634] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,637] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-25 12:02:23,637] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,638] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,638] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,641] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,643] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,644] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-25 12:02:23,644] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,645] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,645] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,648] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,649] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,649] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-25 12:02:23,650] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,650] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,650] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,655] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,657] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,659] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-25 12:02:23,662] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,664] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,668] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,674] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,675] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,676] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-25 12:02:23,676] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,677] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,677] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,679] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,680] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,681] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-25 12:02:23,682] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,683] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,683] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,689] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,691] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,691] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-25 12:02:23,692] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,692] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,693] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,698] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,699] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,700] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-25 12:02:23,700] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,701] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,701] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,704] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,705] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,705] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-25 12:02:23,706] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,707] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,707] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,710] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,711] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,712] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-25 12:02:23,712] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,712] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,713] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,716] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,717] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,717] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-25 12:02:23,718] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,718] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,719] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,722] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,722] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,723] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-25 12:02:23,723] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,723] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,724] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,726] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,729] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,729] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-25 12:02:23,730] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,730] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,730] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,735] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,736] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,736] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-25 12:02:23,737] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,737] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,737] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,742] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,743] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,744] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-25 12:02:23,745] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,745] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,746] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,749] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,750] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,750] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,751] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,751] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,751] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,761] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,762] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,763] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-25 12:02:23,764] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,765] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,765] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,769] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,771] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,772] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-25 12:02:23,772] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,773] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,773] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,779] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,781] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,782] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-25 12:02:23,782] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,783] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,784] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,789] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,791] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,792] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-25 12:02:23,792] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,793] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,793] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,801] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,803] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,803] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-25 12:02:23,804] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,804] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,807] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-37, __consumer_offsets-6, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,808] INFO [Broker id=4] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-25 12:02:23,816] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,818] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,817] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-48 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,819] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,820] INFO [Broker id=4] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-25 12:02:23,818] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,821] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,822] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,822] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,823] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,824] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,824] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,824] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,825] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,825] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,826] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,827] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,827] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,827] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,828] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,828] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,829] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,829] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,829] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,830] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,830] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,830] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,830] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,830] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,831] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,831] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,832] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,832] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,832] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,833] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,833] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,833] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,834] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,842] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,846] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,856] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,860] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,860] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,861] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,862] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,863] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,863] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,864] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,864] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,866] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,869] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,869] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,871] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,874] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,871] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-45 in 21 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,881] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,887] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,888] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,889] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,890] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,890] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,884] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-14 in 23 milliseconds for epoch 0, of which 22 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,891] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-43 in 29 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,890] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,891] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,891] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-9 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,891] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,892] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,893] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-24 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,893] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,893] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,893] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-21 in 24 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,895] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-19 in 23 milliseconds for epoch 0, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,894] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,896] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,898] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,897] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-17 in 16 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,902] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 14 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,900] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,905] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 15 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,907] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,906] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,908] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-8 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,908] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-35 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,909] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,909] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,909] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-4 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-36 in 13 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,912] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,912] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,912] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,914] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,917] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,917] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,917] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,917] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,918] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,918] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,919] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,919] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,919] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,919] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,920] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,920] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,922] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,922] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,922] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,923] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,924] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,924] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,924] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,925] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,925] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,925] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,925] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,927] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,931] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,931] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,932] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,933] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,932] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,933] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,934] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,933] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,934] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,935] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,935] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,935] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,935] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,936] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,936] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,937] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,938] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,939] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,938] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,940] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,939] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,941] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,942] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,942] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,942] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,944] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,943] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,945] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,946] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,947] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,947] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,948] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,949] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,948] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,949] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,949] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,954] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,955] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,955] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,955] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,955] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:02:23,957] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,957] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,957] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,958] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,958] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,959] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,960] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,961] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,962] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,963] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,963] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,964] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,964] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,964] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,965] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,965] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,966] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,967] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,969] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,969] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,969] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,970] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,970] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,971] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,971] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,973] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,973] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,974] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,976] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,977] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,978] INFO [GroupCoordinator 4]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-99b7363f-0240-4d67-9e8b-611ef15f38f4 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,982] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,982] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,030] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-99b7363f-0240-4d67-9e8b-611ef15f38f4 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:27,047] INFO [GroupCoordinator 4]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:27,172] INFO [GroupCoordinator 4]: Assignment received from leader sr-1-99b7363f-0240-4d67-9e8b-611ef15f38f4 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:12:04,561] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,253] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,266] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,320] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,355] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,358] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,410] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,444] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,445] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,497] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,518] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,519] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,570] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,584] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,585] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,636] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,646] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,648] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,700] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,708] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,713] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,720] INFO [RaftManager id=4] Completed transition to Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3911, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:46:36,011] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3911, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:46:36,069] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:36,130] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:36,132] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:36,183] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:55:34,778] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-99b7363f-0240-4d67-9e8b-611ef15f38f4 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:55:34,861] INFO [GroupCoordinator 4]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:55:34,903] INFO [GroupCoordinator 4]: Member MemberMetadata(memberId=sr-1-99b7363f-0240-4d67-9e8b-611ef15f38f4, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.16, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:34,081] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:57:34,621] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:57:34,709] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:57:34,714] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:40,404] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:57:41,185] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:57:41,206] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:42,280] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:42,366] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:42,825] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-25 12:57:42,910] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-25 12:57:42,959] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-25 12:57:43,033] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:44,091] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:44,110] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:44,116] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 1ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:44,200] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-25 12:57:44,617] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-25 12:57:44,661] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:44,689] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:45,092] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1721) from null (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:45,094] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-25 12:57:45,095] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-25 12:57:45,877] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,896] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-25 12:57:45,913] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:46,012] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,026] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:46,029] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:46,029] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:46,089] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:46,061] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@970821424 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:46,124] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,232] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,319] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:57:46,324] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:57:46,335] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,345] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,370] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-25 12:57:46,426] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1721) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:46,442] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,467] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,547] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,580] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-25 12:57:46,592] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,647] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 5 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,654] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,931] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:57:46,977] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-25 12:57:46,983] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:57:47,014] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-25 12:57:47,038] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,043] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,062] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,063] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,076] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,078] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,082] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,082] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,085] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,125] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,127] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,159] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-25 12:57:47,161] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,162] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,165] INFO [BrokerLifecycleManager id=4] Incarnation DW_glk-VSeaMdU8sCT0CPQ of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:47,181] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,216] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:57:47,217] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 10 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:47,218] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:57:47,220] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:57:47,220] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 10 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:47,225] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=10, epoch=2) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-25 12:57:47,235] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-25 12:57:47,239] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 12 (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:47,254] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-25 12:57:47,287] INFO Loaded 0 logs in 47ms (kafka.log.LogManager)
[2025-12-25 12:57:47,291] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-25 12:57:47,297] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-25 12:57:47,311] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-25 12:57:48,287] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-25 12:57:48,315] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-25 12:57:48,327] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-25 12:57:48,333] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:48,352] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:48,375] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:57:48,399] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:57:48,399] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-25 12:57:48,537] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 10 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:48,588] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,595] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:57:48,600] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:57:48,604] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:57:48,615] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:57:48,632] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,635] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:48,685] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:57:48,896] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,897] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:57:48,914] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:57:48,921] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:57:48,924] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-25 12:57:48,930] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:57:48,967] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:57:49,022] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:57:49,029] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:57:49,034] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:57:49,042] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:57:49,047] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-25 12:57:49,048] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:49,052] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:49,053] INFO Kafka startTimeMs: 1766667469047 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:49,062] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-25 12:57:55,397] INFO Sent auto-creation request for Set(_schemas) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-25 12:57:55,727] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:57:55,744] INFO [Broker id=4] Creating new partition _schemas-0 with topic id wkzSqzyiRv-1bUOz43_kpQ. (state.change.logger)
[2025-12-25 12:57:55,762] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:55,765] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-25 12:57:55,769] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-25 12:57:55,774] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:55,776] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:55,778] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:55,780] INFO [Broker id=4] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:57:55,853] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:55,856] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(wkzSqzyiRv-1bUOz43_kpQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:55,857] INFO [Broker id=4] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:57:55,859] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:55,860] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:55,874] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:57:57,228] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-25 12:57:57,235] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-10, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:57,237] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,265] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,268] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,268] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-25 12:57:57,270] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,273] INFO [Broker id=4] Leader __consumer_offsets-15 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,303] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,311] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,313] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,313] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-25 12:57:57,314] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,315] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,337] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,350] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,357] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,361] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-25 12:57:57,363] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,363] INFO [Broker id=4] Leader __consumer_offsets-45 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,370] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,378] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,381] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,382] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-25 12:57:57,383] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,385] INFO [Broker id=4] Leader __consumer_offsets-14 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,398] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,431] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,432] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,433] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-25 12:57:57,433] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,434] INFO [Broker id=4] Leader __consumer_offsets-10 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,448] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,476] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,477] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,480] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-25 12:57:57,480] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,481] INFO [Broker id=4] Leader __consumer_offsets-42 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,488] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,519] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,520] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,521] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-25 12:57:57,521] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,521] INFO [Broker id=4] Leader __consumer_offsets-23 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,560] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,579] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,593] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,596] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-25 12:57:57,601] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,604] INFO [Broker id=4] Leader __consumer_offsets-19 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,623] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,637] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,642] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,644] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-25 12:57:57,647] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,649] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,667] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,674] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,676] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,677] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-25 12:57:57,677] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,678] INFO [Broker id=4] Leader __consumer_offsets-28 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,684] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,693] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,698] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,699] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-25 12:57:57,699] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,701] INFO [Broker id=4] Leader __consumer_offsets-25 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,709] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,716] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,717] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,717] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-25 12:57:57,718] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,719] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,724] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,733] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,739] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,740] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-25 12:57:57,740] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,740] INFO [Broker id=4] Leader __consumer_offsets-6 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,752] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,767] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,774] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,775] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-25 12:57:57,775] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,776] INFO [Broker id=4] Leader __consumer_offsets-38 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,787] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,803] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,807] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,808] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-25 12:57:57,810] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,811] INFO [Broker id=4] Leader __consumer_offsets-3 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,820] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,832] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,838] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,855] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-25 12:57:57,860] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,861] INFO [Broker id=4] Leader __consumer_offsets-33 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,904] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,939] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,941] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,942] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-25 12:57:57,943] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,944] INFO [Broker id=4] Leader __consumer_offsets-2 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,977] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:57:57,989] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,024] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,052] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,061] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-25 12:57:58,067] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,068] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,070] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,107] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,113] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,113] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-25 12:57:58,114] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,114] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,115] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,143] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,146] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,147] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-25 12:57:58,150] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,150] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,150] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,171] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,188] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,195] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-25 12:57:58,195] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,197] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,197] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,221] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,234] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,236] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-25 12:57:58,236] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,238] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,240] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,276] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,278] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,280] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-25 12:57:58,280] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,282] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,283] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,288] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,296] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,296] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-25 12:57:58,297] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,297] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,298] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,307] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,310] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,311] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-25 12:57:58,311] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,311] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,312] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,316] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,317] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,319] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-25 12:57:58,320] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,320] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,321] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,346] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,348] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,352] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-25 12:57:58,354] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,354] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,358] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,364] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,365] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,365] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-25 12:57:58,366] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,366] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,367] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,370] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,372] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,373] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-25 12:57:58,374] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,374] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,375] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,395] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,396] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,397] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-25 12:57:58,397] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,398] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,398] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,406] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,407] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,407] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-25 12:57:58,408] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,408] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,409] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,425] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,431] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,432] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-25 12:57:58,433] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,433] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,434] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,443] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,446] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,447] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-25 12:57:58,447] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,447] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,447] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,458] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,461] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,462] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-25 12:57:58,462] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,463] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,465] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,472] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,476] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,476] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-25 12:57:58,477] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,477] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,478] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,495] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,504] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,524] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-25 12:57:58,553] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,576] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,580] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,622] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,623] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,624] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-25 12:57:58,624] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,624] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,625] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,632] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,633] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,634] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-25 12:57:58,634] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,634] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,634] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,638] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,639] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,640] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-25 12:57:58,640] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,640] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,640] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,648] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,650] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,650] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-25 12:57:58,651] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,651] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,651] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,656] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,657] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,658] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-25 12:57:58,658] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,658] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,659] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,664] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,665] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,665] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-25 12:57:58,666] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,667] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,667] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,680] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,682] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,685] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-25 12:57:58,685] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,685] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,686] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,694] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,695] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,697] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,697] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,698] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,700] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,704] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,729] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,730] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-25 12:57:58,730] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,730] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,731] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,741] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,746] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,747] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-25 12:57:58,747] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,748] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,749] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,757] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,758] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,758] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-25 12:57:58,759] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,762] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,762] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,774] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,775] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,778] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-25 12:57:58,779] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,780] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,780] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,788] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,789] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,791] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-25 12:57:58,791] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,791] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,792] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,805] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,812] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,818] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-25 12:57:58,822] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,823] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,831] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:58,841] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-25 12:57:58,850] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,851] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,852] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,851] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-13 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:58,853] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,855] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,855] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,855] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:58,857] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-25 12:57:58,856] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,857] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,858] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,858] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,858] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,859] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,859] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,859] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,859] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,859] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,860] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,861] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,861] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,861] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,861] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,861] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,861] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,862] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,862] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,862] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,862] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,862] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,862] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,863] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,864] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,867] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,868] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,879] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,885] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,892] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,901] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,902] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,905] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,906] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,907] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-15 in 14 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,914] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,916] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,916] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 15 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,917] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,921] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-45 in 14 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,922] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,925] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-14 in 9 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,927] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,928] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,929] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,928] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-10 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,929] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-42 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,929] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,936] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,934] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-23 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,942] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,936] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,944] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,948] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,950] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,950] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,955] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,965] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,963] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-28 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,974] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,976] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,976] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-25 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,977] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,979] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,979] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,983] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-6 in 3 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,980] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,991] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,993] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,994] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,994] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,997] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,998] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,003] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,006] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,008] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,006] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,014] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,017] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,017] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,022] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,022] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,024] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,024] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,025] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,024] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,026] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,026] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,027] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,027] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,030] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,027] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,032] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,032] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,033] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,034] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,037] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,037] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,037] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,040] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,039] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,044] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,046] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,044] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,047] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,048] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,047] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,049] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,049] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,049] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,050] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,050] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,053] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,053] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,053] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,053] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,053] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,054] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,050] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,054] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,057] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,058] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,062] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,054] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,064] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,064] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,064] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,064] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,064] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,064] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,064] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,065] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,064] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,065] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,065] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,065] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,066] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,067] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,065] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,070] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,069] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,075] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,079] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,087] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,088] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,087] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,088] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,088] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,092] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,093] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,093] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,093] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,094] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,094] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,102] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,103] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,105] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,107] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,103] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,112] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,109] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,116] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,119] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,120] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,117] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,121] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,120] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,123] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,123] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,125] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,125] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,126] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,125] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,130] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,128] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,133] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,134] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:57:59,134] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,328] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,332] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,333] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,336] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,338] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,338] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,339] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,340] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,342] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,346] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,347] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,347] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,348] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,350] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,351] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,351] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,351] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,351] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,354] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,355] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,355] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,355] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,356] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,356] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,356] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,356] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,360] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,362] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,364] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,367] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,368] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,368] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,369] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,369] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 13:11:09,758] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:11:09,798] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=345, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:11:09,833] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=345, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:11:09,841] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:11:09,917] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:11:09,924] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:11:09,975] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:17:49,923] INFO [RaftManager id=4] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=643, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:17:50,016] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=643, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:17:50,887] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:17:50,889] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:23:10,056] INFO [NodeToControllerChannelManager id=4 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:27:16,387] INFO [RaftManager id=4] Completed transition to Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,433] INFO [RaftManager id=4] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,493] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:27:16,500] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,596] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:29:20,364] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:29:20,397] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:29:20,471] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:29:20,534] INFO [RaftManager id=4] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1536, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:20,546] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:29:20,614] INFO [RaftManager id=4] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:20,976] INFO [RaftManager id=4] Completed transition to Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:21,111] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1536, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:21,159] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:29:21,192] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 1536 (kafka.log.UnifiedLog)
[2025-12-25 13:29:21,279] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 1536 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 13:29:21,282] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 1536 (kafka.log.UnifiedLog$)
[2025-12-25 13:29:21,296] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 1536 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 13:29:21,297] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 13ms for segment recovery from offset 1536 (kafka.log.UnifiedLog$)
[2025-12-25 13:29:21,297] INFO [RaftManager id=4] Truncated to offset 1536 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 13:39:21,282] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:39:21,299] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:49:23,684] INFO [RaftManager id=4] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,800] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=11, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,837] INFO [RaftManager id=4] Completed transition to Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=11, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,947] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,954] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 3680 (kafka.log.UnifiedLog)
[2025-12-25 13:49:24,028] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3680 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 13:49:24,028] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3680 (kafka.log.UnifiedLog$)
[2025-12-25 13:49:24,029] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=1536, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000001536.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 13:49:24,066] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:49:24,069] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:49:24,066] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3680 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 13:49:24,072] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 13ms for snapshot load and 30ms for segment recovery from offset 3680 (kafka.log.UnifiedLog$)
[2025-12-25 13:49:24,072] INFO [RaftManager id=4] Truncated to offset 3680 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 13:59:23,992] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:59:24,012] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:19:04,180] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000007224-0000000012 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 14:19:04,629] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000007224-0000000012 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 14:31:39,634] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:39,674] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 9264 due to node 3 being disconnected (elapsed time since creation: 2021ms, elapsed time since send: 2004ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,305] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,329] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,366] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,466] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,471] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,507] INFO [RaftManager id=4] Completed transition to Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=8719, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:40,755] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 14:31:40,774] INFO [RaftManager id=4] Completed transition to Unattached(epoch=14, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:41,104] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=14, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=8719, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=14, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:41,157] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 8720 (kafka.log.UnifiedLog)
[2025-12-25 14:31:41,167] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:41,197] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 8720 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,199] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 8720 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,204] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3680, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000003680.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 14:31:41,242] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 8720 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 14:31:41,243] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 32ms for segment recovery from offset 8720 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,245] INFO [RaftManager id=4] Truncated to offset 8720 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 14:41:40,836] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
