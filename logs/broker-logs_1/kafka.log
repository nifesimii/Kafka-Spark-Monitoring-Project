[2025-12-24 09:10:28,316] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 09:10:28,786] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 09:10:28,911] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 09:10:28,924] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,370] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 09:10:32,495] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 09:10:32,502] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,693] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,699] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,714] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 09:10:32,715] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 09:10:32,717] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 09:10:32,720] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:32,939] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:32,949] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:32,953] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:33,013] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 09:10:33,093] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 09:10:33,096] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,103] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,181] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1320) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:33,191] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 09:10:33,195] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 09:10:33,233] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,241] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-24 09:10:33,273] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:33,323] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1109634719 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 09:10:33,334] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,340] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,341] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,348] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,342] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,362] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,341] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 09:10:33,466] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,544] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 09:10:33,545] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 09:10:33,583] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,589] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:33,590] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,599] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,603] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 09:10:33,688] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,740] INFO [RaftManager id=4] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1320) (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:33,762] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,764] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,774] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,774] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:10:33,789] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,891] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:33,993] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,099] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,200] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,311] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,361] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 09:10:34,413] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,416] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 09:10:34,416] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 09:10:34,421] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 09:10:34,433] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,456] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,501] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,506] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,509] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,576] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,577] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,576] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 09:10:34,577] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,577] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,577] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,597] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,599] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,611] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 09:10:34,614] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,614] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,627] INFO [BrokerLifecycleManager id=4] Incarnation _7wCxPPbSfO81U2Voxbubw of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,641] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 09:10:34,668] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 09:10:34,669] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 09:10:34,669] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,673] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 1 (org.apache.kafka.raft.FollowerState)
[2025-12-24 09:10:34,672] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 09:10:34,676] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 09:10:34,683] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,712] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 7 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,721] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,722] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,723] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:34,724] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=6, epoch=1) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 09:10:34,727] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 09:10:34,735] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 09:10:34,740] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 7 (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:34,752] INFO Loaded 0 logs in 24ms (kafka.log.LogManager)
[2025-12-24 09:10:34,755] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 09:10:34,760] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 09:10:34,785] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 09:10:34,999] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 09:10:35,011] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 09:10:35,011] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 09:10:35,012] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:35,016] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:35,017] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 09:10:35,020] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 09:10:35,020] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 09:10:35,029] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 09:10:35,092] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:35,094] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 09:10:35,094] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 09:10:35,094] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 09:10:35,096] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 09:10:35,099] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 09:10:35,106] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 09:10:35,145] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:35,187] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 09:10:35,188] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 09:10:35,190] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 09:10:35,190] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 09:10:35,191] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 09:10:35,192] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 09:10:35,194] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 09:10:35,198] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 09:10:35,199] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 09:10:35,199] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 09:10:35,200] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 09:10:35,200] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 09:10:35,200] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,201] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,202] INFO Kafka startTimeMs: 1766567435200 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 09:10:35,203] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 09:10:37,818] INFO Sent auto-creation request for Set(_schemas) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 09:10:38,169] INFO [Broker id=4] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-12-24 09:10:38,175] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:38,177] INFO [Broker id=4] Creating new partition _schemas-0 with topic id Im1NE7r1TMiyWVwhbgXZ_g. (state.change.logger)
[2025-12-24 09:10:38,202] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:38,205] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 09:10:38,207] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 09:10:38,209] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:38,210] INFO [Broker id=4] Leader _schemas-0 with topic id Some(Im1NE7r1TMiyWVwhbgXZ_g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:38,223] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 09:10:39,165] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-24 09:10:39,166] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-48, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-0, __consumer_offsets-32, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-5, __consumer_offsets-6, __consumer_offsets-36, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,168] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,172] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,174] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,177] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 09:10:39,177] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,178] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,185] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,197] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,205] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,206] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 09:10:39,207] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,209] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,221] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,231] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,240] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,242] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 09:10:39,243] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,244] INFO [Broker id=4] Leader __consumer_offsets-11 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,266] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,279] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,284] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,284] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 09:10:39,285] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,285] INFO [Broker id=4] Leader __consumer_offsets-12 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,292] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,298] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,299] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,299] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 09:10:39,299] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,300] INFO [Broker id=4] Leader __consumer_offsets-41 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,308] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,311] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,312] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,312] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 09:10:39,313] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,313] INFO [Broker id=4] Leader __consumer_offsets-42 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,318] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,321] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,322] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,322] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 09:10:39,322] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,323] INFO [Broker id=4] Leader __consumer_offsets-21 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,327] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,330] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,331] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,331] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 09:10:39,331] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,332] INFO [Broker id=4] Leader __consumer_offsets-19 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,337] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,341] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,342] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,344] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 09:10:39,345] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,345] INFO [Broker id=4] Leader __consumer_offsets-17 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,352] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,355] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,356] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,357] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,357] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,357] INFO [Broker id=4] Leader __consumer_offsets-0 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,361] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,370] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,371] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,373] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 09:10:39,375] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,375] INFO [Broker id=4] Leader __consumer_offsets-32 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,380] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,384] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,385] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,386] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 09:10:39,387] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,388] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,394] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,397] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,400] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,400] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 09:10:39,401] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,402] INFO [Broker id=4] Leader __consumer_offsets-25 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,406] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,409] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,411] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,412] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 09:10:39,413] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,414] INFO [Broker id=4] Leader __consumer_offsets-5 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,419] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,421] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,423] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,423] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 09:10:39,424] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,424] INFO [Broker id=4] Leader __consumer_offsets-6 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,433] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,440] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,441] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,441] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 09:10:39,441] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,441] INFO [Broker id=4] Leader __consumer_offsets-36 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,448] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,452] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,452] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,453] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 09:10:39,453] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,453] INFO [Broker id=4] Leader __consumer_offsets-34 with topic id Some(8sDVKByjR02JwFSE6vX7PA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 09:10:39,457] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-24 09:10:39,457] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,462] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,465] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,467] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 09:10:39,467] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,469] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,470] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,474] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,475] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,475] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 09:10:39,475] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,475] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,476] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,481] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,482] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,484] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 09:10:39,484] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,485] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,485] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,490] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,491] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,491] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 09:10:39,491] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,491] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,492] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,494] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,495] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,496] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 09:10:39,496] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,496] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,496] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,499] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,499] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,500] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 09:10:39,500] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,501] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,501] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,505] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,507] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,508] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 09:10:39,508] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,508] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,509] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,513] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,513] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,514] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 09:10:39,515] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,516] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,516] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,521] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,522] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,523] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 09:10:39,524] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,525] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,526] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,529] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,530] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,531] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 09:10:39,531] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,532] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,532] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,535] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,536] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,536] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 09:10:39,537] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,538] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,538] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,540] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,541] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,542] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 09:10:39,542] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,542] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,543] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,546] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,547] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,547] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 09:10:39,547] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,547] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,548] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,552] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,554] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,554] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 09:10:39,555] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,555] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,556] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,559] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,562] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,563] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 09:10:39,563] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,564] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,564] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,569] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,569] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,571] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 09:10:39,571] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,571] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,572] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,581] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,583] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,584] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 09:10:39,584] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,585] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,585] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,591] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,592] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,593] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 09:10:39,593] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,594] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,594] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,597] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,597] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,598] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 09:10:39,598] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,598] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,599] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,601] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,602] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,602] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 09:10:39,603] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,603] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,603] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,607] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,609] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,609] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 09:10:39,610] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,610] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,610] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,616] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,617] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,617] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 09:10:39,618] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,618] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,618] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,621] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,622] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,622] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 09:10:39,622] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,622] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,623] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,626] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,626] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,627] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 09:10:39,627] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,627] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,628] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,634] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,639] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,640] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 09:10:39,641] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,645] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,647] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,653] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,654] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,654] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 09:10:39,654] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,654] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,655] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,659] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,660] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,660] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 09:10:39,661] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,661] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,661] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,665] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,668] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,668] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 09:10:39,669] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,669] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,670] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,673] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,675] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,675] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 09:10:39,676] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,676] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,676] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,682] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,682] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,683] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 09:10:39,683] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,683] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,684] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,687] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,690] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,690] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 09:10:39,690] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,691] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,691] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,694] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,696] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,696] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 09:10:39,697] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,697] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,697] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id 8sDVKByjR02JwFSE6vX7PA. (state.change.logger)
[2025-12-24 09:10:39,702] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 09:10:39,704] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 09:10:39,704] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 09:10:39,704] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 09:10:39,705] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 09:10:39,706] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-13, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-1, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,706] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 09:10:39,724] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,728] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,728] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,729] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,732] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,734] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,734] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,734] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,735] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,734] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,735] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,735] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(8sDVKByjR02JwFSE6vX7PA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 09:10:39,735] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,736] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 09:10:39,735] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,736] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,736] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,737] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,736] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,737] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,737] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,737] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,737] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,738] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,738] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,738] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,739] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,738] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,739] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,739] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,739] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,740] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,740] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,739] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,740] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,740] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,740] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,741] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,741] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,741] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,741] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,742] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,742] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,742] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,743] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,743] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,743] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,743] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,743] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,744] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,743] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,744] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,744] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,744] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,745] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,744] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,745] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,745] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,745] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,746] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,746] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,746] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,746] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,746] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,746] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,747] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,747] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,747] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,748] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,748] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 09:10:39,748] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 09:10:39,755] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,756] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,757] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,757] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,757] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,758] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,758] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,758] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,759] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,759] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,760] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,760] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,760] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,760] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,760] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,760] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,761] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,761] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,762] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,762] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,762] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,762] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,762] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,762] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,763] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,763] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,762] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-11 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,763] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,763] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,763] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,763] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,763] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,764] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,764] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,763] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-41 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,764] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,764] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,764] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,764] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,765] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,765] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-21 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,765] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,765] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,765] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,765] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,765] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,766] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,766] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,766] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,766] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,766] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,765] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,766] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,766] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,767] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,766] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-17 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,767] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-0 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,767] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,767] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-32 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-25 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-5 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,768] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,769] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,769] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-6 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,769] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,769] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-36 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,770] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,770] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,770] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-34 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,770] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,770] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,770] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,770] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,770] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,771] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,771] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,771] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,771] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,771] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,771] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,771] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,772] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 09:10:39,772] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 09:10:39,899] INFO [GroupCoordinator 4]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-d97cbd36-3521-4579-b6fd-05fee9ad2175 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:39,903] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-d97cbd36-3521-4579-b6fd-05fee9ad2175 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:42,911] INFO [GroupCoordinator 4]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:10:42,934] INFO [GroupCoordinator 4]: Assignment received from leader sr-1-d97cbd36-3521-4579-b6fd-05fee9ad2175 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 09:20:34,330] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 09:20:37,843] INFO [NodeToControllerChannelManager id=4 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:07:11,891] INFO [RaftManager id=4] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6779, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:07:11,966] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6779, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:07:12,205] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:07:12,206] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:15:24,164] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6898, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:15:24,183] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6898, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:15:24,671] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:15:24,672] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:18:15,406] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000007240-0000000003 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 10:18:15,536] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000007240-0000000003 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 10:24:59,108] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-d97cbd36-3521-4579-b6fd-05fee9ad2175 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:24:59,112] INFO [GroupCoordinator 4]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:24:59,123] INFO [GroupCoordinator 4]: Member MemberMetadata(memberId=sr-1-d97cbd36-3521-4579-b6fd-05fee9ad2175, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.14, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:26:37,722] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 10:26:38,488] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 10:26:38,552] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 10:26:38,558] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:42,422] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 10:26:42,745] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 10:26:42,779] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:43,143] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:43,167] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:43,264] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 10:26:43,302] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 10:26:43,306] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 10:26:43,310] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:43,930] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:43,962] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:43,994] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:44,381] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 10:26:44,525] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 10:26:44,576] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:44,582] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:44,789] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1702) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:44,801] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 10:26:44,807] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 10:26:44,833] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:44,840] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-24 10:26:44,871] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:44,944] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,056] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,068] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,107] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,131] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,137] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 10:26:45,150] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1583505174 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 10:26:45,165] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,223] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,240] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,272] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,374] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,404] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 10:26:45,411] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 10:26:45,418] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,423] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,451] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,457] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,461] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:45,466] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,467] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,477] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,880] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,478] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:45,895] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,898] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,912] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,940] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:45,918] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 10:26:45,996] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,035] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,048] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,071] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,091] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,100] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,114] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,126] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,213] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,323] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,429] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,694] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,712] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,736] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,812] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,940] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:46,866] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:46,991] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,003] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,064] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,072] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,128] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,185] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,287] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,301] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,295] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,447] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,504] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:47,577] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,712] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:47,830] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,017] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,022] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,043] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,075] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,089] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,139] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,240] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,343] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,593] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,629] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:48,628] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 10:26:48,602] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,819] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,928] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 10:26:48,932] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:48,944] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 10:26:49,047] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 10:26:49,079] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,183] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,224] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:49,295] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,342] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:49,404] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,409] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,431] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,450] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,456] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,461] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,510] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,511] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,512] INFO [RaftManager id=4] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1702) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:49,518] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:49,682] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,904] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:49,963] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 10:26:49,981] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:49,989] INFO [BrokerLifecycleManager id=4] Incarnation Hc3RSEWMR_meyc8bhUfATw of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:50,008] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:50,017] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:50,036] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 10:26:50,053] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,156] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 10:26:50,159] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,161] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,161] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 10:26:50,164] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 10:26:50,262] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,365] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,467] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,566] INFO [RaftManager id=4] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:50,569] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,677] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,784] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,829] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 10:26:50,845] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,885] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,886] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:50,909] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:50,917] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,388] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,518] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,664] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,868] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:51,885] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,888] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,928] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:51,983] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:26:51,987] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:51,987] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:52,040] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 10:26:52,106] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,214] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,427] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-24 10:26:52,436] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,572] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,625] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,681] INFO [MetadataLoader id=4] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,685] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,696] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,704] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:52,711] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=2) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 10:26:52,749] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 10:26:52,886] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 10:26:53,046] INFO Loaded 0 logs in 257ms (kafka.log.LogManager)
[2025-12-24 10:26:53,054] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 10:26:53,052] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:53,067] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 10:26:53,109] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 10:26:53,757] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 10:26:53,782] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 10:26:53,783] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 10:26:53,786] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:26:53,789] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:26:53,790] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 10:26:53,796] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 10:26:53,797] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 10:26:53,860] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 10:26:53,956] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:53,957] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 10:26:53,957] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 10:26:53,958] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 10:26:53,976] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 10:26:53,987] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 10:26:53,981] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:54,004] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 10:26:54,083] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 10:26:54,089] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 10:26:54,113] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 10:26:54,117] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 10:26:54,127] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 10:26:54,154] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 10:26:54,192] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 10:26:54,230] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 10:26:54,232] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 10:26:54,234] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 10:26:54,235] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 10:26:54,235] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 10:26:54,245] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,249] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,251] INFO Kafka startTimeMs: 1766572014237 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 10:26:54,256] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 10:26:59,281] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-24 10:26:59,285] INFO [Broker id=4] Creating new partition _schemas-0 with topic id 4gVnUI_IQ3yRWz4RgXc--Q. (state.change.logger)
[2025-12-24 10:26:59,343] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:26:59,348] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 10:26:59,354] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 10:26:59,358] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:26:59,373] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:26:59,375] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:26:59,377] INFO [Broker id=4] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 10:26:59,407] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:26:59,411] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(4gVnUI_IQ3yRWz4RgXc--Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:26:59,413] INFO [Broker id=4] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 10:26:59,411] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:26:59,421] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:26:59,440] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 10:27:00,789] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-24 10:27:00,792] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-42, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-35, __consumer_offsets-36) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:00,792] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,803] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,806] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,807] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 10:27:00,810] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,813] INFO [Broker id=4] Leader __consumer_offsets-15 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,824] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,829] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,831] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,832] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 10:27:00,832] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,833] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,839] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,843] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,844] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,844] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 10:27:00,845] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,846] INFO [Broker id=4] Leader __consumer_offsets-14 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,851] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,857] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,860] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,861] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 10:27:00,861] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,862] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,867] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,873] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,878] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,879] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 10:27:00,881] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,884] INFO [Broker id=4] Leader __consumer_offsets-11 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,890] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,895] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,896] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,896] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 10:27:00,896] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,897] INFO [Broker id=4] Leader __consumer_offsets-42 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,900] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,915] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,919] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,920] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 10:27:00,920] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,921] INFO [Broker id=4] Leader __consumer_offsets-24 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,926] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,929] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,930] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,931] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 10:27:00,932] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,932] INFO [Broker id=4] Leader __consumer_offsets-21 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,936] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,940] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,943] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,944] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 10:27:00,946] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,946] INFO [Broker id=4] Leader __consumer_offsets-18 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,952] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,955] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,956] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,957] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,957] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,958] INFO [Broker id=4] Leader __consumer_offsets-0 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,965] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,968] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,969] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,970] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 10:27:00,971] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,971] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:00,980] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:00,988] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:00,995] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:00,996] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 10:27:00,997] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:00,999] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,006] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,009] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,010] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,011] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 10:27:01,011] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,012] INFO [Broker id=4] Leader __consumer_offsets-8 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,017] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,022] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,024] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,024] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 10:27:01,024] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,026] INFO [Broker id=4] Leader __consumer_offsets-40 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,033] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,035] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,036] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,036] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 10:27:01,036] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,037] INFO [Broker id=4] Leader __consumer_offsets-3 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,039] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,042] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,043] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,043] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 10:27:01,044] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,044] INFO [Broker id=4] Leader __consumer_offsets-35 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,048] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,052] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,052] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,052] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 10:27:01,053] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,053] INFO [Broker id=4] Leader __consumer_offsets-36 with topic id Some(Q37Nxa5RRRS2e_005gQJGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 10:27:01,057] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-24 10:27:01,057] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,060] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,061] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,061] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 10:27:01,061] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,061] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,061] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,064] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,065] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,065] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 10:27:01,066] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,066] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,067] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,078] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,079] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,079] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 10:27:01,080] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,080] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,081] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,085] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,087] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,088] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 10:27:01,089] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,089] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,090] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,094] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,095] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,095] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 10:27:01,095] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,096] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,096] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,100] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,101] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,101] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 10:27:01,102] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,102] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,103] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,110] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,111] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,112] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 10:27:01,112] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,112] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,112] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,120] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,123] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,124] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 10:27:01,124] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,125] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,125] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,133] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,135] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,135] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 10:27:01,136] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,136] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,136] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,139] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,142] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,142] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 10:27:01,142] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,143] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,143] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,146] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,148] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,149] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 10:27:01,149] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,150] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,150] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,154] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,155] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,155] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 10:27:01,156] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,156] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,156] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,161] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,162] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,162] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 10:27:01,162] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,163] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,163] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,166] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,168] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,168] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 10:27:01,169] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,169] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,169] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,173] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,177] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,178] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 10:27:01,178] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,178] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,179] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,183] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,184] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,184] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 10:27:01,185] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,185] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,186] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,188] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,189] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,189] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 10:27:01,189] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,190] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,190] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,196] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,197] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,198] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 10:27:01,199] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,199] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,200] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,204] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,205] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,205] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 10:27:01,206] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,206] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,206] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,210] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,210] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,211] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 10:27:01,211] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,211] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,212] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,217] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,218] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,218] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 10:27:01,218] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,218] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,219] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,223] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,226] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,227] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 10:27:01,227] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,228] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,230] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,233] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,235] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,235] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 10:27:01,235] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,236] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,237] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,242] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,243] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,243] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 10:27:01,243] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,244] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,244] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,250] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,251] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,251] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 10:27:01,252] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,252] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,252] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,256] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,258] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,258] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 10:27:01,259] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,259] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,259] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,263] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,265] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,265] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 10:27:01,266] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,266] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,267] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,271] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,278] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,280] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 10:27:01,281] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,283] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,284] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,291] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,295] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,295] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 10:27:01,295] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,296] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,297] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,304] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,305] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,306] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 10:27:01,306] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,306] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,306] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,322] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,326] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,338] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 10:27:01,340] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,345] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,350] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,371] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,373] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,375] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 10:27:01,375] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,389] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,391] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id Q37Nxa5RRRS2e_005gQJGQ. (state.change.logger)
[2025-12-24 10:27:01,406] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 10:27:01,409] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 10:27:01,413] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 10:27:01,413] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 10:27:01,414] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 10:27:01,415] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-37, __consumer_offsets-6, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,415] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 10:27:01,417] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-16 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,436] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(Q37Nxa5RRRS2e_005gQJGQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 10:27:01,437] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,437] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 10:27:01,453] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,454] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,454] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,455] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,455] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,455] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,458] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,457] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-15 in 3 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,457] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,459] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,458] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,459] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,459] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,460] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,460] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,460] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,461] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,460] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,461] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,461] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,461] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,461] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,462] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,462] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,460] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,462] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,462] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,461] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,463] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,462] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,465] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,462] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,466] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,465] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,467] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,468] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,466] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,469] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,469] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,470] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,470] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,471] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,471] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,471] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,471] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,471] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,471] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,471] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,472] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,472] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,473] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,473] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,475] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,468] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,478] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,477] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,479] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,470] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-42 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,479] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-24 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,479] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,480] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,479] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,482] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,481] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,483] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,480] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-21 in 18 milliseconds for epoch 0, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,483] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-18 in 18 milliseconds for epoch 0, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,483] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,487] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,487] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,488] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,489] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,490] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,487] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-0 in 19 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,494] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,496] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,496] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 18 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,497] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,499] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,499] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 16 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,501] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-8 in 14 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,501] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,507] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-40 in 18 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,509] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-3 in 15 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,508] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,511] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-35 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,514] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,515] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,515] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-36 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,516] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,518] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,517] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,519] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,519] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,520] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,520] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,522] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,522] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,522] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,523] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,523] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,524] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,524] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,524] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,524] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,524] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,526] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,525] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,526] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,526] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,526] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,527] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,527] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,528] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,528] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,528] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,529] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,530] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,530] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,530] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,531] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,532] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,532] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,532] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,533] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,533] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,532] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,533] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,534] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,533] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,534] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,534] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,534] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,538] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,538] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,539] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,540] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,541] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,542] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,542] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,542] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,542] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,543] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,542] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,543] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,543] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,544] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,544] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,545] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,545] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,546] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,546] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,547] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 10:27:01,547] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 10:27:01,661] INFO [GroupCoordinator 4]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-9e0d337e-78aa-4c00-8b29-54e9167b0117 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,667] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-9e0d337e-78aa-4c00-8b29-54e9167b0117 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:01,902] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,903] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,903] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,903] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,904] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,904] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,904] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,904] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,904] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,904] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,905] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,905] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,905] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,905] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,905] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,905] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,906] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,906] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,906] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,906] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,906] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,906] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,907] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,907] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,907] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,907] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,907] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,907] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,908] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,908] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:01,908] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 10:27:01,908] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 10:27:04,672] INFO [GroupCoordinator 4]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:27:04,704] INFO [GroupCoordinator 4]: Assignment received from leader sr-1-9e0d337e-78aa-4c00-8b29-54e9167b0117 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 10:36:50,474] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 10:36:50,902] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:04:30,902] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:04:30,943] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:04:30,956] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:04:30,957] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:14:30,942] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:14:30,947] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:31:29,361] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000007243-0000000003 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 11:31:29,516] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000007243-0000000003 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 11:32:28,041] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-9e0d337e-78aa-4c00-8b29-54e9167b0117 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:32:28,044] INFO [GroupCoordinator 4]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:32:28,056] INFO [GroupCoordinator 4]: Member MemberMetadata(memberId=sr-1-9e0d337e-78aa-4c00-8b29-54e9167b0117, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.14, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:27,309] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 11:34:27,770] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 11:34:27,783] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 11:34:27,793] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,186] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 11:34:32,450] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 11:34:32,460] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,673] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,687] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,726] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 11:34:32,736] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 11:34:32,744] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 11:34:32,755] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:32,876] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,877] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,878] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:32,905] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 11:34:32,953] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 11:34:32,964] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:32,974] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:33,060] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1969) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:33,061] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 11:34:33,061] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 11:34:33,082] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,085] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-24 11:34:33,112] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:33,158] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1103525925 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 11:34:33,185] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,188] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:33,191] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:33,209] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:33,211] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:33,210] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 11:34:33,213] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:33,245] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 11:34:33,252] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 11:34:33,276] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:33,295] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,303] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 11:34:33,402] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,504] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,612] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,687] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 11:34:33,723] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,746] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 11:34:33,747] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 11:34:33,769] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 11:34:33,808] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:33,848] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:33,886] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:33,893] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:33,899] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:33,901] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:33,914] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:33,915] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,083] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,083] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,090] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,122] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 11:34:34,127] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,130] INFO [RaftManager id=4] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1969) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:34,132] INFO [BrokerLifecycleManager id=4] Incarnation 566eob1CRuyU1_jVO7b8lw of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,187] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,199] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 11:34:34,200] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 11:34:34,234] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,279] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,281] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,285] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,286] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 11:34:34,287] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,291] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,294] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:34:34,308] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,292] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,343] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 1 (org.apache.kafka.raft.FollowerState)
[2025-12-24 11:34:34,346] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,360] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 11:34:34,384] INFO [MetadataLoader id=4] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,391] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,395] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,402] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,410] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=1) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 11:34:34,422] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 11:34:34,436] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 11:34:34,437] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,455] INFO Loaded 0 logs in 28ms (kafka.log.LogManager)
[2025-12-24 11:34:34,459] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 11:34:34,462] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 11:34:34,478] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 11:34:34,675] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 11:34:34,683] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 11:34:34,684] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 11:34:34,685] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:34,687] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:34,689] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 11:34:34,694] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 11:34:34,695] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 11:34:34,722] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 11:34:34,763] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,767] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 11:34:34,768] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 11:34:34,768] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 11:34:34,770] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 11:34:34,775] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 11:34:34,784] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,791] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 11:34:34,854] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 11:34:34,864] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 11:34:34,867] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 11:34:34,868] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 11:34:34,870] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 11:34:34,874] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 11:34:34,879] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 11:34:34,889] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,889] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 11:34:34,889] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 11:34:34,890] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 11:34:34,890] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 11:34:34,891] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:34,893] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:34,893] INFO Kafka startTimeMs: 1766576074891 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 11:34:34,895] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 11:34:38,978] INFO Sent auto-creation request for Set(_schemas) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 11:34:39,365] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-24 11:34:39,369] INFO [Broker id=4] Creating new partition _schemas-0 with topic id 0IctBlPsRtCyKBZU8rL83Q. (state.change.logger)
[2025-12-24 11:34:39,385] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:39,389] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 11:34:39,394] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 11:34:39,399] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:39,401] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:39,405] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:39,408] INFO [Broker id=4] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 11:34:39,426] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:39,429] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(0IctBlPsRtCyKBZU8rL83Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:39,431] INFO [Broker id=4] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 11:34:39,433] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:39,435] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:39,450] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 11:34:40,346] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-24 11:34:40,353] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-46, __consumer_offsets-43, __consumer_offsets-9, __consumer_offsets-23, __consumer_offsets-17, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-38, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:40,356] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,370] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,375] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,376] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 11:34:40,377] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,379] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,390] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,393] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,395] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,397] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 11:34:40,398] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,398] INFO [Broker id=4] Leader __consumer_offsets-14 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,406] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,413] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,415] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,415] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 11:34:40,416] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,417] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,424] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,428] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,430] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,434] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 11:34:40,434] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,434] INFO [Broker id=4] Leader __consumer_offsets-43 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,439] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,446] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,447] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,448] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 11:34:40,448] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,448] INFO [Broker id=4] Leader __consumer_offsets-9 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,456] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,463] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,471] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,473] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 11:34:40,476] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,478] INFO [Broker id=4] Leader __consumer_offsets-23 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,488] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,498] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,499] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,500] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 11:34:40,501] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,502] INFO [Broker id=4] Leader __consumer_offsets-17 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,508] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,512] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,516] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,517] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 11:34:40,517] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,518] INFO [Broker id=4] Leader __consumer_offsets-18 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,532] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,534] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,540] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,541] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 11:34:40,542] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,544] INFO [Broker id=4] Leader __consumer_offsets-31 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,549] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,559] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,560] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,560] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 11:34:40,561] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,561] INFO [Broker id=4] Leader __consumer_offsets-28 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,575] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,584] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,585] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,586] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 11:34:40,586] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,587] INFO [Broker id=4] Leader __consumer_offsets-25 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,596] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,603] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,606] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,608] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 11:34:40,608] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,609] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,622] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,628] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,628] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,629] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 11:34:40,630] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,631] INFO [Broker id=4] Leader __consumer_offsets-6 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,638] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,641] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,642] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,643] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 11:34:40,643] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,645] INFO [Broker id=4] Leader __consumer_offsets-38 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,662] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,670] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,675] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,676] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 11:34:40,676] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,677] INFO [Broker id=4] Leader __consumer_offsets-35 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,684] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,688] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,690] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,691] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 11:34:40,691] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,692] INFO [Broker id=4] Leader __consumer_offsets-4 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,700] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,706] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,707] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,707] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 11:34:40,707] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,708] INFO [Broker id=4] Leader __consumer_offsets-1 with topic id Some(uAEioC8OSjiJcY7RlQ096Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 11:34:40,722] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-24 11:34:40,723] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,725] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,730] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,732] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 11:34:40,733] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,734] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,734] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,746] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,748] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,748] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 11:34:40,750] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,750] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,750] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,759] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,762] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,763] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 11:34:40,763] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,763] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,763] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,768] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,776] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,777] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 11:34:40,777] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,777] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,778] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,859] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,887] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,890] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 11:34:40,900] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,903] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,905] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,921] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,922] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,922] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 11:34:40,923] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,923] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,924] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,930] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,932] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,932] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 11:34:40,933] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,933] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,934] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,937] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,939] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,939] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 11:34:40,940] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,942] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,943] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,948] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,949] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,949] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 11:34:40,950] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,952] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,953] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,956] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,958] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,958] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 11:34:40,958] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,959] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,959] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,962] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,963] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,964] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 11:34:40,964] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,964] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,965] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,969] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,973] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,974] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 11:34:40,974] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,974] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,975] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,982] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,983] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,984] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 11:34:40,985] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,985] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,986] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:40,993] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:40,997] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:40,997] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 11:34:40,997] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:40,998] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:40,998] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,008] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,009] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,011] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 11:34:41,012] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,013] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,013] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,018] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,024] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,024] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 11:34:41,025] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,025] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,027] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,030] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,032] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,033] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 11:34:41,033] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,034] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,034] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,039] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,042] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,042] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 11:34:41,043] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,044] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,045] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,051] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,053] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,053] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 11:34:41,053] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,054] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,054] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,057] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,058] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,059] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 11:34:41,059] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,059] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,060] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,066] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,067] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,068] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 11:34:41,069] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,069] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,069] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,072] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,074] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,077] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 11:34:41,080] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,081] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,081] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,091] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,092] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,092] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 11:34:41,093] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,094] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,096] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,105] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,107] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,108] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 11:34:41,108] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,108] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,108] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,119] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,120] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,122] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 11:34:41,123] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,124] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,124] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,134] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,136] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,137] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 11:34:41,137] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,137] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,138] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,141] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,142] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,142] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,142] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,143] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,143] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,145] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,146] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,147] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 11:34:41,147] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,148] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,148] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,156] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,161] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,161] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 11:34:41,162] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,162] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,162] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,166] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,167] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,168] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 11:34:41,168] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,168] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,169] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,171] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,172] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,172] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 11:34:41,172] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,172] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,173] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,178] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,179] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,179] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 11:34:41,179] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,179] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,179] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id uAEioC8OSjiJcY7RlQ096Q. (state.change.logger)
[2025-12-24 11:34:41,182] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 11:34:41,183] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 11:34:41,184] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 11:34:41,184] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 11:34:41,184] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 11:34:41,185] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-13, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,185] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 11:34:41,186] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,190] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,191] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(uAEioC8OSjiJcY7RlQ096Q),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 11:34:41,191] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 11:34:41,191] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,192] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,192] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,192] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,193] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,193] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,193] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,193] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,193] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,194] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,194] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,194] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,194] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,194] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,194] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,195] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,195] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,195] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,196] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,197] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,197] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,198] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,198] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,198] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,199] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,199] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,199] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,199] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,199] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,199] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,200] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,200] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,200] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,200] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,203] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,204] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,205] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,205] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,205] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,205] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,206] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,206] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,206] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,207] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,207] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,207] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,207] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,207] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,207] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,208] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,208] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,208] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 4 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,208] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,208] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-14 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,209] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,209] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,209] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,210] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,209] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,210] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,211] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,211] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,211] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,211] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-43 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,211] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,212] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,212] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,212] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,212] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,213] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,213] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,213] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,214] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,213] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,215] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-17 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,215] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-18 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,214] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,216] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,216] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,217] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-28 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,217] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-25 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,218] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,218] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,218] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,219] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,218] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,220] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,220] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,220] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,220] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-6 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,221] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,221] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,221] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,222] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,222] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,222] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,222] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,221] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-38 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,222] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,223] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,223] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-35 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,223] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,224] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,223] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-4 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,225] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-1 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,224] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,226] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,227] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,227] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,227] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,227] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,227] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,227] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,228] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,228] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,228] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,228] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,229] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,229] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,229] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,229] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,229] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,229] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,229] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,229] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,230] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,230] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,230] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,230] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,230] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,231] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,231] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,231] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,231] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,231] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,232] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,232] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,232] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,232] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,233] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,233] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,233] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,233] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,233] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,234] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,234] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,234] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,234] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,234] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,234] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,234] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 11:34:41,235] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,235] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 11:34:41,233] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,236] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,237] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,237] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,237] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,237] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,239] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,239] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,239] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,239] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,239] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,242] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 11:34:41,683] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,684] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,684] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,684] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,684] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,685] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,685] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,685] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,685] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,686] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,686] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,686] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,686] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,686] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,686] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,686] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,687] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,687] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,687] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,688] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,688] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,689] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,689] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,690] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,690] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,690] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,691] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,691] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,691] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,691] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:34:41,692] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 11:34:41,692] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 11:44:34,193] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:44:34,195] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 11:44:39,281] INFO [NodeToControllerChannelManager id=4 name=forwarding] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:25:13,599] INFO [RaftManager id=4] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=1, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5986, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:25:13,628] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:25:13,687] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5986, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:25:13,734] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:30:26,434] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:26,438] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 6553 due to node 2 being disconnected (elapsed time since creation: 253453ms, elapsed time since send: 253452ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:26,481] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:26,522] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:26,533] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6105, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:26,558] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 6106 (kafka.log.UnifiedLog)
[2025-12-24 12:30:26,569] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 6106 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 12:30:26,569] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6106 (kafka.log.UnifiedLog$)
[2025-12-24 12:30:26,574] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 6106 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:30:26,574] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 5ms for segment recovery from offset 6106 (kafka.log.UnifiedLog$)
[2025-12-24 12:30:26,575] INFO [RaftManager id=4] Truncated to offset 6106 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 12:30:27,047] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:27,049] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:30:59,001] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6165, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6165, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:30:59,878] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:30:59,884] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:35:22,273] INFO [RaftManager id=4] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,289] INFO [RaftManager id=4] Completed transition to Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,318] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6285, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=7, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:35:22,892] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:35:22,896] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:40:53,626] INFO [RaftManager id=4] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,648] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,663] INFO [RaftManager id=4] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,675] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6404, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 12:40:53,680] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 6405 (kafka.log.UnifiedLog)
[2025-12-24 12:40:53,694] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 6405 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,695] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6405 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,698] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=6106, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000006106.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:40:53,712] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 6405 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 12:40:53,713] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 14ms for snapshot load and 3ms for segment recovery from offset 6405 (kafka.log.UnifiedLog$)
[2025-12-24 12:40:53,713] INFO [RaftManager id=4] Truncated to offset 6405 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 12:40:54,491] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:40:54,494] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 12:47:56,427] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000007247-0000000009 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 12:47:56,607] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000007247-0000000009 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 12:50:53,843] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 12:50:53,844] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:28,598] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Disconnecting from node 6 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:28,624] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Cancelled in-flight FETCH request with correlation id 13168 due to node 6 being disconnected (elapsed time since creation: 844804ms, elapsed time since send: 844804ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:28,632] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Client requested connection close from node 6 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:28,713] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Error sending fetch request (sessionId=94211298, epoch=13168) to node 6: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-12-24 13:52:28,817] INFO [RaftManager id=4] Completed transition to Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13281, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 13:52:28,884] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=94211298, epoch=13168), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-12-24 13:52:28,914] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13281, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 13:52:29,601] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 13:52:29,603] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:01:51,132] INFO [RaftManager id=4] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14357, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:01:51,184] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=11, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14357, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:01:51,600] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:01:51,601] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:02:23,346] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000014422-0000000011 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 14:02:23,399] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000014422-0000000011 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 14:06:56,202] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 14:06:56,465] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 14:06:56,504] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 14:06:56,513] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,342] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-24 14:07:02,546] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-24 14:07:02,564] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,878] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,923] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:02,978] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-24 14:07:02,991] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-24 14:07:02,995] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-24 14:07:03,009] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:03,136] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:03,138] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:03,140] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:03,167] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-24 14:07:03,211] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-24 14:07:03,218] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,226] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,302] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1809) from null (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:03,305] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-24 14:07:03,337] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-24 14:07:03,361] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,362] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-24 14:07:03,372] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:03,426] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,427] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,428] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,443] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@441974924 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:07:03,449] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,460] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:03,446] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-24 14:07:03,462] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 14:07:03,465] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,465] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-24 14:07:03,509] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:03,551] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-24 14:07:03,583] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,687] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,710] INFO [RaftManager id=4] Completed transition to Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1809) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:03,794] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,900] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:03,958] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 14:07:04,001] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,021] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-24 14:07:04,029] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-24 14:07:04,053] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-24 14:07:04,073] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,089] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,103] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,104] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,114] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,115] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,118] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,270] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,271] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,273] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,277] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,299] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,303] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,335] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-24 14:07:04,348] INFO [BrokerLifecycleManager id=4] Incarnation bqKrz4WdQiOi3Ubjgw8YXQ of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:04,353] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,370] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-24 14:07:04,374] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,442] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 14:07:04,446] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-24 14:07:04,445] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,448] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 14:07:04,456] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,465] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,550] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,644] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=1, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:07:04,652] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,674] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,675] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,678] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,678] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,685] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:07:04,687] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,721] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-24 14:07:04,723] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,738] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:07:04,754] INFO [MetadataLoader id=4] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,759] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,760] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,760] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:04,760] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=2) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-24 14:07:04,761] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-24 14:07:04,764] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-24 14:07:04,771] INFO Loaded 0 logs in 9ms (kafka.log.LogManager)
[2025-12-24 14:07:04,773] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-24 14:07:04,774] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-24 14:07:04,777] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-24 14:07:04,801] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 5 (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,092] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-24 14:07:05,095] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-24 14:07:05,100] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-24 14:07:05,101] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:05,107] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:05,109] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 14:07:05,110] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-24 14:07:05,110] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-24 14:07:05,119] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-24 14:07:05,139] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,140] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-24 14:07:05,140] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 14:07:05,140] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-24 14:07:05,142] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-24 14:07:05,147] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-24 14:07:05,154] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,155] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 14:07:05,204] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-24 14:07:05,205] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-24 14:07:05,207] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 14:07:05,207] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-24 14:07:05,208] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-24 14:07:05,209] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 14:07:05,211] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-24 14:07:05,218] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 14:07:05,218] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-24 14:07:05,219] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 14:07:05,219] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-24 14:07:05,220] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-24 14:07:05,222] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,223] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,227] INFO Kafka startTimeMs: 1766585225220 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-24 14:07:05,237] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-24 14:07:09,271] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-24 14:07:09,276] INFO [Broker id=4] Creating new partition _schemas-0 with topic id zuwcO5jNTBGzR0UELwhRXw. (state.change.logger)
[2025-12-24 14:07:09,307] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:09,318] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-24 14:07:09,323] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-24 14:07:09,326] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:09,328] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:09,333] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:09,337] INFO [Broker id=4] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 14:07:09,433] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:09,435] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(zuwcO5jNTBGzR0UELwhRXw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:09,443] INFO [Broker id=4] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-24 14:07:09,449] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:09,452] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:09,481] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 14:07:10,521] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 14:07:10,622] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 14:07:10,725] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-24 14:07:10,774] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-24 14:07:10,777] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-47, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-22, __consumer_offsets-17, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:10,778] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,790] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,792] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,793] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-24 14:07:10,795] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,797] INFO [Broker id=4] Leader __consumer_offsets-47 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,806] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,810] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,811] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,811] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-24 14:07:10,812] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,813] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,821] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,828] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,830] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,831] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-24 14:07:10,832] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,833] INFO [Broker id=4] Leader __consumer_offsets-13 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,840] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,845] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,846] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,846] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-24 14:07:10,848] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,848] INFO [Broker id=4] Leader __consumer_offsets-11 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,861] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,867] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,869] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,872] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-24 14:07:10,872] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,873] INFO [Broker id=4] Leader __consumer_offsets-44 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,878] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,880] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,881] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,882] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-24 14:07:10,883] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,884] INFO [Broker id=4] Leader __consumer_offsets-22 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,889] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,892] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,893] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,893] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-24 14:07:10,893] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,894] INFO [Broker id=4] Leader __consumer_offsets-17 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,897] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,901] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,902] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,903] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-24 14:07:10,903] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,903] INFO [Broker id=4] Leader __consumer_offsets-18 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,908] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,910] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,912] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,913] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,913] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,914] INFO [Broker id=4] Leader __consumer_offsets-0 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,916] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,919] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,923] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,923] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-24 14:07:10,923] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,923] INFO [Broker id=4] Leader __consumer_offsets-32 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,929] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,934] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,936] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,936] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-24 14:07:10,937] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,937] INFO [Broker id=4] Leader __consumer_offsets-28 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,942] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,945] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,946] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,947] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-24 14:07:10,947] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,948] INFO [Broker id=4] Leader __consumer_offsets-26 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,953] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,956] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,957] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,957] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-24 14:07:10,958] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,958] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,963] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,966] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,967] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,967] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-24 14:07:10,967] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,968] INFO [Broker id=4] Leader __consumer_offsets-8 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,971] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,974] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,975] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,975] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-24 14:07:10,976] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,977] INFO [Broker id=4] Leader __consumer_offsets-37 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,981] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,985] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,986] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,986] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-24 14:07:10,987] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,987] INFO [Broker id=4] Leader __consumer_offsets-35 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,990] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:10,993] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:10,993] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:10,994] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-24 14:07:10,994] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:10,994] INFO [Broker id=4] Leader __consumer_offsets-4 with topic id Some(UCkbIldXRJ2ybJxYCDDYQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:07:10,998] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-24 14:07:10,998] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,001] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,002] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,002] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-24 14:07:11,002] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,002] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,003] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,006] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,007] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,007] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-24 14:07:11,007] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,008] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,008] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,012] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,013] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,013] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-24 14:07:11,014] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,014] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,014] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,017] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,019] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,020] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-24 14:07:11,020] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,022] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,022] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,025] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,025] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,026] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-24 14:07:11,026] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,027] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,027] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,031] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,032] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,032] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-24 14:07:11,033] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,033] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,033] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,042] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,043] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,043] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-24 14:07:11,044] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,044] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,044] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,047] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,050] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,050] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-24 14:07:11,050] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,051] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,052] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,056] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,057] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,057] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-24 14:07:11,058] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,058] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,059] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,061] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,063] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,065] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-24 14:07:11,065] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,066] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,066] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,071] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,072] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,072] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-24 14:07:11,073] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,073] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,074] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,080] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,081] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,082] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-24 14:07:11,084] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,085] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,086] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,092] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,094] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,094] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-24 14:07:11,095] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,096] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,096] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,099] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,100] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,100] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-24 14:07:11,101] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,101] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,102] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,108] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,109] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,112] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-24 14:07:11,113] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,113] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,113] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,116] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,116] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,117] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-24 14:07:11,117] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,117] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,118] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,121] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,122] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,123] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-24 14:07:11,123] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,124] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,124] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,128] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,129] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,129] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-24 14:07:11,129] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,130] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,130] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,135] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,136] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,136] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-24 14:07:11,136] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,137] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,137] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,139] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,140] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,141] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-24 14:07:11,141] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,141] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,142] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,145] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,147] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,147] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-24 14:07:11,148] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,148] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,149] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,154] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,154] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,155] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-24 14:07:11,155] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,156] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,156] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,161] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,162] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,163] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-24 14:07:11,163] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,164] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,164] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,166] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,168] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,169] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-24 14:07:11,169] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,169] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,170] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,174] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,176] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,176] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-24 14:07:11,176] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,176] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,177] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,179] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,180] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,180] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-24 14:07:11,181] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,181] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,181] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,183] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,184] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,184] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-24 14:07:11,185] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,185] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,185] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,189] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,190] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,191] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-24 14:07:11,191] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,192] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,192] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,196] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,198] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,198] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-24 14:07:11,199] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,199] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,199] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,201] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,202] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,202] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-24 14:07:11,202] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,203] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,203] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,205] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,207] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,207] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-24 14:07:11,208] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,208] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,208] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,211] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,211] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,211] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-24 14:07:11,212] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,212] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,212] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id UCkbIldXRJ2ybJxYCDDYQg. (state.change.logger)
[2025-12-24 14:07:11,217] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:07:11,217] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-24 14:07:11,217] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-24 14:07:11,218] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:07:11,218] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:07:11,218] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-30, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-31, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-6, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,218] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 14:07:11,219] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-16 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,222] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-45 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(UCkbIldXRJ2ybJxYCDDYQg),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:07:11,223] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-24 14:07:11,223] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,225] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,226] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,227] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,227] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,227] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,228] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,228] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,229] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,229] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,229] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,230] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,230] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,230] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,230] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,231] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,231] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,232] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,232] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,233] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,233] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,233] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,234] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,234] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,234] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,235] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,236] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,236] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,237] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,237] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,237] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,237] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,237] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,238] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,238] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,242] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,244] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,245] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,245] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,245] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,245] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,246] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,248] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,249] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,249] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,250] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,250] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,250] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,251] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,251] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,251] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,251] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-47 in 6 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,252] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,251] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,252] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,252] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-13 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,254] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,253] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,254] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-44 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,254] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,255] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,255] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-22 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,256] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,255] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,256] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,257] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,257] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,258] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,258] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,259] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,258] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-32 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,259] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-28 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,259] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,260] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,260] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-26 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,260] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,260] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,261] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,260] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,261] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,261] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,261] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,261] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,262] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,262] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,263] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,264] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,265] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,266] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,266] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,266] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,267] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,267] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,267] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,267] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,268] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,268] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,268] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,268] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,269] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,269] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,269] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,269] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,269] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,269] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,270] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,270] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,270] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,270] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,271] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,271] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,271] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,271] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,272] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,272] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,272] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,272] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,272] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,273] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,273] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,273] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,273] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,274] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,273] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,274] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,275] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,275] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,275] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,276] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,275] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,276] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,277] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,278] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,277] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,279] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,280] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,280] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,281] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,281] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,281] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,281] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,281] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,282] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,283] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,283] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,283] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,283] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,283] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,283] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,284] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,284] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,286] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,286] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,286] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,287] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,287] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,287] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,287] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,287] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,288] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,288] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,288] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,288] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,288] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,289] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,289] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,289] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,289] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,289] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,290] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,290] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,290] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,291] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,291] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,292] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,291] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,292] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-24 14:07:11,292] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,292] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,293] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-24 14:07:11,293] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-24 14:07:11,370] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,370] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,370] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,370] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,370] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,370] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,371] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,371] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,371] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,371] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,371] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,372] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,372] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,373] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,374] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,374] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,375] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,375] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,375] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,375] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,375] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,376] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,376] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,376] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,376] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,376] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,377] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,377] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,377] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,377] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:07:11,377] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:07:11,377] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:17:04,494] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:10,844] INFO [NodeToControllerChannelManager id=4 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:24,710] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1298, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:17:24,811] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1298, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:17:26,418] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:17:26,420] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:26:55,547] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2429, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:26:55,611] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 2429 (kafka.log.UnifiedLog)
[2025-12-24 14:26:55,658] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 2429 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,662] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2429 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,676] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2429 with 0 producer ids in 7 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:26:55,677] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 13ms for segment recovery from offset 2429 (kafka.log.UnifiedLog$)
[2025-12-24 14:26:55,678] INFO [RaftManager id=4] Truncated to offset 2429 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:26:56,972] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:26:56,976] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 14:35:24,681] INFO [Broker id=4] Transitioning 2 partition(s) to local leaders. (state.change.logger)
[2025-12-24 14:35:24,692] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-1, financial_transactions-4) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,693] INFO [Broker id=4] Creating new partition financial_transactions-1 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,730] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,736] INFO Created log for partition financial_transactions-1 in /tmp/kafka-logs/financial_transactions-1 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,739] INFO [Partition financial_transactions-1 broker=4] No checkpointed highwatermark is found for partition financial_transactions-1 (kafka.cluster.Partition)
[2025-12-24 14:35:24,740] INFO [Partition financial_transactions-1 broker=4] Log loaded for partition financial_transactions-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,741] INFO [Broker id=4] Leader financial_transactions-1 with topic id Some(EMZmT9B4Rquk5_L89orVRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:35:24,753] INFO [Broker id=4] Creating new partition financial_transactions-4 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,760] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,762] INFO Created log for partition financial_transactions-4 in /tmp/kafka-logs/financial_transactions-4 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,763] INFO [Partition financial_transactions-4 broker=4] No checkpointed highwatermark is found for partition financial_transactions-4 (kafka.cluster.Partition)
[2025-12-24 14:35:24,764] INFO [Partition financial_transactions-4 broker=4] Log loaded for partition financial_transactions-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,764] INFO [Broker id=4] Leader financial_transactions-4 with topic id Some(EMZmT9B4Rquk5_L89orVRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-24 14:35:24,769] INFO [Broker id=4] Transitioning 3 partition(s) to local followers. (state.change.logger)
[2025-12-24 14:35:24,770] INFO [Broker id=4] Creating new partition financial_transactions-2 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,779] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,780] INFO Created log for partition financial_transactions-2 in /tmp/kafka-logs/financial_transactions-2 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,780] INFO [Partition financial_transactions-2 broker=4] No checkpointed highwatermark is found for partition financial_transactions-2 (kafka.cluster.Partition)
[2025-12-24 14:35:24,781] INFO [Partition financial_transactions-2 broker=4] Log loaded for partition financial_transactions-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,781] INFO [Broker id=4] Follower financial_transactions-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,781] INFO [Broker id=4] Creating new partition financial_transactions-3 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,786] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,788] INFO Created log for partition financial_transactions-3 in /tmp/kafka-logs/financial_transactions-3 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,788] INFO [Partition financial_transactions-3 broker=4] No checkpointed highwatermark is found for partition financial_transactions-3 (kafka.cluster.Partition)
[2025-12-24 14:35:24,788] INFO [Partition financial_transactions-3 broker=4] Log loaded for partition financial_transactions-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,788] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,788] INFO [Broker id=4] Creating new partition financial_transactions-0 with topic id EMZmT9B4Rquk5_L89orVRw. (state.change.logger)
[2025-12-24 14:35:24,794] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:35:24,795] INFO Created log for partition financial_transactions-0 in /tmp/kafka-logs/financial_transactions-0 with properties {} (kafka.log.LogManager)
[2025-12-24 14:35:24,795] INFO [Partition financial_transactions-0 broker=4] No checkpointed highwatermark is found for partition financial_transactions-0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,795] INFO [Partition financial_transactions-0 broker=4] Log loaded for partition financial_transactions-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-24 14:35:24,795] INFO [Broker id=4] Follower financial_transactions-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-24 14:35:24,796] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-0, financial_transactions-2, financial_transactions-3) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,796] INFO [Broker id=4] Stopped fetchers as part of become-follower for 3 partitions (state.change.logger)
[2025-12-24 14:35:24,805] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(financial_transactions-0 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,805] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(financial_transactions-2 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-3 -> InitialFetchState(Some(EMZmT9B4Rquk5_L89orVRw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-24 14:35:24,806] INFO [Broker id=4] Started fetchers as part of become-follower for 3 partitions (state.change.logger)
[2025-12-24 14:35:24,992] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:24,995] INFO [UnifiedLog partition=financial_transactions-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:35:24,995] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:24,998] INFO [UnifiedLog partition=financial_transactions-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:35:25,048] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition financial_transactions-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-24 14:35:25,058] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-24 14:36:55,576] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:53:22,438] INFO [RaftManager id=4] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,555] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,606] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5420, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 14:53:22,608] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 5421 (kafka.log.UnifiedLog)
[2025-12-24 14:53:22,626] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 5421 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,627] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 5421 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,627] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=2429, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000002429.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:53:22,639] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 5421 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 14:53:22,640] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 4ms for segment recovery from offset 5421 (kafka.log.UnifiedLog$)
[2025-12-24 14:53:22,640] INFO [RaftManager id=4] Truncated to offset 5421 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 14:53:23,557] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 14:53:23,558] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 15:03:22,491] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 15:03:22,814] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 15:08:34,714] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000007238-0000000007 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 15:08:34,978] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000007238-0000000007 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 16:18:13,871] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:13,902] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:13,950] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,049] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:14,050] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,054] INFO [RaftManager id=4] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=7, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:18:14,098] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=13968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:18:14,100] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,157] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:18:14,159] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:18:14,210] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:21:54,930] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000014409-0000000008 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 16:21:55,008] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000014409-0000000008 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 16:28:13,783] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:28:14,195] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,424] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,435] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 17043 due to node 2 being disconnected (elapsed time since creation: 1021269ms, elapsed time since send: 1021268ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:14,535] INFO [RaftManager id=4] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=15282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:46:14,564] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=15282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 16:46:15,126] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 16:46:15,131] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 16:56:14,842] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,422] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,444] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,494] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,560] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,562] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,613] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,618] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 17:34:58,619] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,623] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=21100, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=9, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=21100, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 17:34:58,641] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 21100 (kafka.log.UnifiedLog)
[2025-12-24 17:34:58,661] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 21100 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 17:34:58,661] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 21100 (kafka.log.UnifiedLog$)
[2025-12-24 17:34:58,662] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=5421, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000005421.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 17:34:58,670] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 17:34:58,679] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 21100 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 17:34:58,680] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 6ms for snapshot load and 12ms for segment recovery from offset 21100 (kafka.log.UnifiedLog$)
[2025-12-24 17:34:58,680] INFO [RaftManager id=4] Truncated to offset 21100 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 17:38:55,956] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000021573-0000000013 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 17:38:56,016] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000021573-0000000013 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 17:44:58,579] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,032] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,153] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 31197 due to node 2 being disconnected (elapsed time since creation: 2118ms, elapsed time since send: 2050ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,480] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,482] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 7119 due to node 2 being disconnected (elapsed time since creation: 4508ms, elapsed time since send: 4508ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,503] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,661] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-24 18:35:43,799] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,802] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,853] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,885] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:43,887] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:43,922] INFO [RaftManager id=4] Completed transition to Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:44,058] INFO [RaftManager id=4] Completed transition to Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:44,325] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28351, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 18:35:44,353] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:44,400] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:35:44,404] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:35:44,455] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 18:38:56,077] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000028734-0000000016 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-24 18:38:56,159] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000028734-0000000016 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-24 18:45:44,043] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 18:45:44,297] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,667] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,703] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 20:32:35,753] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 20:32:35,802] INFO [RaftManager id=4] Completed transition to Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33975, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 20:32:35,823] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 20:32:35,865] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33975, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 20:32:35,928] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 21:41:15,984] INFO [RaftManager id=4] Completed transition to Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,074] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,090] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=19, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 21:41:16,095] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 34034 (kafka.log.UnifiedLog)
[2025-12-24 21:41:16,117] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 34034 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-24 21:41:16,119] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 34034 (kafka.log.UnifiedLog$)
[2025-12-24 21:41:16,120] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=21100, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000021100.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 21:41:16,145] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 34034 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-24 21:41:16,145] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 10ms for snapshot load and 15ms for segment recovery from offset 34034 (kafka.log.UnifiedLog$)
[2025-12-24 21:41:16,145] INFO [RaftManager id=4] Truncated to offset 34034 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-24 21:41:16,939] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 21:41:16,942] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 22:57:16,683] INFO [RaftManager id=4] Completed transition to Unattached(epoch=20, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=19, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34091, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 22:57:16,730] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=20, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34091, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=20, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 22:57:17,689] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 22:57:17,695] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-24 23:41:42,715] INFO [RaftManager id=4] Completed transition to Unattached(epoch=21, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=20, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-24 23:41:42,729] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=21, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=21, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-24 23:41:44,254] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-24 23:41:44,298] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 00:44:16,911] INFO [RaftManager id=4] Completed transition to Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=21, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:16,921] INFO [RaftManager id=4] Completed transition to Unattached(epoch=23, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:16,962] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=23, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34209, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=23, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 00:44:18,671] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 00:44:18,673] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:19:59,867] INFO [RaftManager id=4] Completed transition to Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=23, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:19:59,889] INFO [RaftManager id=4] Completed transition to Unattached(epoch=25, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:19:59,914] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=25, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=34269, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=25, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:20:01,376] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:20:01,381] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:29:59,999] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:30:00,006] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:06,036] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:06,154] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:06,980] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:07,088] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:07,531] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:07,594] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:07,945] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:07,961] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:31:08,154] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:08,159] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 39507 due to node 2 being disconnected (elapsed time since creation: 2027ms, elapsed time since send: 2018ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:08,184] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 01:31:08,231] INFO [RaftManager id=4] Completed transition to Unattached(epoch=29, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=25, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=35590, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,254] INFO [RaftManager id=4] Completed transition to Unattached(epoch=30, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=29, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,293] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:31:08,312] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=30, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=35590, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=30, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:31:08,321] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 35590 (kafka.log.UnifiedLog)
[2025-12-25 01:31:08,344] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 35590 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,345] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 35590 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,346] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=34034, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000034034.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:31:08,392] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 35590 with 0 producer ids in 10 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 01:31:08,393] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 12ms for snapshot load and 36ms for segment recovery from offset 35590 (kafka.log.UnifiedLog$)
[2025-12-25 01:31:08,393] INFO [RaftManager id=4] Truncated to offset 35590 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 01:31:08,395] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 01:33:39,378] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000035891-0000000030 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 01:33:39,431] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000035891-0000000030 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 01:54:59,170] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,180] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 40726 due to node 1 being disconnected (elapsed time since creation: 899745ms, elapsed time since send: 899745ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,264] INFO [RaftManager id=4] Completed transition to Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=30, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=36648, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:54:59,341] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=31, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=36648, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 01:54:59,682] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 01:54:59,683] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 02:04:59,450] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 02:04:59,502] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 02:48:38,286] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000043059-0000000031 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 02:48:38,459] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000043059-0000000031 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 03:46:47,763] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:47,853] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 55200 due to node 3 being disconnected (elapsed time since creation: 2138ms, elapsed time since send: 2138ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,139] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,175] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 12574 due to node 3 being disconnected (elapsed time since creation: 4564ms, elapsed time since send: 4564ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:49,448] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:46:49,858] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 03:46:53,050] INFO [RaftManager id=4] Completed transition to Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=31, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=50007, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 03:46:53,466] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=34, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=50007, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 03:46:53,495] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 50008 (kafka.log.UnifiedLog)
[2025-12-25 03:46:53,497] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2025-12-25 03:46:53,615] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 50008 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 03:46:53,638] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 50008 (kafka.log.UnifiedLog$)
[2025-12-25 03:46:53,646] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=35590, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000035590.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 03:46:53,696] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 50008 with 0 producer ids in 18 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 03:46:53,698] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 11ms for snapshot load and 44ms for segment recovery from offset 50008 (kafka.log.UnifiedLog$)
[2025-12-25 03:46:53,702] INFO [RaftManager id=4] Truncated to offset 50008 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 03:46:53,703] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:46:53,725] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 03:48:38,637] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000050217-0000000034 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 03:48:38,701] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000050217-0000000034 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 03:56:51,370] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 03:56:53,550] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 04:48:39,115] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000057390-0000000034 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 04:48:39,253] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000057390-0000000034 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 04:54:06,174] INFO [RaftManager id=4] Completed transition to Unattached(epoch=35, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=34, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 04:54:06,399] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=35, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58033, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=35, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 04:54:07,066] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 04:54:07,081] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:10:03,417] INFO [RaftManager id=4] Completed transition to Unattached(epoch=36, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=35, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,480] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=36, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=36, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,495] INFO [RaftManager id=4] Completed transition to Unattached(epoch=37, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=36, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,506] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=37, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58150, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=37, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:10:03,509] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 58151 (kafka.log.UnifiedLog)
[2025-12-25 05:10:03,528] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 58151 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,529] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 58151 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,530] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=50008, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000050008.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:10:03,582] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 58151 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:10:03,583] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 6ms for snapshot load and 47ms for segment recovery from offset 58151 (kafka.log.UnifiedLog$)
[2025-12-25 05:10:03,583] INFO [RaftManager id=4] Truncated to offset 58151 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 05:10:05,270] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:10:05,273] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:27:01,627] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,632] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 64349 due to node 3 being disconnected (elapsed time since creation: 898558ms, elapsed time since send: 898558ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,687] INFO [RaftManager id=4] Completed transition to Unattached(epoch=38, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=37, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,752] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=38, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=38, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,768] INFO [RaftManager id=4] Completed transition to Unattached(epoch=39, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=38, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,785] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=39, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=58389, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=39, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 05:27:01,787] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 58390 (kafka.log.UnifiedLog)
[2025-12-25 05:27:01,792] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 58390 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,793] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 58390 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,794] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=58151, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000058151.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:27:01,802] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 58390 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 05:27:01,803] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 7ms for segment recovery from offset 58390 (kafka.log.UnifiedLog$)
[2025-12-25 05:27:01,804] INFO [RaftManager id=4] Truncated to offset 58390 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 05:27:01,903] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:27:01,904] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 05:37:01,991] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 05:37:02,002] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 06:18:38,659] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000064553-0000000039 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 06:18:38,757] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000064553-0000000039 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 07:18:38,954] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000071718-0000000039 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 07:18:39,018] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000071718-0000000039 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 07:58:34,614] INFO [RaftManager id=4] Completed transition to Unattached(epoch=40, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=39, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=75411, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 07:58:34,707] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=40, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=75411, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=40, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 07:58:35,597] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 07:58:35,600] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:01:41,996] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:01:44,716] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:01:44,770] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:01:44,789] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:52,330] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:01:52,633] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:01:52,653] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,112] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,119] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,169] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-25 12:01:53,175] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-25 12:01:53,176] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-25 12:01:53,183] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,268] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:53,270] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:53,282] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:01:53,299] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-25 12:01:53,321] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-25 12:01:53,330] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:53,343] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:53,423] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1671) from null (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:01:53,432] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-25 12:01:53,432] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-25 12:01:53,458] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-25 12:01:53,459] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:53,477] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:01:53,577] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:53,584] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:53,581] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:53,581] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:53,581] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:01:53,727] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:01:53,745] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:01:53,770] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:53,805] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:01:53,809] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-25 12:01:53,809] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1146069111 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:01:53,835] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:53,884] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:53,889] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:53,974] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:53,985] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,082] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,129] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,143] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,251] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,278] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,287] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,296] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,299] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,308] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,309] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,370] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,423] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,432] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,437] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,449] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,475] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,536] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,539] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,622] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,729] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:54,773] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,775] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,778] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,780] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,794] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,795] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:54,849] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,001] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,219] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,406] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,526] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,562] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,545] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,685] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,701] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,700] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,727] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,739] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:55,834] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:55,962] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,022] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:01:56,159] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,255] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:56,260] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:56,281] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:56,293] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:56,290] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,427] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,543] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,671] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:56,988] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,001] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,045] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,185] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,186] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,273] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,712] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,716] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-25 12:01:57,778] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:01:57,847] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:57,869] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:57,889] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:01:58,052] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,166] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,274] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,385] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,540] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:58,737] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-25 12:01:59,102] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,209] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,272] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:01:59,315] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,417] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,437] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:01:59,569] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,762] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,796] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:01:59,871] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:01:59,916] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:01:59,948] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:01:59,980] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,005] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:00,092] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,111] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:00,252] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,359] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,482] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,589] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:00,590] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:00,621] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:00,729] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,347] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,413] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-25 12:02:01,435] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:01,440] INFO [RaftManager id=4] Completed transition to Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1671) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:01,461] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,474] INFO [BrokerLifecycleManager id=4] Incarnation G1MGr_z5SLS9tyr-oTCwng of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:01,483] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:02:01,516] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,531] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,552] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,554] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:01,568] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:02:01,574] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:02:01,575] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:02:01,568] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,683] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,754] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=2, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:01,789] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:01,940] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,052] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,111] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,112] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,210] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,277] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,292] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,321] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,422] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,526] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,638] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,759] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,872] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:02,920] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,929] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,953] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,964] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.18.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:02,991] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,114] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,225] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,343] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,452] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,455] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,456] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:03,565] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,666] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,834] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:03,940] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,010] INFO [RaftManager id=4] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:04,040] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,046] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:04,047] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.18.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:02:04,141] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,244] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,347] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,449] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,560] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,577] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:02:04,643] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,657] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,658] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,665] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,680] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:02:04,792] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,852] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 7 (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:04,894] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:04,997] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,010] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=6, metadata=Optional.empty)] for the first time for epoch 4 (org.apache.kafka.raft.FollowerState)
[2025-12-25 12:02:05,105] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,145] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,166] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 6 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,178] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,195] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,196] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:05,203] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=4) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-25 12:02:05,205] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-25 12:02:05,216] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-25 12:02:05,222] INFO Loaded 0 logs in 16ms (kafka.log.LogManager)
[2025-12-25 12:02:05,223] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-25 12:02:05,236] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-25 12:02:05,287] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-25 12:02:05,759] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-25 12:02:05,997] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-25 12:02:06,040] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-25 12:02:06,110] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:06,146] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:06,336] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:02:06,368] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:02:06,389] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-25 12:02:06,456] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:02:06,767] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:06,774] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:02:06,780] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:02:06,787] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:02:06,812] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:06,827] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:02:06,866] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:02:06,889] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:02:07,092] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:02:07,097] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:02:07,120] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:02:07,124] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:02:07,127] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-25 12:02:07,137] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:02:07,141] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:02:07,154] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:02:07,156] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:02:07,157] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:02:07,157] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:02:07,160] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-25 12:02:07,166] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:07,174] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:07,176] INFO Kafka startTimeMs: 1766664127165 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:02:07,180] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-25 12:02:21,308] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:02:21,342] INFO [Broker id=4] Creating new partition _schemas-0 with topic id 5ynfWlfWR8GWI1r7sCBFzA. (state.change.logger)
[2025-12-25 12:02:21,439] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:21,456] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-25 12:02:21,470] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-25 12:02:21,476] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:21,491] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:21,494] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:21,496] INFO [Broker id=4] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:02:21,579] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:21,592] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(5ynfWlfWR8GWI1r7sCBFzA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:21,595] INFO [Broker id=4] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:02:21,601] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:21,622] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:21,651] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:02:22,764] INFO [Broker id=4] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-12-25 12:02:22,812] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-9, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-36, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:22,815] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:22,896] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:22,920] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:22,920] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-25 12:02:22,922] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:22,929] INFO [Broker id=4] Leader __consumer_offsets-45 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:22,942] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:22,962] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:22,966] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:22,967] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-25 12:02:22,970] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:22,974] INFO [Broker id=4] Leader __consumer_offsets-14 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:22,995] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,001] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,006] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,007] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-25 12:02:23,008] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,009] INFO [Broker id=4] Leader __consumer_offsets-43 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,027] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,044] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,046] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,046] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-25 12:02:23,047] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,047] INFO [Broker id=4] Leader __consumer_offsets-9 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,076] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,087] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,089] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,090] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-25 12:02:23,091] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,091] INFO [Broker id=4] Leader __consumer_offsets-24 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,116] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,125] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,136] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,137] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-25 12:02:23,143] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,145] INFO [Broker id=4] Leader __consumer_offsets-21 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,156] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,163] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,165] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,165] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-25 12:02:23,166] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,167] INFO [Broker id=4] Leader __consumer_offsets-19 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,177] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,189] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,193] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,195] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-25 12:02:23,197] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,198] INFO [Broker id=4] Leader __consumer_offsets-17 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,206] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,217] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,218] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,219] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-25 12:02:23,219] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,220] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,234] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,244] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,245] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,246] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-25 12:02:23,246] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,248] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,256] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,265] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,269] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,271] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-25 12:02:23,272] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,272] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,296] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,302] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,304] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,305] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-25 12:02:23,305] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,305] INFO [Broker id=4] Leader __consumer_offsets-8 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,313] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,317] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,319] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,321] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-25 12:02:23,322] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,323] INFO [Broker id=4] Leader __consumer_offsets-35 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,331] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,338] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,339] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,339] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-25 12:02:23,339] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,339] INFO [Broker id=4] Leader __consumer_offsets-4 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,344] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,350] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,351] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,351] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-25 12:02:23,352] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,352] INFO [Broker id=4] Leader __consumer_offsets-36 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,358] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,367] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,369] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,369] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-25 12:02:23,370] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,371] INFO [Broker id=4] Leader __consumer_offsets-2 with topic id Some(7ynXfZBTSvqSbSTAo4jvJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:02:23,376] INFO [Broker id=4] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:02:23,377] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,386] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,388] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,389] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-25 12:02:23,390] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,391] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,392] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,397] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,399] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,401] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-25 12:02:23,402] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,403] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,403] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,413] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,415] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,416] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-25 12:02:23,417] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,421] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,423] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,434] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,436] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,438] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-25 12:02:23,442] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,445] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,445] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,473] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,475] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,476] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-25 12:02:23,476] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,477] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,477] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,489] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,491] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,492] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-25 12:02:23,492] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,493] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,494] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,502] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,504] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,507] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-25 12:02:23,507] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,508] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,508] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,514] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,517] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,518] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-25 12:02:23,518] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,519] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,519] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,525] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,527] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,530] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-25 12:02:23,532] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,532] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,533] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,536] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,541] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,542] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-25 12:02:23,542] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,543] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,544] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,562] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,564] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,565] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-25 12:02:23,566] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,567] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,567] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,580] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,581] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,582] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-25 12:02:23,592] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,598] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,601] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,622] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,625] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,625] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-25 12:02:23,626] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,626] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,627] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,632] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,634] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,637] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-25 12:02:23,637] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,638] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,638] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,641] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,643] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,644] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-25 12:02:23,644] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,645] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,645] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,648] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,649] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,649] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-25 12:02:23,650] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,650] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,650] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,655] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,657] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,659] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-25 12:02:23,662] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,664] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,668] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,674] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,675] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,676] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-25 12:02:23,676] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,677] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,677] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,679] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,680] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,681] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-25 12:02:23,682] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,683] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,683] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,689] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,691] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,691] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-25 12:02:23,692] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,692] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,693] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,698] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,699] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,700] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-25 12:02:23,700] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,701] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,701] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,704] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,705] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,705] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-25 12:02:23,706] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,707] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,707] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,710] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,711] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,712] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-25 12:02:23,712] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,712] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,713] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,716] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,717] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,717] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-25 12:02:23,718] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,718] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,719] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,722] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,722] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,723] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-25 12:02:23,723] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,723] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,724] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,726] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,729] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,729] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-25 12:02:23,730] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,730] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,730] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,735] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,736] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,736] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-25 12:02:23,737] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,737] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,737] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,742] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,743] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,744] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-25 12:02:23,745] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,745] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,746] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,749] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,750] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,750] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,751] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,751] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,751] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,761] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,762] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,763] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-25 12:02:23,764] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,765] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,765] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,769] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,771] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,772] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-25 12:02:23,772] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,773] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,773] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,779] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,781] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,782] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-25 12:02:23,782] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,783] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,784] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,789] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,791] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,792] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-25 12:02:23,792] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,793] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,793] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id 7ynXfZBTSvqSbSTAo4jvJQ. (state.change.logger)
[2025-12-25 12:02:23,801] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:02:23,803] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:02:23,803] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-25 12:02:23,804] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:02:23,804] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:02:23,807] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-37, __consumer_offsets-6, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,808] INFO [Broker id=4] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-25 12:02:23,816] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,818] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,817] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-48 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,819] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(7ynXfZBTSvqSbSTAo4jvJQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:02:23,820] INFO [Broker id=4] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-12-25 12:02:23,818] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,821] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,822] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,822] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,823] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,824] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,824] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,824] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,825] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,825] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,826] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,827] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,827] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,827] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,828] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,828] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,829] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,829] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,829] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,830] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,830] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,830] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,830] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,830] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,831] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,831] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,832] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,832] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,832] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,833] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,833] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,833] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,834] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,842] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,846] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,856] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,860] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,860] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,861] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,862] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,863] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,863] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,864] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,864] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,866] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,869] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,869] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,871] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,874] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,871] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-45 in 21 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,881] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,887] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,888] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,889] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,890] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,890] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,884] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-14 in 23 milliseconds for epoch 0, of which 22 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,891] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-43 in 29 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,890] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,891] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,891] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-9 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,891] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,892] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,893] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-24 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,893] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,893] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,893] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-21 in 24 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,895] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-19 in 23 milliseconds for epoch 0, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,894] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,896] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,898] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,897] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-17 in 16 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,902] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 14 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,900] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,905] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 15 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,907] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,906] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,908] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-8 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,908] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-35 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,909] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,909] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,909] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-4 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-36 in 13 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,910] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,910] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,912] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,911] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,912] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,912] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,913] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,914] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,914] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,915] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,916] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,917] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,917] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,917] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,917] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,918] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,918] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,919] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,919] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,919] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,919] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,920] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,920] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,922] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,922] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,922] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,923] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,924] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,924] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,924] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,921] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,925] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,925] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,925] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,925] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,927] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,926] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,931] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,931] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,932] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,933] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,932] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,933] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,934] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,933] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,934] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,935] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,935] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,935] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,935] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,936] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,936] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,937] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,938] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,939] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,938] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,940] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,939] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,941] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,942] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,942] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,942] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,944] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,943] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,945] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,946] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,947] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,947] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,948] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,949] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,948] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,949] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,949] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,954] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,955] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,955] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:02:23,955] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,955] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:02:23,957] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,957] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,957] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,958] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,958] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,959] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,960] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,961] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,962] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,963] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,963] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,964] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,964] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,964] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,965] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,965] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,966] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,967] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,969] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,969] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,969] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,970] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,970] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,971] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,971] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,973] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,973] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,974] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,976] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,977] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:23,978] INFO [GroupCoordinator 4]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-99b7363f-0240-4d67-9e8b-611ef15f38f4 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:23,982] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:02:23,982] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:02:24,030] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-99b7363f-0240-4d67-9e8b-611ef15f38f4 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:27,047] INFO [GroupCoordinator 4]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:02:27,172] INFO [GroupCoordinator 4]: Assignment received from leader sr-1-99b7363f-0240-4d67-9e8b-611ef15f38f4 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:12:04,561] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,253] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,266] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,320] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,355] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,358] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,410] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,444] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,445] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,497] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,518] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,519] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,570] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,584] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,585] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,636] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,646] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,648] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,700] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,708] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:35,713] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:35,720] INFO [RaftManager id=4] Completed transition to Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3911, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:46:36,011] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=5, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3911, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:46:36,069] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:36,130] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 12:46:36,132] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:46:36,183] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:55:34,778] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-99b7363f-0240-4d67-9e8b-611ef15f38f4 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:55:34,861] INFO [GroupCoordinator 4]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:55:34,903] INFO [GroupCoordinator 4]: Member MemberMetadata(memberId=sr-1-99b7363f-0240-4d67-9e8b-611ef15f38f4, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.16, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:34,081] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:57:34,621] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:57:34,709] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:57:34,714] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:40,404] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-12-25 12:57:41,185] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-12-25 12:57:41,206] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:42,280] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:42,366] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:42,825] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-12-25 12:57:42,910] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-12-25 12:57:42,959] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-12-25 12:57:43,033] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:44,091] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:44,110] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:44,116] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 1ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:44,200] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-12-25 12:57:44,617] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-12-25 12:57:44,661] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:44,689] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:45,092] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1721) from null (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:45,094] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-12-25 12:57:45,095] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-12-25 12:57:45,877] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:45,896] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-12-25 12:57:45,913] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:46,012] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,026] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:46,029] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:46,029] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:46,089] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-12-25 12:57:46,061] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@970821424 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 12:57:46,124] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,232] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,319] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:57:46,324] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-12-25 12:57:46,335] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,345] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,370] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-12-25 12:57:46,426] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1721) (org.apache.kafka.raft.QuorumState)
[2025-12-25 12:57:46,442] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,467] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:46,547] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,580] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 2 (org.apache.kafka.raft.FollowerState)
[2025-12-25 12:57:46,592] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,647] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 5 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,654] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:46,931] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:57:46,977] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-12-25 12:57:46,983] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-12-25 12:57:47,014] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-12-25 12:57:47,038] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,043] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,062] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,063] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,076] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,078] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,082] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,082] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,085] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,125] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,127] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,159] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-12-25 12:57:47,161] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,162] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 12:57:47,165] INFO [BrokerLifecycleManager id=4] Incarnation DW_glk-VSeaMdU8sCT0CPQ of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:47,181] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-12-25 12:57:47,216] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:57:47,217] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 10 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:47,218] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-12-25 12:57:47,220] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:57:47,220] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 10 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:47,225] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=10, epoch=2) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-12-25 12:57:47,235] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-12-25 12:57:47,239] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 12 (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:47,254] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-12-25 12:57:47,287] INFO Loaded 0 logs in 47ms (kafka.log.LogManager)
[2025-12-25 12:57:47,291] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-12-25 12:57:47,297] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-12-25 12:57:47,311] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-12-25 12:57:48,287] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-12-25 12:57:48,315] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-12-25 12:57:48,327] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-12-25 12:57:48,333] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:48,352] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:48,375] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:57:48,399] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-12-25 12:57:48,399] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-12-25 12:57:48,537] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 10 (org.apache.kafka.image.loader.MetadataLoader)
[2025-12-25 12:57:48,588] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,595] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-12-25 12:57:48,600] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:57:48,604] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-12-25 12:57:48,615] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-12-25 12:57:48,632] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,635] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-12-25 12:57:48,685] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:57:48,896] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-12-25 12:57:48,897] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-12-25 12:57:48,914] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:57:48,921] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-12-25 12:57:48,924] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-12-25 12:57:48,930] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:57:48,967] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-12-25 12:57:49,022] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:57:49,029] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-12-25 12:57:49,034] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:57:49,042] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-12-25 12:57:49,047] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-12-25 12:57:49,048] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:49,052] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:49,053] INFO Kafka startTimeMs: 1766667469047 (org.apache.kafka.common.utils.AppInfoParser)
[2025-12-25 12:57:49,062] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-12-25 12:57:55,397] INFO Sent auto-creation request for Set(_schemas) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-12-25 12:57:55,727] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:57:55,744] INFO [Broker id=4] Creating new partition _schemas-0 with topic id wkzSqzyiRv-1bUOz43_kpQ. (state.change.logger)
[2025-12-25 12:57:55,762] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:55,765] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-12-25 12:57:55,769] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-12-25 12:57:55,774] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:55,776] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:55,778] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:55,780] INFO [Broker id=4] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:57:55,853] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:55,856] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(wkzSqzyiRv-1bUOz43_kpQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:55,857] INFO [Broker id=4] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-12-25 12:57:55,859] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:55,860] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:55,874] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:57:57,228] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-12-25 12:57:57,235] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-10, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:57,237] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,265] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,268] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,268] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-12-25 12:57:57,270] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,273] INFO [Broker id=4] Leader __consumer_offsets-15 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,303] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,311] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,313] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,313] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-12-25 12:57:57,314] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,315] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,337] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,350] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,357] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,361] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-12-25 12:57:57,363] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,363] INFO [Broker id=4] Leader __consumer_offsets-45 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,370] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,378] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,381] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,382] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-12-25 12:57:57,383] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,385] INFO [Broker id=4] Leader __consumer_offsets-14 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,398] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,431] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,432] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,433] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-12-25 12:57:57,433] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,434] INFO [Broker id=4] Leader __consumer_offsets-10 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,448] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,476] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,477] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,480] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-12-25 12:57:57,480] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,481] INFO [Broker id=4] Leader __consumer_offsets-42 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,488] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,519] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,520] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,521] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-12-25 12:57:57,521] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,521] INFO [Broker id=4] Leader __consumer_offsets-23 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,560] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,579] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,593] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,596] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-12-25 12:57:57,601] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,604] INFO [Broker id=4] Leader __consumer_offsets-19 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,623] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,637] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,642] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,644] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-12-25 12:57:57,647] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,649] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,667] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,674] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,676] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,677] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-12-25 12:57:57,677] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,678] INFO [Broker id=4] Leader __consumer_offsets-28 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,684] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,693] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,698] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,699] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-12-25 12:57:57,699] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,701] INFO [Broker id=4] Leader __consumer_offsets-25 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,709] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,716] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,717] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,717] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-12-25 12:57:57,718] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,719] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,724] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,733] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,739] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,740] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-12-25 12:57:57,740] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,740] INFO [Broker id=4] Leader __consumer_offsets-6 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,752] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,767] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,774] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,775] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-12-25 12:57:57,775] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,776] INFO [Broker id=4] Leader __consumer_offsets-38 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,787] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,803] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,807] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,808] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-12-25 12:57:57,810] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,811] INFO [Broker id=4] Leader __consumer_offsets-3 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,820] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,832] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,838] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,855] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-12-25 12:57:57,860] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,861] INFO [Broker id=4] Leader __consumer_offsets-33 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,904] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:57,939] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:57,941] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:57,942] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-12-25 12:57:57,943] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:57,944] INFO [Broker id=4] Leader __consumer_offsets-2 with topic id Some(ue_8SQUBS_2rDuQWfoEGdw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-12-25 12:57:57,977] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-12-25 12:57:57,989] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,024] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,052] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,061] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-12-25 12:57:58,067] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,068] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,070] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,107] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,113] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,113] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-12-25 12:57:58,114] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,114] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,115] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,143] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,146] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,147] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-12-25 12:57:58,150] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,150] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,150] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,171] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,188] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,195] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-12-25 12:57:58,195] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,197] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,197] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,221] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,234] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,236] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-12-25 12:57:58,236] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,238] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,240] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,276] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,278] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,280] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-12-25 12:57:58,280] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,282] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,283] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,288] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,296] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,296] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-12-25 12:57:58,297] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,297] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,298] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,307] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,310] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,311] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-12-25 12:57:58,311] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,311] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,312] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,316] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,317] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,319] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-12-25 12:57:58,320] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,320] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,321] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,346] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,348] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,352] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-12-25 12:57:58,354] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,354] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,358] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,364] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,365] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,365] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-12-25 12:57:58,366] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,366] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,367] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,370] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,372] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,373] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-12-25 12:57:58,374] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,374] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,375] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,395] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,396] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,397] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-12-25 12:57:58,397] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,398] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,398] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,406] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,407] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,407] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-12-25 12:57:58,408] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,408] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,409] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,425] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,431] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,432] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-12-25 12:57:58,433] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,433] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,434] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,443] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,446] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,447] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-12-25 12:57:58,447] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,447] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,447] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,458] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,461] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,462] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-12-25 12:57:58,462] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,463] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,465] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,472] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,476] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,476] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-12-25 12:57:58,477] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,477] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,478] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,495] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,504] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,524] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-12-25 12:57:58,553] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,576] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,580] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,622] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,623] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,624] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-12-25 12:57:58,624] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,624] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,625] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,632] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,633] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,634] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-12-25 12:57:58,634] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,634] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,634] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,638] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,639] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,640] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-12-25 12:57:58,640] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,640] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,640] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,648] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,650] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,650] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-12-25 12:57:58,651] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,651] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,651] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,656] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,657] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,658] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-12-25 12:57:58,658] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,658] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,659] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,664] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,665] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,665] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-12-25 12:57:58,666] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,667] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,667] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,680] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,682] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,685] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-12-25 12:57:58,685] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,685] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,686] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,694] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,695] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,697] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,697] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,698] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,700] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,704] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,729] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,730] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-12-25 12:57:58,730] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,730] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,731] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,741] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,746] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,747] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-12-25 12:57:58,747] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,748] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,749] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,757] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,758] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,758] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-12-25 12:57:58,759] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,762] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,762] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,774] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,775] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,778] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-12-25 12:57:58,779] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,780] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,780] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,788] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,789] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,791] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-12-25 12:57:58,791] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,791] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,792] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id ue_8SQUBS_2rDuQWfoEGdw. (state.change.logger)
[2025-12-25 12:57:58,805] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 12:57:58,812] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-12-25 12:57:58,818] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-12-25 12:57:58,822] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-12-25 12:57:58,823] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-12-25 12:57:58,831] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:58,841] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-25 12:57:58,850] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,851] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,852] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,851] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-13 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:58,853] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,855] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,855] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,855] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(ue_8SQUBS_2rDuQWfoEGdw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-12-25 12:57:58,857] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-12-25 12:57:58,856] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,857] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,858] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,858] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,858] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,859] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,859] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,859] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,859] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,859] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,860] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,861] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,861] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,861] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,861] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,861] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,861] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,862] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,862] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,862] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,862] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,862] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,862] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,863] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,864] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,867] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:58,868] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:58,879] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,885] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,892] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,901] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,902] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,905] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,906] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,907] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-15 in 14 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,914] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,916] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,916] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 15 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,917] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,921] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-45 in 14 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,922] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,925] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-14 in 9 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,927] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,928] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,929] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,928] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-10 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,929] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-42 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,929] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,936] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,934] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-23 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,942] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,936] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,944] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,948] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,950] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,950] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,955] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,965] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,963] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-28 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,974] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,976] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,976] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-25 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,977] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,979] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,979] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,983] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-6 in 3 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,980] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,991] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,993] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,994] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:58,994] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,997] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:58,998] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,003] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,006] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,008] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,006] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,014] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,017] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,017] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,022] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,022] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,024] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,024] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,025] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,024] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,026] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,026] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,027] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,027] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,030] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,027] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,032] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,032] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,033] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,034] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,037] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,037] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,037] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,040] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,039] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,044] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,046] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,044] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,047] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,048] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,047] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,049] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,049] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,049] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,050] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,050] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,053] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,053] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,053] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,053] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,053] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,054] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,050] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,054] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,057] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,058] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,062] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,054] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,064] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,064] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,064] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,064] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,064] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,064] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,064] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,065] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,064] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,065] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,065] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,065] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,066] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,067] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,065] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,070] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,069] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,075] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,079] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,087] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,088] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,087] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,088] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,088] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,092] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,093] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,093] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,093] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,094] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,094] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,102] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,103] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,105] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,107] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,103] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,112] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,109] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,116] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,119] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,120] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,117] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,121] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,120] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,123] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,123] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,125] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,125] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,126] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,125] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,130] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,128] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-12-25 12:57:59,133] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,134] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-12-25 12:57:59,134] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-12-25 12:57:59,328] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,332] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,333] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,336] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,338] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,338] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,339] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,340] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,342] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,346] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,347] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,347] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,348] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,350] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,351] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,351] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,351] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,351] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,354] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,355] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,355] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,355] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,356] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,356] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,356] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,356] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,360] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,362] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,364] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,367] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,368] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,368] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 12:57:59,369] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-12-25 12:57:59,369] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-12-25 13:11:09,758] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:11:09,798] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=2, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=345, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:11:09,833] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=345, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:11:09,841] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:11:09,917] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:11:09,924] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:11:09,975] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:17:49,923] INFO [RaftManager id=4] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=643, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:17:50,016] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=643, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:17:50,887] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:17:50,889] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:23:10,056] INFO [NodeToControllerChannelManager id=4 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:27:16,387] INFO [RaftManager id=4] Completed transition to Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=4, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,433] INFO [RaftManager id=4] Completed transition to Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=5, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,493] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:27:16,500] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1300, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=6, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:27:16,596] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:29:20,364] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:29:20,397] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:29:20,471] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:29:20,534] INFO [RaftManager id=4] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=6, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1536, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:20,546] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:29:20,614] INFO [RaftManager id=4] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:20,976] INFO [RaftManager id=4] Completed transition to Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:21,111] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1536, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:29:21,159] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:29:21,192] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 1536 (kafka.log.UnifiedLog)
[2025-12-25 13:29:21,279] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 1536 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 13:29:21,282] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 1536 (kafka.log.UnifiedLog$)
[2025-12-25 13:29:21,296] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 1536 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 13:29:21,297] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 13ms for segment recovery from offset 1536 (kafka.log.UnifiedLog$)
[2025-12-25 13:29:21,297] INFO [RaftManager id=4] Truncated to offset 1536 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 13:39:21,282] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:39:21,299] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:49:23,684] INFO [RaftManager id=4] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=10, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,800] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=11, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,837] INFO [RaftManager id=4] Completed transition to Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=11, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,947] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3679, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 13:49:23,954] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 3680 (kafka.log.UnifiedLog)
[2025-12-25 13:49:24,028] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3680 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 13:49:24,028] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3680 (kafka.log.UnifiedLog$)
[2025-12-25 13:49:24,029] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=1536, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000001536.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 13:49:24,066] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:49:24,069] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 13:49:24,066] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3680 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 13:49:24,072] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 13ms for snapshot load and 30ms for segment recovery from offset 3680 (kafka.log.UnifiedLog$)
[2025-12-25 13:49:24,072] INFO [RaftManager id=4] Truncated to offset 3680 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 13:59:23,992] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 13:59:24,012] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:19:04,180] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000007224-0000000012 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-12-25 14:19:04,629] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000007224-0000000012 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-12-25 14:31:39,634] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:39,674] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 9264 due to node 3 being disconnected (elapsed time since creation: 2021ms, elapsed time since send: 2004ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,305] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,329] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,366] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,466] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-12-25 14:31:40,471] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:40,507] INFO [RaftManager id=4] Completed transition to Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=8719, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:40,755] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-12-25 14:31:40,774] INFO [RaftManager id=4] Completed transition to Unattached(epoch=14, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:41,104] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=14, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=8719, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=14, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-12-25 14:31:41,157] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 8720 (kafka.log.UnifiedLog)
[2025-12-25 14:31:41,167] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-12-25 14:31:41,197] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 8720 with message format version 2 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,199] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 8720 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,204] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3680, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000003680.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 14:31:41,242] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 8720 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-12-25 14:31:41,243] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 32ms for segment recovery from offset 8720 (kafka.log.UnifiedLog$)
[2025-12-25 14:31:41,245] INFO [RaftManager id=4] Truncated to offset 8720 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-12-25 14:41:40,836] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:12:18,976] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2026-01-16 08:12:21,021] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2026-01-16 08:12:21,368] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2026-01-16 08:12:21,404] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:00,467] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2026-01-16 08:13:04,309] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2026-01-16 08:13:04,402] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:07,700] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:08,110] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:09,030] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2026-01-16 08:13:09,101] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2026-01-16 08:13:09,116] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2026-01-16 08:13:09,153] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:10,191] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:13:10,210] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2026-01-16 08:13:10,216] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2026-01-16 08:13:10,283] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2026-01-16 08:13:10,357] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2026-01-16 08:13:10,379] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:13:10,384] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:13:11,391] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1659) from null (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:13:11,415] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2026-01-16 08:13:11,416] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2026-01-16 08:13:11,847] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:11,936] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2026-01-16 08:13:11,995] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,063] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:12,115] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,230] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,318] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2026-01-16 08:13:12,343] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2026-01-16 08:13:12,362] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,372] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2026-01-16 08:13:12,408] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2026-01-16 08:13:12,465] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,567] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,696] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,808] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,815] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@460145306 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:13:12,847] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2026-01-16 08:13:12,858] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2026-01-16 08:13:12,914] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:12,938] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:12,948] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:13,016] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,026] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2026-01-16 08:13:13,017] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,132] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,247] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,349] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,425] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,432] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,438] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,439] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,453] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,465] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,458] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,486] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,495] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,570] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,589] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,598] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,629] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,631] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,679] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,688] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,696] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,788] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,827] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,831] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,894] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:13,972] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:13,973] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.24.0.8:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:14,015] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:14,022] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.24.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:14,022] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:14,159] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,358] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,408] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:15,455] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.24.0.9:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:13:15,491] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,595] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,695] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,800] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:15,938] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:16,074] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:16,176] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:16,281] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:16,384] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:16,485] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:16,592] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:16,699] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:16,821] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:16,933] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,052] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,218] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,366] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,469] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,482] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2026-01-16 08:13:17,583] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,629] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2026-01-16 08:13:17,638] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2026-01-16 08:13:17,652] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2026-01-16 08:13:17,691] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,696] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:17,758] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:17,794] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,882] INFO [RaftManager id=4] Completed transition to Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1659) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:13:17,897] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:17,968] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:17,996] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:18,003] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,013] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:18,099] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:18,114] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,130] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:18,223] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,354] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:18,357] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,398] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:18,493] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,601] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,713] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,761] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2026-01-16 08:13:18,812] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:18,818] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:18,851] INFO [BrokerLifecycleManager id=4] Incarnation 4kcfyaNLQCqI38PYl3cz_A of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:13:18,920] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,008] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2026-01-16 08:13:19,033] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,150] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,253] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,663] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,689] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=3, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:13:19,713] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2026-01-16 08:13:19,726] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2026-01-16 08:13:19,728] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2026-01-16 08:13:19,719] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,795] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:19,796] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:19,795] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:19,795] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:13:19,843] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:19,946] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,058] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,164] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,272] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,392] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,471] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1, metadata=Optional.empty)] for the first time for epoch 3 (org.apache.kafka.raft.FollowerState)
[2026-01-16 08:13:20,516] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,608] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,628] INFO [MetadataLoader id=4] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,652] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,666] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,685] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:20,719] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=0, epoch=3) with metadata.version 3.0-IV1. (kafka.server.metadata.BrokerMetadataPublisher)
[2026-01-16 08:13:20,741] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2026-01-16 08:13:20,771] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:13:20,773] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2026-01-16 08:13:20,804] INFO Loaded 0 logs in 43ms (kafka.log.LogManager)
[2026-01-16 08:13:20,809] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2026-01-16 08:13:20,816] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2026-01-16 08:13:20,874] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2026-01-16 08:13:23,845] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2026-01-16 08:13:24,581] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2026-01-16 08:13:24,588] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2026-01-16 08:13:24,623] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:13:24,683] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:13:24,721] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2026-01-16 08:13:24,735] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2026-01-16 08:13:24,736] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2026-01-16 08:13:24,877] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2026-01-16 08:13:25,301] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:13:25,326] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2026-01-16 08:13:25,328] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2026-01-16 08:13:25,339] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2026-01-16 08:13:25,872] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2026-01-16 08:13:25,889] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2026-01-16 08:13:25,920] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2026-01-16 08:13:25,980] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:13:26,790] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:13:26,880] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2026-01-16 08:13:27,099] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2026-01-16 08:13:27,130] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2026-01-16 08:13:27,157] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2026-01-16 08:13:27,304] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2026-01-16 08:13:27,607] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2026-01-16 08:13:27,940] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2026-01-16 08:13:27,959] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2026-01-16 08:13:27,961] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2026-01-16 08:13:27,976] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2026-01-16 08:13:28,041] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2026-01-16 08:13:28,090] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2026-01-16 08:13:28,110] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2026-01-16 08:13:28,133] INFO Kafka startTimeMs: 1768551208060 (org.apache.kafka.common.utils.AppInfoParser)
[2026-01-16 08:13:28,261] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2026-01-16 08:14:38,050] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:14:39,533] INFO [RaftManager id=4] Completed transition to Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=3, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=158, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:14:40,195] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=158, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=4, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:14:40,253] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:14:44,667] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:14:45,782] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:14:46,319] INFO [Broker id=4] Creating new partition _schemas-0 with topic id UjN16IrSRlipZQeCwkGrdQ. (state.change.logger)
[2026-01-16 08:14:49,122] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:14:49,336] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2026-01-16 08:14:49,581] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2026-01-16 08:14:49,623] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:14:50,214] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:14:51,522] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:14:51,687] INFO [Broker id=4] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2026-01-16 08:14:55,236] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:14:55,360] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(UjN16IrSRlipZQeCwkGrdQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:14:55,420] INFO [Broker id=4] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2026-01-16 08:14:55,460] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:14:55,500] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:14:56,524] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2026-01-16 08:15:05,539] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2026-01-16 08:15:05,585] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-46, __consumer_offsets-10, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-24, __consumer_offsets-19, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:15:05,602] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:05,934] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:05,964] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:05,974] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2026-01-16 08:15:05,980] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:06,150] INFO [Broker id=4] Leader __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:06,350] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:06,403] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:06,419] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:06,609] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2026-01-16 08:15:06,637] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:06,660] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:06,693] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:06,803] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:06,845] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:06,847] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2026-01-16 08:15:06,848] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:06,853] INFO [Broker id=4] Leader __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:06,889] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:06,983] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:07,040] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:07,055] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2026-01-16 08:15:07,057] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:07,060] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:07,133] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:07,273] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:07,312] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:07,327] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2026-01-16 08:15:07,330] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:07,333] INFO [Broker id=4] Leader __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:07,365] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:07,466] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:07,526] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:07,526] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2026-01-16 08:15:07,532] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:07,553] INFO [Broker id=4] Leader __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:07,618] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:07,839] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:07,860] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:07,912] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2026-01-16 08:15:07,922] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:07,934] INFO [Broker id=4] Leader __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,057] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,167] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,220] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,225] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2026-01-16 08:15:08,226] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,228] INFO [Broker id=4] Leader __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,253] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,354] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,371] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,376] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2026-01-16 08:15:08,381] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,382] INFO [Broker id=4] Leader __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,416] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,483] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,505] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,511] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2026-01-16 08:15:08,516] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,518] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,547] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,562] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,568] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,569] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2026-01-16 08:15:08,573] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,574] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,595] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,638] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,645] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,646] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2026-01-16 08:15:08,647] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,648] INFO [Broker id=4] Leader __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:08,661] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:08,949] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:08,979] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:08,989] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2026-01-16 08:15:08,990] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:08,993] INFO [Broker id=4] Leader __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:09,082] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:09,175] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:09,197] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:09,204] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2026-01-16 08:15:09,206] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:09,207] INFO [Broker id=4] Leader __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:09,225] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:09,322] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:09,373] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:09,485] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2026-01-16 08:15:09,507] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:09,542] INFO [Broker id=4] Leader __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:09,574] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:09,619] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:09,633] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:09,642] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2026-01-16 08:15:09,650] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:09,652] INFO [Broker id=4] Leader __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:09,694] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:09,833] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:09,856] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:09,875] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2026-01-16 08:15:09,877] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:09,882] INFO [Broker id=4] Leader __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2026-01-16 08:15:09,938] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:15:09,939] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,056] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,068] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,077] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2026-01-16 08:15:10,088] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,094] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,094] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,158] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,167] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,173] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2026-01-16 08:15:10,173] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,176] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,180] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,272] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,280] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,282] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2026-01-16 08:15:10,283] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,284] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,284] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,311] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,318] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,319] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2026-01-16 08:15:10,320] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,323] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,324] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,361] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,368] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,369] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2026-01-16 08:15:10,370] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,373] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,378] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,411] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,423] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,425] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2026-01-16 08:15:10,426] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,427] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,431] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,597] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,640] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,693] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2026-01-16 08:15:10,704] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,717] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,726] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:10,847] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:10,884] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:10,896] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2026-01-16 08:15:10,911] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:10,923] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:10,930] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,059] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,068] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,069] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2026-01-16 08:15:11,074] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,083] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,084] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,215] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,239] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,245] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2026-01-16 08:15:11,257] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,257] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,257] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,316] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,328] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,330] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2026-01-16 08:15:11,333] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,335] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,339] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,393] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,418] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,421] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2026-01-16 08:15:11,424] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,428] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,438] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,506] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,524] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,535] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2026-01-16 08:15:11,535] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,543] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,548] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,635] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,659] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,693] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2026-01-16 08:15:11,699] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,701] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,719] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,795] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,801] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,803] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2026-01-16 08:15:11,803] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,807] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,811] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:11,875] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:11,923] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:11,931] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2026-01-16 08:15:11,932] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:11,935] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:11,939] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:12,202] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:12,242] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:12,243] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2026-01-16 08:15:12,244] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:12,259] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:12,262] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:12,690] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:12,743] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:12,751] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2026-01-16 08:15:12,758] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:12,760] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:12,761] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:12,970] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,022] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,040] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2026-01-16 08:15:13,049] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,054] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,086] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,116] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,118] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,119] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2026-01-16 08:15:13,120] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,121] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,122] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,149] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,164] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,169] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2026-01-16 08:15:13,170] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,170] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,171] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,230] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,255] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,275] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2026-01-16 08:15:13,284] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,296] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,296] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,408] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,428] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,430] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2026-01-16 08:15:13,432] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,438] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,440] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,542] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,578] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,582] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2026-01-16 08:15:13,591] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,593] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,600] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,677] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,685] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,688] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,688] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,688] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,688] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,704] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,707] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,710] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2026-01-16 08:15:13,711] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,711] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,711] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,722] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,723] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,725] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2026-01-16 08:15:13,725] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,726] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,730] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,769] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,782] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,783] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2026-01-16 08:15:13,785] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,786] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,786] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,821] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,822] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,826] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2026-01-16 08:15:13,828] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,828] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,830] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:13,908] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:13,946] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:13,953] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2026-01-16 08:15:13,957] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:13,963] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:13,971] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:14,084] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:14,131] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:14,135] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2026-01-16 08:15:14,138] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:14,144] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:14,151] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:14,236] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:14,267] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:14,274] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2026-01-16 08:15:14,279] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:14,282] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:14,286] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id p0HtGUHKReOSVWe-VAlPVA. (state.change.logger)
[2026-01-16 08:15:14,384] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:15:14,414] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2026-01-16 08:15:14,437] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2026-01-16 08:15:14,441] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2026-01-16 08:15:14,456] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:15:14,499] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-4) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:15:14,509] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2026-01-16 08:15:14,572] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,578] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,583] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,585] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-45 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:15:14,594] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,603] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,605] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,614] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,611] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-16 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:15:14,622] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,628] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,628] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2026-01-16 08:15:14,630] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,631] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,639] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,642] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,658] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,679] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,682] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,684] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,695] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,722] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,781] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,787] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,791] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,791] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,791] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,791] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,791] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,791] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,791] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,792] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,798] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,743] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,814] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,815] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,820] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,840] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,881] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,916] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,917] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,921] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,930] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,940] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,945] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,946] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,954] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,958] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,960] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,961] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,963] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,964] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,982] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,984] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:14,987] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:14,994] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,000] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,002] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,008] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,010] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,012] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,022] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,023] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,024] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,024] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,039] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,040] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,041] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,046] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:15:15,047] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:15:15,114] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,131] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,150] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,158] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,161] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,162] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,164] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,171] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,177] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,178] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,180] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,181] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,182] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,183] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,183] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,185] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,185] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,187] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,194] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,202] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,206] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,206] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,211] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,211] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,213] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,226] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,227] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,228] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,236] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,247] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,250] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,256] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,257] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,265] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,284] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,293] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,311] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,316] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,325] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,325] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,328] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,329] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,331] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,331] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,333] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,335] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,337] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,362] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,364] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,365] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,366] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,370] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,371] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,372] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,372] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,372] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,373] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,383] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,384] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,384] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,385] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,385] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,388] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,390] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,391] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,396] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,401] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,401] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,402] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,402] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,403] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,407] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,411] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,391] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-15 in 221 milliseconds for epoch 0, of which 55 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,423] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 263 milliseconds for epoch 0, of which 262 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,429] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-14 in 266 milliseconds for epoch 0, of which 266 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,420] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,433] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,433] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,434] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,442] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,437] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 260 milliseconds for epoch 0, of which 256 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,446] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,450] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,455] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,455] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,456] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,453] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-10 in 273 milliseconds for epoch 0, of which 265 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,458] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-42 in 276 milliseconds for epoch 0, of which 276 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,464] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-23 in 281 milliseconds for epoch 0, of which 281 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,473] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-24 in 288 milliseconds for epoch 0, of which 287 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,458] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,476] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,476] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,484] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,485] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,488] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,488] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,488] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,498] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,500] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,502] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,479] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-19 in 285 milliseconds for epoch 0, of which 282 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,506] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,512] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,522] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,532] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,534] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,535] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,511] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 305 milliseconds for epoch 0, of which 305 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,541] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 331 milliseconds for epoch 0, of which 331 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,542] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-7 in 330 milliseconds for epoch 0, of which 330 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,546] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-40 in 319 milliseconds for epoch 0, of which 319 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,557] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-5 in 321 milliseconds for epoch 0, of which 315 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,566] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-38 in 316 milliseconds for epoch 0, of which 316 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,574] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-33 in 317 milliseconds for epoch 0, of which 310 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,576] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-2 in 310 milliseconds for epoch 0, of which 310 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,574] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2026-01-16 08:15:15,619] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,621] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,621] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,623] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,629] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,631] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,637] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,639] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,642] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,644] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,646] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,649] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,653] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,653] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,656] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,658] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,645] INFO [GroupCoordinator 4]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-6a020778-7b37-403a-bbc3-6c4e58ef51d0 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:15,663] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,668] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,679] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,679] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,681] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,686] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,698] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,703] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,704] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,706] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,707] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,714] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,718] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,721] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,726] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,728] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,728] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:15:15,791] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-6a020778-7b37-403a-bbc3-6c4e58ef51d0 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:19,018] INFO [GroupCoordinator 4]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:19,949] INFO [GroupCoordinator 4]: Assignment received from leader sr-1-6a020778-7b37-403a-bbc3-6c4e58ef51d0 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:15:53,380] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:15:53,522] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:15:53,778] INFO [RaftManager id=4] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=353, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:15:54,111] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:15:55,180] INFO [RaftManager id=4] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:15:55,381] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=353, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:15:55,446] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:19:01,709] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:01,749] INFO [RaftManager id=4] Completed transition to Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=12, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=715, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:02,274] INFO [RaftManager id=4] Completed transition to Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:02,312] INFO [RaftManager id=4] Completed transition to Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:02,599] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=715, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:02,629] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:19:39,518] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:40,130] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 801 due to node 1 being disconnected (elapsed time since creation: 2018ms, elapsed time since send: 2010ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:41,424] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:41,475] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 207 due to node 1 being disconnected (elapsed time since creation: 4603ms, elapsed time since send: 4603ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:41,590] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:19:45,002] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:45,764] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 804 due to node 1 being disconnected (elapsed time since creation: 4176ms, elapsed time since send: 2653ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:45,812] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:45,841] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 805 due to node 3 being disconnected (elapsed time since creation: 2173ms, elapsed time since send: 2106ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:45,922] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:19:47,547] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:48,092] INFO [RaftManager id=4] Completed transition to Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:51,032] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:19:52,748] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:52,919] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 807 due to node 3 being disconnected (elapsed time since creation: 4520ms, elapsed time since send: 3777ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:19:54,042] INFO [RaftManager id=4] Completed transition to Unattached(epoch=26, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:19:56,332] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:19:59,880] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:02,909] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 813 due to node 3 being disconnected (elapsed time since creation: 2288ms, elapsed time since send: 2137ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:03,885] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:20:05,434] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:05,672] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 816 due to node 2 being disconnected (elapsed time since creation: 5007ms, elapsed time since send: 2136ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:08,752] INFO [RaftManager id=4] Completed transition to Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=26, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:20:09,030] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:09,065] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 819 due to node 3 being disconnected (elapsed time since creation: 4650ms, elapsed time since send: 3898ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:09,308] INFO [RaftManager id=4] Completed transition to Unattached(epoch=32, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:20:09,421] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:20:15,950] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:16,068] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 826 due to node 1 being disconnected (elapsed time since creation: 3266ms, elapsed time since send: 2677ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:16,696] INFO [RaftManager id=4] Completed transition to Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=32, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:20:17,648] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:20:20,700] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:20,976] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 828 due to node 2 being disconnected (elapsed time since creation: 2333ms, elapsed time since send: 2211ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:25,741] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:20:28,953] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:29,159] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 833 due to node 3 being disconnected (elapsed time since creation: 4044ms, elapsed time since send: 2269ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:30,566] INFO [RaftManager id=4] Completed transition to Unattached(epoch=43, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:20:34,087] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:34,436] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 839 due to node 1 being disconnected (elapsed time since creation: 2027ms, elapsed time since send: 2005ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:37,132] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:37,501] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:20:37,669] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 840 due to node 3 being disconnected (elapsed time since creation: 2771ms, elapsed time since send: 2176ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:20:41,188] INFO [RaftManager id=4] Completed transition to Unattached(epoch=46, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=43, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:20:48,099] INFO [RaftManager id=4] Completed transition to Unattached(epoch=47, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=46, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:20:52,500] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:21:00,248] INFO [RaftManager id=4] Completed transition to Unattached(epoch=51, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=47, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:21:06,036] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:06,986] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 849 due to node 3 being disconnected (elapsed time since creation: 2951ms, elapsed time since send: 2532ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:07,285] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:07,300] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 850 due to node 1 being disconnected (elapsed time since creation: 3518ms, elapsed time since send: 3267ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:15,101] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:21:22,956] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:23,904] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 855 due to node 1 being disconnected (elapsed time since creation: 4696ms, elapsed time since send: 4202ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:31,549] INFO [RaftManager id=4] Completed transition to Unattached(epoch=63, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=51, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:21:32,025] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:21:43,824] INFO [RaftManager id=4] Completed transition to Unattached(epoch=70, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=63, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:21:50,890] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:21:50,992] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:51,763] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 864 due to node 1 being disconnected (elapsed time since creation: 3933ms, elapsed time since send: 3045ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:54,641] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:55,034] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 865 due to node 3 being disconnected (elapsed time since creation: 3978ms, elapsed time since send: 2486ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:21:58,528] INFO [RaftManager id=4] Completed transition to Unattached(epoch=78, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=70, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:05,822] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:22:06,181] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:06,990] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 873 due to node 2 being disconnected (elapsed time since creation: 2152ms, elapsed time since send: 2013ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:10,209] INFO [RaftManager id=4] Completed transition to Unattached(epoch=81, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=78, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:16,562] INFO [RaftManager id=4] Completed transition to Unattached(epoch=86, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=81, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:20,536] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:21,270] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 879 due to node 1 being disconnected (elapsed time since creation: 2689ms, elapsed time since send: 2480ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:21,318] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:22:26,359] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:30,145] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 882 due to node 3 being disconnected (elapsed time since creation: 3745ms, elapsed time since send: 2446ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:36,987] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:22:37,885] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:38,066] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 888 due to node 1 being disconnected (elapsed time since creation: 3825ms, elapsed time since send: 3824ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:38,218] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:38,337] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 887 due to node 3 being disconnected (elapsed time since creation: 5938ms, elapsed time since send: 4234ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:47,052] INFO [RaftManager id=4] Completed transition to Unattached(epoch=95, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=86, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:22:50,989] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:51,594] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 895 due to node 1 being disconnected (elapsed time since creation: 2157ms, elapsed time since send: 2059ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:22:51,623] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:22:56,252] INFO [RaftManager id=4] Completed transition to Unattached(epoch=103, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=95, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:02,711] INFO [RaftManager id=4] Completed transition to Unattached(epoch=106, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=103, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:03,314] INFO [RaftManager id=4] Completed transition to Unattached(epoch=107, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=106, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:06,116] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:23:06,923] INFO [RaftManager id=4] Completed transition to Unattached(epoch=108, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=107, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:08,095] INFO [RaftManager id=4] Completed transition to Unattached(epoch=109, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=108, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:10,136] INFO [RaftManager id=4] Completed transition to Unattached(epoch=110, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=109, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:16,587] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:17,839] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 908 due to node 1 being disconnected (elapsed time since creation: 2202ms, elapsed time since send: 2026ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:20,625] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:23:21,497] INFO [RaftManager id=4] Completed transition to Unattached(epoch=115, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=110, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:25,797] INFO [RaftManager id=4] Completed transition to Unattached(epoch=116, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=115, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:28,300] INFO [RaftManager id=4] Completed transition to Unattached(epoch=119, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=116, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:28,775] INFO [RaftManager id=4] Completed transition to Unattached(epoch=120, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=119, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:29,333] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=120, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=120, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:30,440] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:23:32,023] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:32,122] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:23:32,168] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:23:33,171] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:33,242] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:23:33,285] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:23:36,747] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:37,101] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 918 due to node 2 being disconnected (elapsed time since creation: 2666ms, elapsed time since send: 2546ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:39,203] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:23:41,800] INFO [RaftManager id=4] Completed transition to Unattached(epoch=125, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=120, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:43,406] INFO [RaftManager id=4] Completed transition to Unattached(epoch=127, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=125, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:43,602] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=128, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=127, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:46,134] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:46,348] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 923 due to node 2 being disconnected (elapsed time since creation: 2142ms, elapsed time since send: 2015ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:23:46,966] INFO [RaftManager id=4] Completed transition to Unattached(epoch=129, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=128, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:23:59,383] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:00,453] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 925 due to node 2 being disconnected (elapsed time since creation: 9800ms, elapsed time since send: 8277ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:00,601] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:03,616] INFO [RaftManager id=4] Completed transition to Unattached(epoch=133, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=129, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:24:04,636] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:24:08,169] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:08,216] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=133, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=133, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:24:14,715] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 788 (kafka.log.UnifiedLog)
[2026-01-16 08:24:17,743] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:17,872] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:17,988] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:18,171] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:18,296] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:19,791] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:20,330] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:23,625] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:24,130] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:24,131] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:24:29,964] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 788 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:24:30,302] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 788 (kafka.log.UnifiedLog$)
[2026-01-16 08:24:33,629] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 788 with 0 producer ids in 313 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 08:24:33,775] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 87ms for snapshot load and 3314ms for segment recovery from offset 788 (kafka.log.UnifiedLog$)
[2026-01-16 08:24:33,788] INFO [RaftManager id=4] Truncated to offset 788 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:24:37,847] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=147, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=133, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=780, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:24:48,473] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:24:49,022] INFO [NodeToControllerChannelManager id=4 name=forwarding] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:49,350] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:24:49,437] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:24:51,289] INFO [RaftManager id=4] Completed transition to Unattached(epoch=148, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=147, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:24:55,300] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:25:00,880] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:00,943] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=155, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=148, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:01,034] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:25:01,523] INFO [RaftManager id=4] Completed transition to Unattached(epoch=158, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=155, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:08,123] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:08,969] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=158, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=158, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:10,372] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:10,547] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:25:13,618] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:13,786] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:14,043] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:14,088] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:14,164] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:17,566] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=166, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=158, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:19,298] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:19,386] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 242 due to node 2 being disconnected (elapsed time since creation: 4650ms, elapsed time since send: 4527ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:19,388] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:19,397] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:25:19,964] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:19,965] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 942 due to node 2 being disconnected (elapsed time since creation: 2099ms, elapsed time since send: 2047ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:20,249] INFO [RaftManager id=4] Completed transition to Unattached(epoch=169, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=166, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:24,166] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:25:25,610] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=170, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=169, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:25,763] INFO [RaftManager id=4] Completed transition to Unattached(epoch=171, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=170, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:29,527] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:29,536] INFO [RaftManager id=4] Completed transition to Unattached(epoch=172, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=171, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:33,460] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=174, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=172, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:33,549] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:33,573] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:25:38,170] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:39,012] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 962 due to node 1 being disconnected (elapsed time since creation: 2242ms, elapsed time since send: 2077ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:41,325] INFO [RaftManager id=4] Completed transition to Unattached(epoch=179, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=174, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:42,140] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:43,272] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=179, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=808, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=179, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:43,371] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:45,038] INFO [Broker id=4] Transitioning 26 partition(s) to local leaders. (state.change.logger)
[2026-01-16 08:25:49,972] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-29, __consumer_offsets-39, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:25:50,362] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:50,374] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:50,377] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:50,388] INFO [Broker id=4] Leader __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 1, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:25:52,505] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:52,528] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:52,569] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:52,578] INFO [Broker id=4] Leader __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 1, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:25:52,658] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:52,681] INFO [Broker id=4] Leader __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 1, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:25:52,843] INFO [Broker id=4] Leader __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 1, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:25:53,048] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:53,103] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:53,406] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:53,408] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:53,415] INFO [Broker id=4] Leader __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 1, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:25:53,434] INFO [RaftManager id=4] Completed transition to Unattached(epoch=180, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=179, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=892, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:53,596] INFO [Broker id=4] Leader __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 1, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:25:53,612] INFO [RaftManager id=4] Completed transition to Unattached(epoch=183, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=180, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:53,637] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:53,638] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:53,642] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:53,644] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 1, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:53,649] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 1, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:25:53,709] INFO [Broker id=4] Leader __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 1, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:25:53,783] INFO [Broker id=4] Leader __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 1 from offset 0 with partition epoch 1, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2026-01-16 08:25:53,911] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:53,913] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=1, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:25:53,916] INFO [Broker id=4] Transitioning 25 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:25:53,921] INFO [RaftManager id=4] Completed transition to Unattached(epoch=184, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=183, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:54,827] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:25:55,032] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:55,122] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:55,185] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:55,233] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:55,294] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:55,319] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:55,170] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-21 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:25:55,261] INFO [RaftManager id=4] Completed transition to Unattached(epoch=185, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=184, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:55,373] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:55,427] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:55,460] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:55,469] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:55,461] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-21 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:25:55,470] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-20 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:25:55,470] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-20 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:25:55,473] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-31 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:25:55,486] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-31 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:25:55,489] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-8 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:25:55,489] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-8 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:25:55,499] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-3 has an older epoch (0) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:25:55,499] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-3 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:25:55,486] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:55,519] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:55,949] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:55,979] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:56,057] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:56,005] INFO [RaftManager id=4] Completed transition to Unattached(epoch=186, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=185, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:56,087] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:56,108] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:56,117] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:56,151] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:56,250] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:56,268] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:56,279] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:56,280] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:25:56,304] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:56,310] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:25:56,665] INFO [RaftManager id=4] Completed transition to Unattached(epoch=187, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=186, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:58,007] INFO [RaftManager id=4] Completed transition to Unattached(epoch=188, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=187, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:58,054] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-31, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-3, __consumer_offsets-20) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:25:58,222] INFO [Broker id=4] Stopped fetchers as part of become-follower for 7 partitions (state.change.logger)
[2026-01-16 08:25:58,524] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=189, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=892, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=188, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:25:58,539] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:25:59,919] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:00,935] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:01,447] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:26:02,617] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-31 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-43 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-12 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-8 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-21 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-3 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-20 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:26:02,680] INFO [Broker id=4] Started fetchers as part of become-follower for 7 partitions (state.change.logger)
[2026-01-16 08:26:02,753] INFO [ReplicaFetcherThread-0-6]: Shutting down (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:26:02,893] INFO [ReplicaFetcherThread-0-6]: Stopped (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:26:02,908] INFO [ReplicaFetcherThread-0-6]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:26:03,495] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:26:03,542] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:26:03,555] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:26:03,630] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:26:03,699] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:26:03,751] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:26:03,778] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:26:03,785] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:26:03,809] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:26:03,813] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:26:03,816] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:26:03,821] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:26:03,821] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:26:03,821] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:26:04,573] INFO [NodeToControllerChannelManager id=4 name=alter-partition] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:26:04,619] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:26:05,398] INFO [Partition __consumer_offsets-15 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,112] INFO [Partition __consumer_offsets-48 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,124] INFO [Partition __consumer_offsets-46 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,125] INFO [Partition __consumer_offsets-9 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,126] INFO [Partition __consumer_offsets-42 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,134] INFO [Partition __consumer_offsets-23 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,138] INFO [Partition __consumer_offsets-19 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,170] INFO [Partition __consumer_offsets-17 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,171] INFO [Partition __consumer_offsets-30 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,201] INFO [Partition __consumer_offsets-28 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,228] INFO [Partition __consumer_offsets-26 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,230] INFO [Partition __consumer_offsets-7 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,242] INFO [Partition __consumer_offsets-40 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,248] INFO [Partition __consumer_offsets-5 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,248] INFO [Partition __consumer_offsets-38 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,249] INFO [Partition __consumer_offsets-1 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,250] INFO [Partition __consumer_offsets-45 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,251] INFO [Partition __consumer_offsets-14 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,254] INFO [Partition __consumer_offsets-10 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,260] INFO [Partition __consumer_offsets-24 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,270] INFO [Partition __consumer_offsets-29 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,289] INFO [Partition __consumer_offsets-39 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,294] INFO [Partition __consumer_offsets-37 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,315] INFO [Partition __consumer_offsets-35 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,325] INFO [Partition __consumer_offsets-33 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:06,367] INFO [Partition __consumer_offsets-2 broker=4] ISR updated to 4,5,6  and version updated to 2 (kafka.cluster.Partition)
[2026-01-16 08:26:08,455] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 45 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,514] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,574] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 28 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,580] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,593] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 9 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,601] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,635] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 26 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,636] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,641] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,646] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,646] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 37 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,646] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,646] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 35 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,646] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,657] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 1 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,660] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,661] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 17 in epoch 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,662] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,670] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,673] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,673] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,673] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,674] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,674] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,674] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,694] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,700] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,707] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,714] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,715] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,724] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,739] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,762] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,758] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-45 in 189 milliseconds for epoch 1, of which 27 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,799] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-28 in 206 milliseconds for epoch 1, of which 200 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,769] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,803] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,806] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,808] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,808] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,810] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,810] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-9 in 175 milliseconds for epoch 1, of which 174 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,816] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-26 in 176 milliseconds for epoch 1, of which 176 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,815] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,822] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,823] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,823] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 177 milliseconds for epoch 1, of which 177 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,823] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-37 in 177 milliseconds for epoch 1, of which 177 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,823] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,831] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,833] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,828] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-35 in 170 milliseconds for epoch 1, of which 170 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,836] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,843] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,851] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,845] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-1 in 184 milliseconds for epoch 1, of which 182 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,854] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-17 in 184 milliseconds for epoch 1, of which 184 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,852] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,867] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,892] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,904] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,895] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,906] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,906] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,908] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,908] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,909] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,910] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:08,912] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:08,912] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,027] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:10,030] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,028] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,038] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,037] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:10,039] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,039] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:10,040] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,039] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,057] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,057] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:10,077] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,088] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:10,114] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,077] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,123] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,123] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,124] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,124] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,124] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,124] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,124] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,124] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,124] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,124] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,136] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,222] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:10,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:11,696] INFO [Broker id=4] Transitioning 26 partition(s) to local leaders. (state.change.logger)
[2026-01-16 08:26:11,895] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-29, __consumer_offsets-39, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:26:12,197] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:12,869] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:12,939] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:12,951] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:12,970] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:12,979] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:12,990] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,010] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,067] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,093] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,097] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,136] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,150] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,185] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,227] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,260] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,286] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,322] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,368] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,377] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,392] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 1, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,431] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,450] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,466] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,477] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,735] INFO [Partition __consumer_offsets-15 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:13,815] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5], partitionEpoch=3, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:13,951] INFO [Broker id=4] Transitioning 25 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:26:13,960] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:13,964] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:13,964] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:13,964] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:13,965] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:13,968] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:13,969] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:13,970] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:13,978] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:13,997] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,005] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,006] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,006] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,009] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,011] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,014] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,014] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,015] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,015] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,015] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,015] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,015] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,016] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:14,016] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,016] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:14,092] INFO [Partition __consumer_offsets-48 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,097] INFO [Partition __consumer_offsets-46 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,134] INFO [Partition __consumer_offsets-9 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,172] INFO [Partition __consumer_offsets-42 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,176] INFO [Partition __consumer_offsets-23 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,177] INFO [Partition __consumer_offsets-19 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,184] INFO [Partition __consumer_offsets-17 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,187] INFO [Partition __consumer_offsets-30 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,187] INFO [Partition __consumer_offsets-28 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,222] INFO [Partition __consumer_offsets-26 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,228] INFO [Partition __consumer_offsets-7 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,230] INFO [Partition __consumer_offsets-40 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,234] INFO [Partition __consumer_offsets-5 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,236] INFO [Partition __consumer_offsets-38 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,211] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,252] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,253] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,262] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,262] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,264] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,267] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,267] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,271] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,253] INFO [Partition __consumer_offsets-1 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,272] INFO [Partition __consumer_offsets-45 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,273] INFO [Partition __consumer_offsets-14 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,278] INFO [Partition __consumer_offsets-10 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,290] INFO [Partition __consumer_offsets-24 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,291] INFO [Partition __consumer_offsets-29 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,292] INFO [Partition __consumer_offsets-39 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,297] INFO [Partition __consumer_offsets-37 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,299] INFO [Partition __consumer_offsets-35 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:14,272] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,309] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,310] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,310] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,310] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,310] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,312] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,341] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,344] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,264] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,345] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,345] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,382] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,394] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,394] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,394] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,394] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,394] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,394] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,394] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,394] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,398] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,399] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,399] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,399] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,399] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,399] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,363] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,400] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,400] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,400] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,401] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,401] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,401] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,407] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,407] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,408] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,408] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,408] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,408] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,409] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,410] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,411] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,411] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,410] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:14,412] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,412] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,414] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,415] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,420] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,420] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,421] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,422] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,424] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,425] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,426] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,427] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,428] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,428] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,428] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,429] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,429] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:14,725] INFO [Broker id=4] Transitioning 24 partition(s) to local leaders. (state.change.logger)
[2026-01-16 08:26:14,769] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-10, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-24, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-39, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-35, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:26:14,840] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:14,853] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:14,952] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,017] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,091] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,114] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,197] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,252] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,269] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,285] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,287] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,321] INFO [Partition __consumer_offsets-33 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:15,338] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,381] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 1, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,648] INFO [Partition __consumer_offsets-2 broker=4] ISR updated to 4,5,6  and version updated to 4 (kafka.cluster.Partition)
[2026-01-16 08:26:15,685] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,709] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,738] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,781] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,823] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,852] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,888] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:15,974] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:16,042] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:16,062] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:16,083] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=4, leaderEpoch=1, isr=[4, 5, 6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 1. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:16,089] INFO [Broker id=4] Transitioning 25 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:26:16,093] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,100] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,154] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,161] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:16,167] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,169] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:16,180] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,182] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,182] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,183] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,184] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,185] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:16,438] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:16,464] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,473] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,498] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:16,506] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,520] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,548] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:16,569] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,596] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,624] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,625] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5, 4, 6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:26:16,626] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,629] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5, 4, 6], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:26:16,737] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,753] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,769] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,781] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,795] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,809] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,809] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,810] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,810] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,810] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,810] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,814] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,815] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,828] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,828] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,828] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,789] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,838] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,846] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,846] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,846] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,847] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,847] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,847] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,846] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,848] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,848] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,849] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,848] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,850] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,851] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,851] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,853] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,860] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,863] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,873] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,875] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,882] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,882] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,882] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,883] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,883] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,883] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,883] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,883] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,883] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,883] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,883] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,884] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,884] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,884] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,884] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,884] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,884] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,853] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,885] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,886] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,894] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:26:16,896] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,895] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,899] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,923] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,923] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,923] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,924] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,925] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,925] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,926] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,926] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,929] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,930] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:16,930] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:26:17,232] INFO [Broker id=4] Transitioning 2 partition(s) to local leaders. (state.change.logger)
[2026-01-16 08:26:17,294] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:26:17,346] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:26:17,422] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=0, isr=[4, 5, 6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2026-01-16 08:28:04,169] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:28:04,351] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 1242 due to node 3 being disconnected (elapsed time since creation: 2075ms, elapsed time since send: 2075ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:28:05,455] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:28:05,579] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:28:06,031] INFO [RaftManager id=4] Completed transition to Unattached(epoch=190, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=189, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:28:06,065] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:28:06,248] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=191, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1282, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=190, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:28:06,275] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:28:06,874] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:28:06,917] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:28:06,978] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:28:07,121] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:28:07,177] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:28:07,231] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:01,068] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:01,197] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:29:01,377] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 1346 due to node 1 being disconnected (elapsed time since creation: 2010ms, elapsed time since send: 2009ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:01,527] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:01,663] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:02,304] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:02,335] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:02,409] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:02,574] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:02,577] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:02,629] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:02,666] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:02,671] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:02,721] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:02,761] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:02,767] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:02,822] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:02,843] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:02,847] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:02,904] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:03,321] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:03,419] INFO [RaftManager id=4] Completed transition to Unattached(epoch=193, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=191, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1378, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:29:03,845] INFO [RaftManager id=4] Completed transition to Unattached(epoch=194, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=193, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:29:03,860] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:03,884] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 1348 due to node 3 being disconnected (elapsed time since creation: 2038ms, elapsed time since send: 2038ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:29:05,744] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:29:05,777] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=194, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1378, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=194, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:08,928] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:10,454] INFO [RaftManager id=4] Completed transition to Unattached(epoch=195, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=194, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1704, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:11,182] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 441 due to node 1 being disconnected (elapsed time since creation: 4534ms, elapsed time since send: 4534ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:12,149] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:13,003] INFO [RaftManager id=4] Completed transition to Unattached(epoch=197, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=195, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:15,903] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:16,035] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 1708 due to node 3 being disconnected (elapsed time since creation: 2267ms, elapsed time since send: 2151ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:17,283] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:22,232] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:23,294] INFO [RaftManager id=4] Completed transition to Unattached(epoch=203, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=197, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:28,463] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:35,500] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:36,102] INFO [RaftManager id=4] Completed transition to Unattached(epoch=207, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=203, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:41,023] INFO [RaftManager id=4] Completed transition to Unattached(epoch=211, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=207, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:42,886] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:44,862] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=211, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1704, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=211, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:44,925] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:32:47,459] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:47,526] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 1721 due to node 1 being disconnected (elapsed time since creation: 2244ms, elapsed time since send: 2063ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:48,445] INFO [RaftManager id=4] Completed transition to Unattached(epoch=214, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=211, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1704, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:48,946] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:32:49,217] INFO [RaftManager id=4] Completed transition to Unattached(epoch=215, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=214, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:50,655] INFO [RaftManager id=4] Completed transition to Unattached(epoch=216, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=215, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:51,054] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:32:55,809] INFO [RaftManager id=4] Completed transition to Unattached(epoch=218, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=216, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:57,259] INFO [RaftManager id=4] Completed transition to Unattached(epoch=220, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=218, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:32:58,732] INFO [RaftManager id=4] Completed transition to Unattached(epoch=221, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=220, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:01,720] INFO [RaftManager id=4] Completed transition to Unattached(epoch=222, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=221, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:01,972] INFO [RaftManager id=4] Completed transition to Unattached(epoch=224, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=222, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:02,021] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:33:06,308] INFO [RaftManager id=4] Completed transition to Unattached(epoch=225, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=224, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:09,858] INFO [RaftManager id=4] Completed transition to Unattached(epoch=226, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=225, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:10,604] INFO [RaftManager id=4] Completed transition to Unattached(epoch=228, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=226, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:15,063] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:15,710] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 1745 due to node 1 being disconnected (elapsed time since creation: 2044ms, elapsed time since send: 2044ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:15,714] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:33:19,173] INFO [RaftManager id=4] Completed transition to Unattached(epoch=230, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=228, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:24,511] INFO [RaftManager id=4] Completed transition to Unattached(epoch=236, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=230, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:24,951] INFO [RaftManager id=4] Completed transition to Unattached(epoch=237, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=236, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:25,172] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=238, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1704, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=237, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:25,202] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:30,184] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:30,980] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:33:34,305] INFO [RaftManager id=4] Completed transition to Unattached(epoch=240, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=238, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1704, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:35,346] INFO [RaftManager id=4] Completed transition to Unattached(epoch=242, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=240, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:35,899] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=243, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1704, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=242, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:41,438] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:41,609] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:41,624] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:41,679] INFO [RaftManager id=4] Completed transition to Unattached(epoch=244, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=243, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1720, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:41,805] INFO [RaftManager id=4] Completed transition to Unattached(epoch=246, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=244, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:41,839] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:42,510] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=246, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1720, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=246, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:42,825] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:43,219] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:43,221] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:43,272] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:43,346] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:43,351] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:43,404] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:43,545] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:43,594] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:43,653] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:44,059] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:44,201] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:45,624] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:45,722] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:33:45,692] INFO [RaftManager id=4] Completed transition to Unattached(epoch=247, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=246, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1720, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:46,466] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=247, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1720, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=247, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:33:46,735] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 1720 (kafka.log.UnifiedLog)
[2026-01-16 08:33:47,594] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 1720 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:33:47,610] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 1720 (kafka.log.UnifiedLog$)
[2026-01-16 08:33:47,613] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=788, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000000788.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 08:33:52,264] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 1720 with 0 producer ids in 133 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 08:33:52,373] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 2545ms for snapshot load and 2216ms for segment recovery from offset 1720 (kafka.log.UnifiedLog$)
[2026-01-16 08:33:52,432] INFO [RaftManager id=4] Truncated to offset 1720 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:33:56,392] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:33:57,372] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:33:57,852] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:33:58,374] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:58,474] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 1 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:33:58,487] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:33:58,573] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:33:58,587] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:33:58,610] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:33:58,625] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:33:58,637] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:33:58,644] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:33:58,644] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:33:58,644] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:33:58,653] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:58,697] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:58,701] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:58,728] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:33:58,730] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:33:58,739] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:58,760] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:33:58,821] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:33:58,856] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:58,870] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:58,892] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 1 from offset 1 with partition epoch 6 and high watermark 1. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:33:58,893] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:58,931] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:33:59,476] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:33:59,491] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:59,500] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:33:59,502] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:33:59,504] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:33:59,510] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:59,511] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:59,512] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:33:59,515] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:33:59,519] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:59,536] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:33:59,539] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:33:59,541] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:33:59,550] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:33:59,555] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:59,569] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:59,571] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:33:59,574] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:33:59,609] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:33:59,628] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:59,678] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:59,682] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=1, isr=[5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2026-01-16 08:33:59,682] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:59,683] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:33:59,686] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2026-01-16 08:33:59,699] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:33:59,699] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 1 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:33:59,863] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-29, __consumer_offsets-39, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:33:59,879] INFO [Broker id=4] Stopped fetchers as part of become-follower for 26 partitions (state.change.logger)
[2026-01-16 08:34:01,100] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-48 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-46 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-9 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-42 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-23 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-19 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-17 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-30 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-28 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-26 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-7 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-40 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-5 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-38 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-1 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-45 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-14 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-10 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-24 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-29 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,1), __consumer_offsets-39 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-37 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-35 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-33 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0), __consumer_offsets-2 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),1,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:34:01,243] INFO [Broker id=4] Started fetchers as part of become-follower for 26 partitions (state.change.logger)
[2026-01-16 08:34:02,960] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,296] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,299] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,304] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,302] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,319] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,330] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,327] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,331] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,330] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,339] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,340] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,341] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,341] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,347] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,345] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,349] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,349] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,351] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,349] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,356] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,355] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,357] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,358] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,358] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,360] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,365] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,365] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,368] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,369] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,370] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,375] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,376] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,379] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,380] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,380] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,380] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,380] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,384] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,382] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,388] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,393] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,403] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,404] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,415] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,414] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,419] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,422] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,426] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,423] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,435] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,433] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,437] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,439] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,441] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,440] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,444] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,444] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,451] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,453] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,453] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,456] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,463] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,509] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,518] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,523] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,533] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,539] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,565] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,566] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,567] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,580] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,594] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,595] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,596] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,605] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,605] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,607] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,625] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,632] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,635] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,638] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,639] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,641] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,642] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,649] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,651] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,652] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,652] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,653] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,673] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,674] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,674] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,676] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,679] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,685] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,689] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,696] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,703] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,713] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,714] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,717] INFO [GroupCoordinator 4]: Unloading group metadata for schema-registry with generation 1 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,719] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,733] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,736] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,738] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,742] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,748] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,751] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,754] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,760] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,773] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,788] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,788] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,792] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,808] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,840] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,855] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,855] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:03,855] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:03,861] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:04,991] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,011] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,065] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,140] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,140] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,141] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,141] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,141] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,142] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,144] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,144] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,145] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,145] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,145] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,149] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,149] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,149] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,150] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,150] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,151] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,151] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,152] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,152] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,152] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,152] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,153] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,153] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,153] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,153] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:05,153] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:14,772] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:34:16,291] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:34:16,361] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:34:16,654] INFO [RaftManager id=4] Completed transition to Unattached(epoch=248, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=247, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1877, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:34:17,115] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:34:19,307] INFO [RaftManager id=4] Completed transition to Unattached(epoch=252, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=248, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:34:19,682] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=252, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=1877, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=252, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:34:19,720] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:34:20,029] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 1877 (kafka.log.UnifiedLog)
[2026-01-16 08:34:20,172] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 1877 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:34:20,181] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 1877 (kafka.log.UnifiedLog$)
[2026-01-16 08:34:20,187] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=1720, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000001720.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 08:34:20,820] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 1877 with 0 producer ids in 59 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 08:34:20,836] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 8ms for snapshot load and 641ms for segment recovery from offset 1877 (kafka.log.UnifiedLog$)
[2026-01-16 08:34:20,851] INFO [RaftManager id=4] Truncated to offset 1877 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:34:29,649] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:34:29,827] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 1231 due to node 5 being disconnected (elapsed time since creation: 8652ms, elapsed time since send: 8652ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:34:29,846] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:34:29,929] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:34:30,113] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=287426646, epoch=1231) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:34:31,953] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:34:32,508] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,653] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,662] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:32,664] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:32,667] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,672] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:32,676] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:32,682] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:32,684] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:32,687] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,692] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:32,754] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,770] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 1 from offset 2 with partition epoch 7 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,774] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,789] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:32,793] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,799] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,813] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,833] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,841] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,859] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,873] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 1 with partition epoch 7 and high watermark 1. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,876] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,879] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,881] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:32,885] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,886] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,887] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,887] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,908] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,909] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,911] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:32,914] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:32,915] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,916] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:32,917] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:32,918] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,924] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,926] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,927] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,927] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 3 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:34:32,928] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,931] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,932] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,934] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,935] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:32,940] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,944] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:32,945] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2026-01-16 08:34:32,947] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 4 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:34:32,949] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 7 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:34:33,502] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={__consumer_offsets-45=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[1]), __consumer_offsets-14=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-9=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[1]), __consumer_offsets-42=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-23=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-19=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-17=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[1]), __consumer_offsets-30=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-28=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[1]), __consumer_offsets-26=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[1]), __consumer_offsets-39=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[1]), __consumer_offsets-7=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-37=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[1]), __consumer_offsets-5=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[2], lastFetchedEpoch=Optional[0]), __consumer_offsets-35=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[1]), __consumer_offsets-1=PartitionData(topicId=p0HtGUHKReOSVWe-VAlPVA, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[1])}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=287426646, epoch=1231), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:34:35,019] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:34:35,046] INFO [Broker id=4] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2026-01-16 08:34:35,082] INFO [Broker id=4] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2026-01-16 08:34:35,200] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:35,218] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:35,231] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:34:36,822] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,874] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,876] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,881] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,882] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,883] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,883] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,883] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,886] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,888] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,888] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,891] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,892] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,894] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,896] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,896] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,897] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,897] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,897] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,900] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,901] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,901] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,902] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,904] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,905] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,904] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,907] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,906] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,908] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,909] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,910] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,909] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,914] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,913] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,915] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,917] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,918] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,918] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,920] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,921] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,920] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,922] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,921] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,925] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,926] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,926] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,927] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,928] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,929] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,928] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,931] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,929] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,934] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,936] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,938] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,936] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,945] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,943] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,945] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,946] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,951] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,961] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,963] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,963] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,964] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,966] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,967] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,966] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,969] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,969] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,969] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,971] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,971] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,972] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,974] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,978] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,977] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,979] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,979] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,982] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,984] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,986] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,985] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,987] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,987] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,989] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,990] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:36,991] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:36,990] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,002] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,016] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,013] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,020] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,027] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,030] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,034] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,036] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,035] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,047] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,046] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,048] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,049] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,050] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,049] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,054] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,093] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,109] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,137] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,137] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,138] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,149] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,150] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,149] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,150] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,150] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,151] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,154] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,163] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,165] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,166] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,166] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,184] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,186] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,187] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,187] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,190] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,190] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,197] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,204] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,211] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,205] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,214] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,213] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,230] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,230] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,239] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,231] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,240] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,298] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,275] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,304] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,313] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,329] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,310] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,331] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,333] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,343] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:34:37,343] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,343] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:34:37,345] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:36:16,290] INFO [NodeToControllerChannelManager id=4 name=alter-partition] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:36:37,679] INFO [RaftManager id=4] Completed transition to Unattached(epoch=253, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=252, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2183, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:36:38,269] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:36:38,548] INFO [RaftManager id=4] Completed transition to Unattached(epoch=256, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=253, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:36:38,733] INFO [RaftManager id=4] Completed transition to Unattached(epoch=257, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=256, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:36:41,952] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:36:41,984] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=258, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2183, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=257, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:36:57,292] INFO [RaftManager id=4] Completed transition to Unattached(epoch=259, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=258, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:36:57,415] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:36:57,678] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 540 due to node 1 being disconnected (elapsed time since creation: 5175ms, elapsed time since send: 5010ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:36:57,790] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:00,817] INFO [RaftManager id=4] Completed transition to Unattached(epoch=262, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=259, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:03,305] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:06,776] INFO [RaftManager id=4] Completed transition to Unattached(epoch=264, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=262, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:08,580] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:09,845] INFO [RaftManager id=4] Completed transition to Unattached(epoch=266, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=264, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:13,610] INFO [RaftManager id=4] Completed transition to Unattached(epoch=267, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=266, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:14,179] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:20,044] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:23,630] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:24,757] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2128 due to node 1 being disconnected (elapsed time since creation: 5600ms, elapsed time since send: 3785ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:30,422] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:31,509] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:32,919] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2131 due to node 2 being disconnected (elapsed time since creation: 7360ms, elapsed time since send: 4612ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:36,181] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:36,281] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2133 due to node 1 being disconnected (elapsed time since creation: 5672ms, elapsed time since send: 5672ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:40,477] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:41,306] INFO [RaftManager id=4] Completed transition to Unattached(epoch=276, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=267, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:47,605] INFO [RaftManager id=4] Completed transition to Unattached(epoch=278, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=276, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:37:54,595] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:37:55,517] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:37:55,774] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2145 due to node 1 being disconnected (elapsed time since creation: 4370ms, elapsed time since send: 4147ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:02,148] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:05,200] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2148 due to node 3 being disconnected (elapsed time since creation: 5176ms, elapsed time since send: 2955ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:09,152] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:09,255] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:38:09,277] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2150 due to node 2 being disconnected (elapsed time since creation: 6695ms, elapsed time since send: 2491ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:19,101] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:21,311] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2156 due to node 3 being disconnected (elapsed time since creation: 4162ms, elapsed time since send: 3764ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:25,617] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:38:25,627] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:26,049] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2159 due to node 1 being disconnected (elapsed time since creation: 5407ms, elapsed time since send: 2346ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:33,618] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:37,921] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2164 due to node 3 being disconnected (elapsed time since creation: 2095ms, elapsed time since send: 2095ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:40,183] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:38:50,595] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:51,032] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2171 due to node 1 being disconnected (elapsed time since creation: 3364ms, elapsed time since send: 3364ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:51,109] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:51,206] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2172 due to node 2 being disconnected (elapsed time since creation: 3151ms, elapsed time since send: 2805ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:38:55,493] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:38:59,131] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:00,188] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2177 due to node 3 being disconnected (elapsed time since creation: 2928ms, elapsed time since send: 2928ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:06,110] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:07,208] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2180 due to node 1 being disconnected (elapsed time since creation: 6860ms, elapsed time since send: 4850ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:09,322] INFO [RaftManager id=4] Completed transition to Unattached(epoch=300, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=278, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:39:10,518] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:39:13,836] INFO [RaftManager id=4] Completed transition to Unattached(epoch=303, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=300, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:39:24,249] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:24,399] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2185 due to node 1 being disconnected (elapsed time since creation: 4798ms, elapsed time since send: 2065ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:30,131] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:39:32,369] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:32,688] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2188 due to node 1 being disconnected (elapsed time since creation: 7032ms, elapsed time since send: 7032ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:41,043] INFO [RaftManager id=4] Completed transition to Unattached(epoch=310, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=303, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:39:41,935] INFO [RaftManager id=4] Completed transition to Unattached(epoch=312, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=310, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:39:44,168] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:39:44,528] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:44,597] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2197 due to node 3 being disconnected (elapsed time since creation: 2193ms, elapsed time since send: 2192ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:44,996] INFO [RaftManager id=4] Completed transition to Unattached(epoch=315, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=312, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:39:49,401] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:51,376] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2199 due to node 3 being disconnected (elapsed time since creation: 4180ms, elapsed time since send: 3408ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:55,626] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:56,209] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2204 due to node 3 being disconnected (elapsed time since creation: 2576ms, elapsed time since send: 2576ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:39:57,152] INFO [RaftManager id=4] Completed transition to Unattached(epoch=321, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=315, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:00,176] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:40:00,779] INFO [RaftManager id=4] Completed transition to Unattached(epoch=323, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=321, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:03,471] INFO [RaftManager id=4] Completed transition to Unattached(epoch=324, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=323, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:04,001] INFO [RaftManager id=4] Completed transition to Unattached(epoch=325, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=324, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:04,903] INFO [RaftManager id=4] Completed transition to Unattached(epoch=326, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=325, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:06,614] INFO [RaftManager id=4] Completed transition to Unattached(epoch=327, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=326, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:07,419] INFO [RaftManager id=4] Completed transition to Unattached(epoch=328, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=327, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:09,924] INFO [RaftManager id=4] Completed transition to Unattached(epoch=329, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=328, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:14,521] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:40:17,117] INFO [RaftManager id=4] Completed transition to Unattached(epoch=333, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=329, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:19,900] INFO [RaftManager id=4] Completed transition to Unattached(epoch=334, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=333, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:24,106] INFO [RaftManager id=4] Completed transition to Unattached(epoch=336, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=334, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:29,746] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=338, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=336, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:30,017] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:40:30,332] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:40:33,005] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:34,224] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2222 due to node 1 being disconnected (elapsed time since creation: 2285ms, elapsed time since send: 2113ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:40:37,378] INFO [RaftManager id=4] Completed transition to Unattached(epoch=343, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=338, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:37,823] INFO [RaftManager id=4] Completed transition to Unattached(epoch=344, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=343, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:38,813] INFO [RaftManager id=4] Completed transition to Unattached(epoch=345, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=344, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:40,436] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=345, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=345, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:45,857] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:40:46,426] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:40:48,312] INFO [RaftManager id=4] Completed transition to Unattached(epoch=347, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=345, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:51,197] INFO [RaftManager id=4] Completed transition to Unattached(epoch=349, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=347, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:40:52,057] INFO [RaftManager id=4] Completed transition to Unattached(epoch=350, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=349, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:41:02,460] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:03,331] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2235 due to node 1 being disconnected (elapsed time since creation: 7856ms, elapsed time since send: 2091ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:04,221] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:05,934] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:41:08,911] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:09,387] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2240 due to node 2 being disconnected (elapsed time since creation: 3071ms, elapsed time since send: 2644ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:09,763] INFO [RaftManager id=4] Completed transition to Unattached(epoch=358, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=350, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:41:15,881] INFO [RaftManager id=4] Completed transition to Unattached(epoch=361, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=358, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:41:19,742] INFO [RaftManager id=4] Completed transition to Unattached(epoch=362, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=361, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:41:19,791] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:41:26,243] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:27,252] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2245 due to node 3 being disconnected (elapsed time since creation: 2327ms, elapsed time since send: 2230ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:30,379] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:34,892] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:41:34,919] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2246 due to node 2 being disconnected (elapsed time since creation: 7860ms, elapsed time since send: 2390ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:45,831] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:46,190] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2249 due to node 1 being disconnected (elapsed time since creation: 9595ms, elapsed time since send: 7830ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:50,020] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:50,619] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:41:51,552] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2252 due to node 2 being disconnected (elapsed time since creation: 10685ms, elapsed time since send: 10685ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:53,850] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:54,825] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2256 due to node 1 being disconnected (elapsed time since creation: 4949ms, elapsed time since send: 4949ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:56,325] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:57,054] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2259 due to node 2 being disconnected (elapsed time since creation: 2467ms, elapsed time since send: 2467ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:57,066] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:41:57,081] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2257 due to node 3 being disconnected (elapsed time since creation: 9270ms, elapsed time since send: 4347ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:04,350] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:05,562] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:42:05,562] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2263 due to node 3 being disconnected (elapsed time since creation: 2566ms, elapsed time since send: 2566ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:15,615] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:16,202] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2271 due to node 1 being disconnected (elapsed time since creation: 3481ms, elapsed time since send: 2270ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:21,174] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:42:21,734] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:22,553] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2273 due to node 2 being disconnected (elapsed time since creation: 5918ms, elapsed time since send: 5191ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:38,380] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:42:39,821] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:40,482] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2283 due to node 1 being disconnected (elapsed time since creation: 4207ms, elapsed time since send: 2194ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:49,579] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:52,108] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2287 due to node 3 being disconnected (elapsed time since creation: 2747ms, elapsed time since send: 2362ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:42:53,767] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:43:07,711] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:10,062] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:43:10,189] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2294 due to node 2 being disconnected (elapsed time since creation: 6998ms, elapsed time since send: 2682ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:13,479] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:14,157] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2298 due to node 1 being disconnected (elapsed time since creation: 3499ms, elapsed time since send: 2575ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:15,363] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:15,593] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2300 due to node 2 being disconnected (elapsed time since creation: 2043ms, elapsed time since send: 2043ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:23,710] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:24,814] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:43:24,912] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2305 due to node 1 being disconnected (elapsed time since creation: 3181ms, elapsed time since send: 3181ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:25,023] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:25,042] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2306 due to node 3 being disconnected (elapsed time since creation: 2248ms, elapsed time since send: 2221ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:36,348] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:39,824] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:43:43,446] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2310 due to node 3 being disconnected (elapsed time since creation: 8399ms, elapsed time since send: 3491ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:49,000] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:43:50,613] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2312 due to node 1 being disconnected (elapsed time since creation: 11014ms, elapsed time since send: 11014ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:01,260] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:01,697] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2317 due to node 3 being disconnected (elapsed time since creation: 14282ms, elapsed time since send: 14282ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:02,396] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:02,484] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2318 due to node 2 being disconnected (elapsed time since creation: 17371ms, elapsed time since send: 2426ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:03,447] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:44:05,536] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:06,071] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2323 due to node 1 being disconnected (elapsed time since creation: 3925ms, elapsed time since send: 3925ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:06,072] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:06,077] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2325 due to node 3 being disconnected (elapsed time since creation: 2898ms, elapsed time since send: 2898ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:15,751] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:19,265] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:44:19,974] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2331 due to node 3 being disconnected (elapsed time since creation: 4468ms, elapsed time since send: 3785ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:22,838] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:24,801] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2336 due to node 3 being disconnected (elapsed time since creation: 2241ms, elapsed time since send: 2241ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:27,486] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:27,677] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2338 due to node 1 being disconnected (elapsed time since creation: 4649ms, elapsed time since send: 2035ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:35,762] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:44:36,751] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:37,043] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2342 due to node 1 being disconnected (elapsed time since creation: 2942ms, elapsed time since send: 2942ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:38,081] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:38,262] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2343 due to node 2 being disconnected (elapsed time since creation: 8561ms, elapsed time since send: 2063ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:43,981] INFO [RaftManager id=4] Completed transition to Unattached(epoch=406, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=362, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:44:48,783] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:49,849] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2350 due to node 1 being disconnected (elapsed time since creation: 2252ms, elapsed time since send: 2203ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:44:50,692] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:45:05,534] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:45:08,008] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:10,707] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2355 due to node 3 being disconnected (elapsed time since creation: 7788ms, elapsed time since send: 5485ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:11,621] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:11,642] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2358 due to node 1 being disconnected (elapsed time since creation: 4181ms, elapsed time since send: 4181ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:14,433] INFO [RaftManager id=4] Completed transition to Unattached(epoch=411, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=406, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:45:20,131] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:20,348] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:45:20,705] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2364 due to node 2 being disconnected (elapsed time since creation: 4360ms, elapsed time since send: 4019ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:24,261] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:26,718] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2366 due to node 2 being disconnected (elapsed time since creation: 2540ms, elapsed time since send: 2540ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:38,263] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:45:40,825] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:43,026] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2372 due to node 1 being disconnected (elapsed time since creation: 11570ms, elapsed time since send: 7222ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:44,581] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:45,486] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2371 due to node 2 being disconnected (elapsed time since creation: 11133ms, elapsed time since send: 11133ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:45,529] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:45,535] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2373 due to node 3 being disconnected (elapsed time since creation: 9282ms, elapsed time since send: 7222ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:50,819] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:51,296] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2378 due to node 1 being disconnected (elapsed time since creation: 4407ms, elapsed time since send: 4407ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:51,357] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:51,357] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2379 due to node 2 being disconnected (elapsed time since creation: 4407ms, elapsed time since send: 4407ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:51,359] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:51,363] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2380 due to node 3 being disconnected (elapsed time since creation: 4407ms, elapsed time since send: 4407ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:55,104] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:45:57,176] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:45:57,466] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2386 due to node 2 being disconnected (elapsed time since creation: 2440ms, elapsed time since send: 2376ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:02,781] INFO [RaftManager id=4] Completed transition to Unattached(epoch=420, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=411, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:05,952] INFO [RaftManager id=4] Completed transition to Unattached(epoch=423, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=420, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:07,589] INFO [RaftManager id=4] Completed transition to Unattached(epoch=425, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=423, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:09,019] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:46:13,141] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:13,556] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2395 due to node 3 being disconnected (elapsed time since creation: 2623ms, elapsed time since send: 2300ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:13,984] INFO [RaftManager id=4] Completed transition to Unattached(epoch=428, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=425, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:15,675] INFO [RaftManager id=4] Completed transition to Unattached(epoch=429, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=428, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:16,720] INFO [RaftManager id=4] Completed transition to Unattached(epoch=430, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=429, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:18,935] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:19,272] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2400 due to node 2 being disconnected (elapsed time since creation: 2013ms, elapsed time since send: 2013ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:22,718] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:46:25,134] INFO [RaftManager id=4] Completed transition to Unattached(epoch=433, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=430, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:26,085] INFO [RaftManager id=4] Completed transition to Unattached(epoch=434, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=433, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:27,509] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=434, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=434, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:27,667] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:46:31,485] INFO [RaftManager id=4] Completed transition to Unattached(epoch=437, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=434, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:33,925] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:33,992] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:34,452] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2409 due to node 3 being disconnected (elapsed time since creation: 2101ms, elapsed time since send: 2041ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:37,604] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:46:39,245] INFO [RaftManager id=4] Completed transition to Unattached(epoch=441, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=437, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:42,902] INFO [RaftManager id=4] Completed transition to Unattached(epoch=444, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=441, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:44,677] INFO [RaftManager id=4] Completed transition to Unattached(epoch=447, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=444, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:48,794] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:46:49,109] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=448, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=447, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:52,227] INFO [RaftManager id=4] Completed transition to Unattached(epoch=450, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=448, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:53,040] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:46:54,280] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:46:54,284] INFO [RaftManager id=4] Completed transition to Unattached(epoch=451, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=450, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:55,293] INFO [RaftManager id=4] Completed transition to Unattached(epoch=452, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=451, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:46:56,480] INFO [RaftManager id=4] Completed transition to Unattached(epoch=453, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=452, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:06,137] INFO [RaftManager id=4] Completed transition to Unattached(epoch=454, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=453, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:09,665] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:47:10,791] INFO [RaftManager id=4] Completed transition to Unattached(epoch=458, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=454, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:11,308] INFO [RaftManager id=4] Completed transition to Unattached(epoch=461, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=458, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:11,732] INFO [RaftManager id=4] Completed transition to Unattached(epoch=462, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=461, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:13,890] INFO [RaftManager id=4] Completed transition to Unattached(epoch=463, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=462, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:20,872] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=469, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=463, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:20,897] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:23,905] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:24,697] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:24,946] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:47:25,862] INFO [RaftManager id=4] Completed transition to Unattached(epoch=470, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=469, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:26,607] INFO [RaftManager id=4] Completed transition to Unattached(epoch=472, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=470, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:29,434] INFO [RaftManager id=4] Completed transition to Unattached(epoch=473, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=472, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:31,057] INFO [RaftManager id=4] Completed transition to Unattached(epoch=474, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=473, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:33,688] INFO [RaftManager id=4] Completed transition to Unattached(epoch=476, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=474, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:35,049] INFO [RaftManager id=4] Completed transition to Unattached(epoch=478, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=476, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:35,425] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=480, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=478, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:35,463] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:35,491] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:35,555] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:35,907] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:35,918] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:36,104] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:36,134] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:36,284] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:41,940] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:42,323] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:42,395] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:43,050] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:43,051] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:43,143] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:43,571] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:43,597] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:43,744] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:43,839] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:43,844] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:43,900] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:44,241] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:44,279] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:44,422] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:44,489] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:44,597] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:45,001] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:45,184] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:45,250] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:47:46,048] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:47:47,934] INFO [RaftManager id=4] Completed transition to Unattached(epoch=484, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=480, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:47,978] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:47:49,522] INFO [RaftManager id=4] Completed transition to Unattached(epoch=487, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=484, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:53,083] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:47:56,144] INFO [RaftManager id=4] Completed transition to Unattached(epoch=488, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=487, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:47:59,201] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:47:59,800] INFO [RaftManager id=4] Completed transition to Unattached(epoch=491, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=488, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:03,198] INFO [RaftManager id=4] Completed transition to Unattached(epoch=492, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=491, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:05,337] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:48:08,274] INFO [RaftManager id=4] Completed transition to Unattached(epoch=496, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=492, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:11,005] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:48:11,432] INFO [RaftManager id=4] Completed transition to Unattached(epoch=497, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=496, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:16,673] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:17,238] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2451 due to node 2 being disconnected (elapsed time since creation: 4051ms, elapsed time since send: 3732ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:19,644] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=502, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2197, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=497, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:19,653] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:48:37,344] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:48:38,317] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:38,614] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:48:41,746] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:42,252] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:48:42,296] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:48:43,844] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:46,225] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=513, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2226, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=502, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2226, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:46,568] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:48:48,105] INFO [RaftManager id=4] Completed transition to Unattached(epoch=517, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=513, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2226, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:48:48,969] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:48:49,170] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:48:53,295] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:48:53,515] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=517, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2226, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=517, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:03,347] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:03,668] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2464 due to node 2 being disconnected (elapsed time since creation: 3708ms, elapsed time since send: 3589ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:06,956] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:07,540] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:07,603] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:49:09,407] INFO [RaftManager id=4] Completed transition to Unattached(epoch=520, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=517, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2247, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:11,217] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:12,015] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:12,033] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=522, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2247, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=520, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:20,210] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:49:21,278] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:21,521] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:21,568] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:22,044] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:22,151] INFO [RaftManager id=4] Completed transition to Unattached(epoch=525, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=522, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:22,324] INFO [RaftManager id=4] Completed transition to Unattached(epoch=526, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=525, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:24,440] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=526, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=526, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:24,487] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:24,625] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:24,684] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:24,787] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:25,206] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:25,328] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:25,511] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:25,526] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=527, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=526, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:26,562] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:26,772] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:49:27,465] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 2268 (kafka.log.UnifiedLog)
[2026-01-16 08:49:30,509] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 2268 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 08:49:30,884] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2268 (kafka.log.UnifiedLog$)
[2026-01-16 08:49:31,011] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=1877, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000001877.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 08:49:38,756] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:38,917] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:39,175] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2268 with 0 producer ids in 195 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 08:49:39,187] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 493ms for snapshot load and 7792ms for segment recovery from offset 2268 (kafka.log.UnifiedLog$)
[2026-01-16 08:49:39,192] INFO [RaftManager id=4] Truncated to offset 2268 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 08:49:40,397] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:40,538] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:41,299] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=530, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=527, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:41,443] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:41,448] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:42,391] INFO [RaftManager id=4] Completed transition to Unattached(epoch=531, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=530, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:42,525] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:43,048] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:49:46,306] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:46,897] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=531, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=531, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:49:55,980] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:49:57,272] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:49:57,569] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:49:58,290] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2481 due to node 2 being disconnected (elapsed time since creation: 2297ms, elapsed time since send: 2077ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:02,778] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:03,708] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:03,848] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:07,416] INFO [RaftManager id=4] Completed transition to Unattached(epoch=534, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=531, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:07,474] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:08,563] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:11,596] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:13,931] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2488 due to node 2 being disconnected (elapsed time since creation: 2668ms, elapsed time since send: 2346ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:14,062] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:20,664] INFO [RaftManager id=4] Completed transition to Unattached(epoch=539, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=534, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:20,949] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:25,419] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:25,498] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=543, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=539, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:25,771] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:25,782] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:25,831] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:25,870] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:25,873] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:25,923] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:26,086] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:26,139] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:26,194] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:26,368] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:26,542] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:26,601] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:27,063] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:27,291] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:32,690] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:33,001] INFO [RaftManager id=4] Completed transition to Unattached(epoch=546, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=543, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:34,355] INFO [RaftManager id=4] Completed transition to Unattached(epoch=550, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=546, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:34,738] INFO [RaftManager id=4] Completed transition to Unattached(epoch=551, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=550, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:36,865] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:37,171] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=551, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=551, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:41,653] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:42,393] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:44,068] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:44,387] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2499 due to node 2 being disconnected (elapsed time since creation: 2852ms, elapsed time since send: 2839ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:45,079] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:50:45,125] INFO [RaftManager id=4] Completed transition to Unattached(epoch=556, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=551, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:45,355] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:50:47,615] INFO [RaftManager id=4] Completed transition to Unattached(epoch=557, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=556, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:49,424] INFO [RaftManager id=4] Completed transition to Unattached(epoch=559, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=557, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:50,364] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=559, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2264, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=559, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:50:50,451] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:50:52,330] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:50:54,737] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:55,460] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:55,486] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:55,539] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:55,556] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:55,559] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:55,566] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:55,568] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:55,570] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:55,577] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:55,582] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:55,584] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:55,607] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 2 from offset 2 with partition epoch 8 and high watermark 2. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:55,609] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:55,610] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:55,624] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:55,637] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:55,647] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:55,653] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:55,660] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:55,673] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:55,689] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 3 from offset 1 with partition epoch 8 and high watermark 1. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:55,694] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:55,789] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:55,827] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:55,934] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:55,998] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:56,012] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:56,030] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:56,045] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:56,956] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:56,986] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:56,988] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:56,991] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:56,993] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:56,999] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:57,000] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:57,011] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:57,013] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:57,016] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:57,017] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 4 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 08:50:57,018] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:57,021] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:57,021] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:57,263] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:57,275] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:57,395] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:57,470] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:57,512] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 2 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2026-01-16 08:50:57,543] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 5 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 08:50:57,560] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 3 from offset 0 with partition epoch 8 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 08:50:57,744] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:50:57,767] INFO [Broker id=4] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2026-01-16 08:51:06,491] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,572] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,613] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,631] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-13 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-46 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-9 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-42 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-21 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-17 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-30 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-26 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-5 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-38 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-1 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-34 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-16 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), _schemas-0 -> InitialFetchState(Some(UjN16IrSRlipZQeCwkGrdQ),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,2), __consumer_offsets-45 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-12 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-41 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-24 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-20 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-49 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-0 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-29 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,1), __consumer_offsets-25 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-8 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-37 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-4 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-33 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-15 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-48 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-11 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-44 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-23 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-19 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-32 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-28 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-7 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-40 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-3 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-36 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-47 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-14 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-43 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-10 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-22 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-18 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-31 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0), __consumer_offsets-27 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-39 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-6 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-35 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-2 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 08:51:06,651] INFO [Broker id=4] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2026-01-16 08:51:06,704] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,709] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,732] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,771] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,774] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,800] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,841] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,842] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,847] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,870] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,881] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,882] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,882] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,886] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,887] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,889] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,895] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,897] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,900] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,903] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,907] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,909] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,919] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,919] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,924] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,929] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,939] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,942] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,949] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,952] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,959] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,980] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,982] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,984] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,988] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,989] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,989] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,990] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,992] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,998] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:06,999] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:06,999] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:07,000] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:07,000] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:07,000] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 08:51:07,001] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 08:51:07,425] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:07,557] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,363] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,476] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,477] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,477] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,478] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,485] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,486] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,491] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,492] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,493] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,505] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,506] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,509] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,517] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,517] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,519] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,521] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,522] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,524] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,525] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,526] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,526] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,531] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,531] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,533] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,544] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,545] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,546] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,547] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,548] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,557] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,560] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,561] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,597] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,598] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,599] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,600] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,601] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,602] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,604] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,605] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,607] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,590] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,608] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,640] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,644] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,669] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,671] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,637] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,671] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,671] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,673] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,673] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,673] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,677] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,677] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,677] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,677] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,678] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,678] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,678] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,678] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,678] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,678] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,678] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,678] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,679] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,679] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,681] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,681] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,682] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,682] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,683] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,683] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,683] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,684] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,684] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,685] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,685] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,686] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,686] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,689] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,687] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,691] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,689] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,692] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,692] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,692] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,693] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,692] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,696] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,695] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,697] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,696] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,698] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,698] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,699] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,704] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,705] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,704] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,707] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,709] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,706] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,709] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,710] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,709] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,713] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,710] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,714] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,713] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,717] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,721] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,725] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,717] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,727] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,728] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,732] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,733] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,726] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,737] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,744] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,736] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,748] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,748] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,749] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,752] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,749] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,755] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,756] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,754] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,758] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,757] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,760] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,761] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,761] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,762] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,762] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,763] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,765] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:08,763] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,770] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,765] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,784] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,785] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,786] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,790] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:08,790] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:27,762] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:51:28,692] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,466] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:29,485] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:29,523] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:29,626] INFO [Broker id=4] Transitioning 50 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:51:29,672] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,690] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:29,701] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:29,715] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,720] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:29,730] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:29,743] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:29,748] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:29,750] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,760] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:29,766] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,767] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,774] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,776] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:29,776] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,781] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,784] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,789] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,791] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,803] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,803] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,803] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,804] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,805] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:29,805] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,807] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,852] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,884] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,954] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,957] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,965] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:29,970] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:29,978] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:29,981] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:29,986] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:29,988] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,990] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:29,995] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,003] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,019] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=9, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:30,039] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,042] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,046] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,047] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,048] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,048] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,048] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:30,051] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=9, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:30,052] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=9, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:30,052] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4], partitionEpoch=9, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:30,055] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,057] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,058] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,062] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,061] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,065] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,065] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,065] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,065] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,065] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,066] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,068] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,069] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,069] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,074] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,076] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,075] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,079] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,080] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,084] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,099] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,099] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,101] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,103] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,103] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,106] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,108] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,108] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,108] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,108] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,108] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,109] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,109] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,109] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,109] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,116] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,115] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,119] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,121] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,124] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,128] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,132] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,134] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,162] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,185] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,195] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,196] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,195] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,198] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,200] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,204] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,206] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,211] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,212] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,215] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,218] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,221] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,222] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,223] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,227] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,240] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,230] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,250] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,250] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,262] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,271] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,272] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,275] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,283] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,283] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,284] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,284] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,283] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,284] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,284] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,290] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,289] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,305] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,306] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,307] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,307] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,311] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,314] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,307] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,316] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,316] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,320] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,322] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,325] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,326] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,320] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,327] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,327] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,328] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,329] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,331] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,332] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,332] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,334] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,339] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,339] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,339] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,339] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,340] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,335] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,340] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,340] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,341] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,340] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,346] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,348] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,348] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,355] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,355] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,357] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,357] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,355] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,358] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,358] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,358] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,359] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,462] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,466] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,470] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,470] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,471] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,471] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,471] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,473] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,480] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,475] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,481] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,483] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,481] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,486] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,486] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,487] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,487] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,487] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,487] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,488] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,488] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,488] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,488] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,489] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:30,490] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:30,494] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,095] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:51:32,746] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:32,816] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:32,926] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:32,963] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:33,053] INFO [Broker id=4] Transitioning 23 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:51:33,203] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,319] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,320] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,336] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:33,380] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,392] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:33,480] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,824] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,835] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,839] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:33,846] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,853] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:33,857] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,874] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,882] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,892] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:33,899] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:33,904] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:33,915] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:33,926] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:33,940] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:33,942] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:33,957] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:34,051] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,074] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,090] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,094] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,098] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,098] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,100] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,103] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,105] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,106] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,106] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,107] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,107] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,109] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,110] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,111] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,113] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,114] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,115] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,116] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,116] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,117] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,117] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,118] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,120] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,121] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,121] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,123] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,124] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,125] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,126] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,127] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,129] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,131] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,131] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,132] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,134] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,134] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,135] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,135] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,136] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,136] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,137] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,139] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,140] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:34,123] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,143] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,143] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,144] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,146] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,148] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,148] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,149] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,149] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,149] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,150] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,150] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,150] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,152] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,153] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,154] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,156] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,158] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,159] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,160] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,160] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,160] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,142] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,186] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:34,719] INFO [Broker id=4] Transitioning 27 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:51:34,830] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:34,891] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:34,937] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:35,010] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:35,218] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,225] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:35,326] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 08:51:35,349] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,360] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,420] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,427] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,439] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,465] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,485] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,509] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,523] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,557] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,563] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,881] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,908] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:35,910] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,924] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:35,925] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,926] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=10, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 08:51:35,929] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 08:51:35,946] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:35,993] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 4, 6], partitionEpoch=10, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:51:36,287] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,350] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,377] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,409] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,419] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,431] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,442] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,444] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,445] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,422] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,454] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,448] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,466] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,469] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,474] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,481] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,493] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,500] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,503] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,526] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,468] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,528] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,546] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,548] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,528] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,574] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,555] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,581] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,607] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,608] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,607] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,609] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,613] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,657] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,668] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,632] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,689] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,690] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,698] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,713] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:36,683] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,717] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:36,714] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,143] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:37,184] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,205] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:37,207] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,210] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:37,213] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,226] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:37,238] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,118] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,242] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,276] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,277] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,277] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,277] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,279] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,280] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,285] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,243] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:37,305] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,307] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:37,307] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,310] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:37,312] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,329] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:37,329] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,333] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:37,335] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,336] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:51:37,359] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,308] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,365] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,368] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,370] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,374] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:51:37,392] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:53:53,115] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Disconnecting from node 5 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:53:55,953] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 13 due to node 5 being disconnected (elapsed time since creation: 107792ms, elapsed time since send: 107791ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:53:56,792] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:53:56,890] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:53:57,148] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2619 due to node 2 being disconnected (elapsed time since creation: 15351ms, elapsed time since send: 3669ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:53:57,248] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=324163015, epoch=13) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:54:01,067] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:03,760] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:03,761] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=324163015, epoch=13), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 08:54:03,986] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2620 due to node 3 being disconnected (elapsed time since creation: 5929ms, elapsed time since send: 2559ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:08,950] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:09,578] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 672 due to node 2 being disconnected (elapsed time since creation: 4686ms, elapsed time since send: 4659ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:13,313] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:54:13,925] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:13,948] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:14,704] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2623 due to node 2 being disconnected (elapsed time since creation: 4563ms, elapsed time since send: 4563ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:17,476] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:17,874] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2627 due to node 2 being disconnected (elapsed time since creation: 2144ms, elapsed time since send: 2144ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:20,268] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:20,660] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight API_VERSIONS request with correlation id 674 due to node 2 being disconnected (elapsed time since creation: 4713ms, elapsed time since send: 4713ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:20,922] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:54:20,970] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:23,872] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:24,157] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2630 due to node 3 being disconnected (elapsed time since creation: 2086ms, elapsed time since send: 2086ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:28,277] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:29,592] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:54:29,597] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2634 due to node 3 being disconnected (elapsed time since creation: 3026ms, elapsed time since send: 3026ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:29,663] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:34,364] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:34,685] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2635 due to node 1 being disconnected (elapsed time since creation: 5581ms, elapsed time since send: 2956ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:34,703] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:36,483] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:36,613] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2637 due to node 2 being disconnected (elapsed time since creation: 2993ms, elapsed time since send: 2993ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:37,146] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:37,153] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:54:37,222] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:54:37,665] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:37,684] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:54:38,083] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:54:41,099] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:41,487] INFO [RaftManager id=4] Completed transition to Unattached(epoch=577, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=559, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2596, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:54:41,758] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:45,260] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:45,738] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2646 due to node 3 being disconnected (elapsed time since creation: 2020ms, elapsed time since send: 2020ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:48,699] INFO [RaftManager id=4] Completed transition to Unattached(epoch=581, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=577, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:54:50,040] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:54:51,018] INFO [RaftManager id=4] Completed transition to Unattached(epoch=582, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=581, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:54:52,296] INFO [RaftManager id=4] Completed transition to Unattached(epoch=583, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=582, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:54:52,782] INFO [RaftManager id=4] Completed transition to Unattached(epoch=584, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=583, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:54:55,382] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:55,615] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2654 due to node 2 being disconnected (elapsed time since creation: 2484ms, elapsed time since send: 2484ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:54:58,002] INFO [RaftManager id=4] Completed transition to Unattached(epoch=586, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=584, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:02,080] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:55:03,600] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:03,782] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 2657 due to node 2 being disconnected (elapsed time since creation: 2159ms, elapsed time since send: 2159ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:06,386] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:06,407] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=590, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2596, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=586, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:14,211] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:14,523] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2662 due to node 2 being disconnected (elapsed time since creation: 3234ms, elapsed time since send: 3117ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:14,916] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:14,987] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:15,040] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:15,094] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:15,543] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:15,737] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:15,748] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:15,800] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:15,828] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:15,836] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:15,899] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:16,030] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:16,048] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:16,070] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=595, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2596, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=590, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2596, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:16,104] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:18,180] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:18,271] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:55:18,448] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:55:19,889] INFO [RaftManager id=4] Completed transition to Unattached(epoch=599, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=595, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2601, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:21,427] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=599, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2601, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=599, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:23,899] INFO [RaftManager id=4] Completed transition to Unattached(epoch=600, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=599, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2601, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:25,774] INFO [RaftManager id=4] Completed transition to Unattached(epoch=602, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=600, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:26,011] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=603, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2601, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=602, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:55:29,840] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:55:30,356] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5], partitionEpoch=11, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:55:34,061] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:55:34,482] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:55:34,699] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:55:36,043] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:55:36,860] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:56:03,498] INFO [RaftManager id=4] Completed transition to Unattached(epoch=604, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=603, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2613, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:06,787] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:56:08,260] INFO [RaftManager id=4] Completed transition to Unattached(epoch=610, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=604, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:08,664] INFO [RaftManager id=4] Completed transition to Unattached(epoch=612, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=610, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:10,745] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:13,764] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:56:13,857] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2686 due to node 2 being disconnected (elapsed time since creation: 2009ms, elapsed time since send: 2008ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:19,050] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:56:19,211] INFO [RaftManager id=4] Completed transition to Unattached(epoch=618, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=612, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:22,771] INFO [RaftManager id=4] Completed transition to Unattached(epoch=619, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=618, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:25,752] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:56:26,208] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:26,957] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2693 due to node 3 being disconnected (elapsed time since creation: 2214ms, elapsed time since send: 2138ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:33,187] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:56:37,181] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:37,968] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2698 due to node 1 being disconnected (elapsed time since creation: 2856ms, elapsed time since send: 2277ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:43,024] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:56:45,500] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=624, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2613, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=619, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:45,504] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:56:49,200] INFO [RaftManager id=4] Completed transition to Unattached(epoch=628, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=624, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2613, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:49,846] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:52,356] INFO [RaftManager id=4] Completed transition to Unattached(epoch=631, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=628, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:56:52,887] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:56:54,849] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:56:54,959] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2705 due to node 2 being disconnected (elapsed time since creation: 2044ms, elapsed time since send: 2043ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:03,501] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:03,821] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2707 due to node 3 being disconnected (elapsed time since creation: 4275ms, elapsed time since send: 3454ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:06,005] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:57:10,789] INFO [RaftManager id=4] Completed transition to Unattached(epoch=637, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=631, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:17,073] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:17,565] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 2715 due to node 3 being disconnected (elapsed time since creation: 3903ms, elapsed time since send: 3579ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:19,958] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:57:20,148] INFO [RaftManager id=4] Completed transition to Unattached(epoch=642, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=637, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:20,916] INFO [RaftManager id=4] Completed transition to Unattached(epoch=644, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=642, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:22,292] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:57:22,322] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=644, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2613, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=644, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:22,540] INFO [RaftManager id=4] Completed transition to Unattached(epoch=645, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=644, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2613, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:27,042] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=647, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2613, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=645, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:33,923] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:57:34,627] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:57:36,730] INFO [RaftManager id=4] Completed transition to Unattached(epoch=650, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=647, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2613, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:38,183] INFO [RaftManager id=4] Completed transition to Unattached(epoch=652, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=650, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:39,251] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=652, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2613, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=652, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:40,320] INFO [RaftManager id=4] Completed transition to Unattached(epoch=653, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=652, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2613, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:40,771] INFO [RaftManager id=4] Completed transition to Unattached(epoch=654, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=653, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:42,276] INFO [RaftManager id=4] Completed transition to Unattached(epoch=655, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=654, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:44,911] INFO [RaftManager id=4] Completed transition to Unattached(epoch=656, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=655, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:45,588] INFO [RaftManager id=4] Completed transition to Unattached(epoch=657, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=656, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:45,753] INFO [RaftManager id=4] Completed transition to Unattached(epoch=659, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=657, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:45,782] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:57:46,530] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=659, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2613, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=659, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:57:46,718] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:57:49,556] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:57:50,086] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6], partitionEpoch=12, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:57:50,610] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:57:50,617] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:57:51,457] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:57:52,070] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 08:57:52,450] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=3, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2026-01-16 08:57:52,711] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 08:57:52,764] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:57:52,792] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 08:58:25,264] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:58:25,718] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:58:28,211] INFO [RaftManager id=4] Completed transition to Unattached(epoch=662, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=659, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2698, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:58:28,705] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=663, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2698, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=662, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:58:38,958] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:58:39,574] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:58:39,750] INFO [RaftManager id=4] Completed transition to Unattached(epoch=664, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=663, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2715, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:58:40,099] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:58:40,163] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=666, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2715, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=664, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:58:40,220] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:58:40,676] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:58:40,687] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:58:40,743] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:59:11,113] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:59:11,244] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:59:11,297] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:59:11,926] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:59:11,939] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:59:11,997] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:59:13,073] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:59:13,204] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:59:13,346] INFO [RaftManager id=4] Completed transition to Unattached(epoch=667, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=666, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2768, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:59:13,364] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:59:17,743] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=674, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2768, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=667, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:59:19,956] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:59:20,257] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 08:59:20,332] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 08:59:20,914] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 08:59:21,056] INFO [RaftManager id=4] Completed transition to Unattached(epoch=679, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=674, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2768, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:59:21,115] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=680, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2768, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=679, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 08:59:21,207] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:00:59,223] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:00:59,676] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:00,066] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:03,350] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 3024 due to node 1 being disconnected (elapsed time since creation: 2029ms, elapsed time since send: 2029ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:05,040] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:05,153] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:01:06,008] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:06,543] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:06,603] INFO [RaftManager id=4] Completed transition to Unattached(epoch=684, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=680, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2943, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:06,750] INFO [RaftManager id=4] Completed transition to Unattached(epoch=685, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=684, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:07,141] INFO [RaftManager id=4] Completed transition to Unattached(epoch=686, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=685, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:08,792] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:08,822] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=686, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2943, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=686, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:09,193] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:10,670] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:11,494] INFO [RaftManager id=4] Completed transition to Unattached(epoch=691, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=686, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2943, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:11,511] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:01:11,962] INFO [RaftManager id=4] Completed transition to Unattached(epoch=692, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=691, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:13,016] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=692, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2943, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=692, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:13,084] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:13,518] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:13,520] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:13,571] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:13,645] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:13,647] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:13,726] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:13,794] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:13,836] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:13,899] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:14,424] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:14,478] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:14,615] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:14,815] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:14,820] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:14,899] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:15,624] INFO [RaftManager id=4] Completed transition to Unattached(epoch=693, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=692, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2943, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:15,697] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=693, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2943, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=693, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:17,417] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:17,582] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:37,483] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:37,586] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:37,724] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:01:38,106] INFO [RaftManager id=4] Completed transition to Unattached(epoch=695, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=693, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2974, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:38,317] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=697, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=2974, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=695, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:01:39,036] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:01:39,135] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:27,310] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:28,629] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 3259 due to node 2 being disconnected (elapsed time since creation: 2004ms, elapsed time since send: 2002ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:29,394] INFO [RaftManager id=4] Completed transition to Unattached(epoch=698, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=697, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3184, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:29,413] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:31,898] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:03:35,868] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=702, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3184, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=698, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:35,924] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:38,958] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:39,023] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 3265 due to node 3 being disconnected (elapsed time since creation: 2117ms, elapsed time since send: 2076ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:40,003] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:40,016] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:40,069] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:03:40,508] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:40,534] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:40,616] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:40,715] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:40,726] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:40,777] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:40,820] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:40,821] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:40,884] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:40,937] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:40,945] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:41,008] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:41,081] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:41,100] INFO [RaftManager id=4] Completed transition to Unattached(epoch=704, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=702, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3187, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:41,973] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=705, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3187, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=704, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:42,038] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:03:42,282] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 3187 (kafka.log.UnifiedLog)
[2026-01-16 09:03:42,395] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3187 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 09:03:42,398] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3187 (kafka.log.UnifiedLog$)
[2026-01-16 09:03:42,403] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=2268, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000002268.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 09:03:42,571] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3187 with 0 producer ids in 33 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 09:03:42,575] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 23ms for snapshot load and 153ms for segment recovery from offset 3187 (kafka.log.UnifiedLog$)
[2026-01-16 09:03:42,580] INFO [RaftManager id=4] Truncated to offset 3187 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 09:03:56,282] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:56,365] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 3292 due to node 1 being disconnected (elapsed time since creation: 2168ms, elapsed time since send: 2065ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:57,235] INFO [RaftManager id=4] Completed transition to Unattached(epoch=706, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=705, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3210, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:57,662] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:03:58,010] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:03:58,855] INFO [RaftManager id=4] Completed transition to Unattached(epoch=707, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=706, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:59,010] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=707, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3210, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=707, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:03:59,029] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:06:30,279] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:06:30,440] INFO [RaftManager id=4] Completed transition to Unattached(epoch=708, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=707, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3488, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:30,594] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=709, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3488, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=708, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:30,899] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:06:32,109] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:06:32,133] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:06:32,169] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:06:32,388] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:06:32,391] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:06:32,442] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:06:32,512] INFO [RaftManager id=4] Completed transition to Unattached(epoch=711, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=709, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3490, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:32,520] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:06:32,999] INFO [RaftManager id=4] Completed transition to Unattached(epoch=713, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=711, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:33,375] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=713, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=3490, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=713, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:06:33,476] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:31,473] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:31,809] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:31,849] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:32,107] INFO [RaftManager id=4] Completed transition to Unattached(epoch=715, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=713, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4054, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:11:32,170] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:32,308] INFO [RaftManager id=4] Completed transition to Unattached(epoch=717, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=715, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:11:33,010] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=717, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=4054, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=717, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:11:33,109] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:33,501] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:11:33,532] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:11:33,575] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:14:26,089] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000004383-0000000717 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 09:14:35,426] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000004383-0000000717 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 09:16:35,464] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2026-01-16 09:16:36,082] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-46, __consumer_offsets-10, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-24, __consumer_offsets-19, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 09:16:36,405] INFO [Broker id=4] Leader __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:36,629] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:36,669] INFO [Broker id=4] Leader __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,699] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:36,750] INFO [Broker id=4] Leader __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:36,775] INFO [Broker id=4] Leader __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,825] INFO [Broker id=4] Leader __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:36,914] INFO [Broker id=4] Leader __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:36,937] INFO [Broker id=4] Leader __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,005] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 8 with partition epoch 14, high watermark 8, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:37,051] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,083] INFO [Broker id=4] Leader __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,127] INFO [Broker id=4] Leader __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:37,151] INFO [Broker id=4] Leader __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,184] INFO [Broker id=4] Leader __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:37,216] INFO [Broker id=4] Leader __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:37,242] INFO [Broker id=4] Leader __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 4 from offset 0 with partition epoch 11, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2026-01-16 09:16:37,268] INFO [Broker id=4] Transitioning 16 partition(s) to local followers. (state.change.logger)
[2026-01-16 09:16:37,270] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 09:16:37,274] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,275] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,282] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 09:16:37,284] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,289] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,291] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 09:16:37,293] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,294] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 09:16:37,294] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 09:16:37,304] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 09:16:37,304] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,304] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 09:16:37,305] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 4 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2026-01-16 09:16:37,305] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 09:16:37,305] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 09:16:37,372] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-9, __consumer_offsets-21, __consumer_offsets-20, __consumer_offsets-17, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-3, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2026-01-16 09:16:37,383] INFO [Broker id=4] Stopped fetchers as part of become-follower for 16 partitions (state.change.logger)
[2026-01-16 09:16:37,862] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:37,916] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-45 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-43 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,0), __consumer_offsets-12 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,0), __consumer_offsets-9 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-21 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,0), __consumer_offsets-20 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,0), __consumer_offsets-17 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-31 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,0), __consumer_offsets-28 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-26 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-39 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-8 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,0), __consumer_offsets-37 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-35 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-3 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,0), __consumer_offsets-1 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 09:16:37,918] INFO [Broker id=4] Started fetchers as part of become-follower for 16 partitions (state.change.logger)
[2026-01-16 09:16:37,918] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:38,000] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:38,027] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:38,035] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:38,038] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:38,040] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:38,042] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:38,043] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:38,046] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:38,047] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:38,048] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:38,052] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:38,056] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2026-01-16 09:16:38,058] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2026-01-16 09:16:38,165] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 15 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:38,179] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:38,186] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:38,200] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:38,206] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 14 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,179] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,271] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,274] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,277] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 10 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,303] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,329] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 42 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,330] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,358] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 23 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,308] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-15 in 1128 milliseconds for epoch 4, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,358] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,359] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 24 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,360] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,366] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 19 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,369] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,372] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,388] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,381] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 1176 milliseconds for epoch 4, of which 1161 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,402] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,420] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,423] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 7 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,424] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,425] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 40 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,427] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,428] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 5 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,429] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,429] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 38 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,430] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,433] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 33 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,439] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,440] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 2 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,444] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,444] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,424] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-14 in 153 milliseconds for epoch 5, of which 144 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,456] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 179 milliseconds for epoch 4, of which 178 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,463] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,475] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,475] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,476] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,478] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,481] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,482] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,484] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,486] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,491] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,497] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,502] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,510] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,511] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,514] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,515] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,516] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,520] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,521] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,521] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,522] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,524] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,530] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,532] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,536] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,536] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,544] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,555] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,557] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,559] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:39,560] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,495] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-10 in 166 milliseconds for epoch 4, of which 144 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,585] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-42 in 227 milliseconds for epoch 5, of which 226 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,599] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-23 in 240 milliseconds for epoch 5, of which 240 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,606] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-24 in 244 milliseconds for epoch 4, of which 244 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:39,607] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-19 in 235 milliseconds for epoch 5, of which 235 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,145] INFO Loaded member MemberMetadata(memberId=sr-1-6a020778-7b37-403a-bbc3-6c4e58ef51d0, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 1. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 09:16:44,188] INFO Loaded member MemberMetadata(memberId=sr-1-e62db0b3-13b7-4d47-8090-857ecbf5befb, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 2. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 09:16:44,220] INFO Loaded member MemberMetadata(memberId=sr-1-e62db0b3-13b7-4d47-8090-857ecbf5befb, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 2. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 09:16:44,221] INFO Loaded member MemberMetadata(memberId=sr-1-e62db0b3-13b7-4d47-8090-857ecbf5befb, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 3. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 09:16:44,232] INFO Loaded member MemberMetadata(memberId=sr-1-e62db0b3-13b7-4d47-8090-857ecbf5befb, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 4. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 09:16:44,251] INFO Loaded member MemberMetadata(memberId=sr-1-fdf07d79-beb7-4769-9925-953baba62875, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 6. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 09:16:44,258] INFO Loaded member MemberMetadata(memberId=sr-1-fdf07d79-beb7-4769-9925-953baba62875, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 7. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 09:16:44,465] INFO [GroupCoordinator 4]: Loading group metadata for schema-registry with generation 7 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 09:16:44,493] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 5091 milliseconds for epoch 4, of which 213 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,507] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 5085 milliseconds for epoch 5, of which 5085 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,514] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-7 in 5089 milliseconds for epoch 5, of which 5088 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,522] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-40 in 5094 milliseconds for epoch 4, of which 5094 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,528] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-5 in 5099 milliseconds for epoch 5, of which 5097 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,529] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-38 in 5096 milliseconds for epoch 4, of which 5096 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,532] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-33 in 5092 milliseconds for epoch 4, of which 5092 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,534] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-2 in 5090 milliseconds for epoch 4, of which 5090 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,537] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,538] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,539] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,541] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,542] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,543] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,544] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,544] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,546] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,546] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,548] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,549] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,549] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,549] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,550] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:16:44,550] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 09:21:33,374] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:21:33,529] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:21:34,471] INFO [RaftManager id=4] Completed transition to Unattached(epoch=718, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=717, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5237, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:21:34,479] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:21:36,329] INFO [RaftManager id=4] Completed transition to Unattached(epoch=720, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=718, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:21:36,450] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=720, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5237, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=720, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:21:36,544] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:21:36,695] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:21:36,747] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:21:36,849] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:22:46,852] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:22:46,975] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 5578 due to node 2 being disconnected (elapsed time since creation: 2007ms, elapsed time since send: 2004ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:22:48,000] INFO [RaftManager id=4] Completed transition to Unattached(epoch=722, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=720, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5368, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:22:49,702] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:22:49,911] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:22:50,603] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=722, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5368, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=722, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:22:50,657] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:22:50,889] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:22:50,891] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:22:50,950] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:11,852] INFO [RaftManager id=4] Completed transition to Unattached(epoch=723, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=722, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5754, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:12,079] INFO [RaftManager id=4] Completed transition to Unattached(epoch=725, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=723, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:12,238] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:12,734] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=725, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5754, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=725, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:12,832] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:13,816] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 5754 (kafka.log.UnifiedLog)
[2026-01-16 09:26:13,827] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:13,845] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:13,918] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:13,983] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 5754 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 09:26:13,999] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:14,004] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 5754 (kafka.log.UnifiedLog$)
[2026-01-16 09:26:14,010] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3187, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000003187.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 09:26:14,009] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:14,061] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:14,106] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:14,112] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:14,170] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:14,190] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:14,192] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:14,244] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:14,266] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:14,277] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:14,283] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 5754 with 0 producer ids in 28 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 09:26:14,285] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 63ms for snapshot load and 211ms for segment recovery from offset 5754 (kafka.log.UnifiedLog$)
[2026-01-16 09:26:14,287] INFO [RaftManager id=4] Truncated to offset 5754 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 09:26:14,333] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:14,406] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:14,415] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:14,847] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:14,894] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:14,929] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:14,991] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:15,012] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=726, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5754, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=725, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5754, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:15,049] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:15,055] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:38,623] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:38,793] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:38,883] INFO [RaftManager id=4] Completed transition to Unattached(epoch=727, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=726, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5796, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:38,936] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:38,989] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=729, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5796, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=727, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:39,066] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:26:39,106] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 09:26:39,228] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 5796 (kafka.log.UnifiedLog)
[2026-01-16 09:26:39,363] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 5796 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 09:26:39,369] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 5796 (kafka.log.UnifiedLog$)
[2026-01-16 09:26:39,396] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=5754, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000005754.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 09:26:39,774] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 5796 with 0 producer ids in 23 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 09:26:39,775] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 160ms for snapshot load and 219ms for segment recovery from offset 5796 (kafka.log.UnifiedLog$)
[2026-01-16 09:26:39,777] INFO [RaftManager id=4] Truncated to offset 5796 from Fetch response from leader 3 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 09:26:41,851] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:26:42,087] INFO [RaftManager id=4] Completed transition to Unattached(epoch=730, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=729, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5799, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:44,382] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=732, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=5799, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=730, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:26:44,451] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:30:04,169] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:30:04,294] INFO [RaftManager id=4] Completed transition to Unattached(epoch=733, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=732, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6184, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:30:04,644] INFO [RaftManager id=4] Completed transition to Unattached(epoch=736, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=733, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:30:05,434] INFO [RaftManager id=4] Completed transition to Unattached(epoch=737, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=736, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:30:05,897] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=737, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=6184, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=737, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:30:05,923] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:37:53,656] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:37:53,894] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 7437 due to node 2 being disconnected (elapsed time since creation: 2021ms, elapsed time since send: 2020ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:37:55,076] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:37:55,111] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:37:55,191] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:37:55,219] INFO [RaftManager id=4] Completed transition to Unattached(epoch=739, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=737, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=7106, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:37:55,300] INFO [RaftManager id=4] Completed transition to Unattached(epoch=740, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=739, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:37:55,307] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:37:55,447] INFO [RaftManager id=4] Completed transition to Unattached(epoch=741, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=740, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:37:55,857] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=741, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=7106, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=741, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 09:37:55,943] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:37:55,986] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:37:55,989] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:37:56,043] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:37:56,078] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:37:56,082] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:37:56,141] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:37:56,310] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:37:56,324] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:37:56,383] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 09:53:33,288] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 09:53:33,320] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:14:25,630] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000011439-0000000741 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 10:14:26,188] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000011439-0000000741 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 10:26:36,659] INFO [RaftManager id=4] Completed transition to Unattached(epoch=742, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=741, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=12885, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:37,648] INFO [RaftManager id=4] Completed transition to Unattached(epoch=743, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=742, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:37,672] INFO [RaftManager id=4] Completed transition to Unattached(epoch=744, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=743, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:38,084] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=744, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=12885, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=744, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:38,095] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:26:38,110] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:26:38,160] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:26:38,200] INFO [RaftManager id=4] Completed transition to Unattached(epoch=745, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=744, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=12885, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:38,536] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:26:39,180] INFO [RaftManager id=4] Completed transition to Unattached(epoch=746, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=745, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:39,628] INFO [RaftManager id=4] Completed transition to Unattached(epoch=747, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=746, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:39,993] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=748, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=12885, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=747, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:26:40,083] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:26:40,124] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:26:40,171] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:26:40,221] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:36:40,048] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:36:40,372] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:44:17,605] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:44:17,724] INFO [RaftManager id=4] Completed transition to Unattached(epoch=749, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=748, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14976, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:18,161] INFO [RaftManager id=4] Completed transition to Unattached(epoch=750, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=749, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:18,410] INFO [RaftManager id=4] Completed transition to Unattached(epoch=751, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=750, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:18,503] INFO [RaftManager id=4] Completed transition to Unattached(epoch=752, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=751, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:19,420] INFO [RaftManager id=4] Completed transition to Unattached(epoch=753, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=752, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:19,589] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=753, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=14976, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=753, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 10:44:19,647] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:44:19,722] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:44:19,742] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:44:19,787] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:44:19,800] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:44:19,804] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:44:19,858] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:44:19,882] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:44:19,885] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:44:19,936] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 10:54:19,408] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 10:54:19,584] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:14:26,257] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000018569-0000000753 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 11:14:26,741] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000018569-0000000753 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 11:25:55,795] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:25:56,320] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 20846 due to node 2 being disconnected (elapsed time since creation: 2006ms, elapsed time since send: 2006ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:25:58,531] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:25:58,611] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:25:58,657] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:25:58,812] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:25:58,813] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:25:58,864] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:25:58,908] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:25:58,911] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:25:58,964] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:25:58,973] INFO [RaftManager id=4] Completed transition to Unattached(epoch=754, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=753, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=19938, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:25:59,041] INFO [RaftManager id=4] Completed transition to Unattached(epoch=755, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=754, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:25:59,044] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:25:59,129] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 11:25:59,393] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=755, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=19938, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=755, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:25:59,481] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:40,872] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:42,228] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:42,328] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 21910 due to node 1 being disconnected (elapsed time since creation: 2030ms, elapsed time since send: 2028ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:42,343] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:44,436] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:44,845] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=759, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=755, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20968, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:34:46,099] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:46,637] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 11:34:52,358] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:52,579] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 11:34:53,866] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:53,932] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 21917 due to node 1 being disconnected (elapsed time since creation: 3718ms, elapsed time since send: 3718ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:56,757] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:56,989] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:57,699] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:34:57,719] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:34:57,738] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 11:35:00,171] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:00,276] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 21920 due to node 3 being disconnected (elapsed time since creation: 5939ms, elapsed time since send: 5753ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:03,249] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:04,602] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:04,659] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 21922 due to node 2 being disconnected (elapsed time since creation: 4832ms, elapsed time since send: 2349ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:04,885] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:35:04,909] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 11:35:07,288] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:07,322] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 21927 due to node 3 being disconnected (elapsed time since creation: 2546ms, elapsed time since send: 2329ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:07,826] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:07,834] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:35:07,903] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:35:09,551] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:09,723] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:09,729] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:35:09,986] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 21928 due to node 3 being disconnected (elapsed time since creation: 2499ms, elapsed time since send: 2038ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:10,601] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:10,705] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:35:10,710] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 11:35:13,394] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:13,559] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:35:13,755] INFO [RaftManager id=4] Completed transition to Unattached(epoch=769, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=759, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20974, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:13,917] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:13,988] INFO [RaftManager id=4] Completed transition to Unattached(epoch=770, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=769, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:14,105] INFO [RaftManager id=4] Completed transition to Unattached(epoch=771, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=770, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:15,204] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=771, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20974, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=771, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:15,241] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:35:19,953] INFO [RaftManager id=4] Completed transition to Unattached(epoch=772, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=771, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20990, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:20,152] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:35:20,439] INFO [RaftManager id=4] Completed transition to Unattached(epoch=774, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=772, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:21,760] INFO [RaftManager id=4] Completed transition to Unattached(epoch=776, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=774, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:21,999] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=776, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=20990, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=776, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 11:35:22,037] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 11:45:21,057] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 11:45:21,874] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:05:30,578] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:05:31,132] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:14:26,488] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000025641-0000000776 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 12:14:26,692] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000025641-0000000776 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 12:18:10,648] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:10,649] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:16,484] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27201 due to node 1 being disconnected (elapsed time since creation: 2052ms, elapsed time since send: 2052ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:16,930] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 6677 due to node 1 being disconnected (elapsed time since creation: 4616ms, elapsed time since send: 4616ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:23,579] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:18:23,936] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:18:34,194] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:35,114] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 27205 due to node 2 being disconnected (elapsed time since creation: 2034ms, elapsed time since send: 2034ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:35,130] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:35,115] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:35,163] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 27203 due to node 3 being disconnected (elapsed time since creation: 3219ms, elapsed time since send: 3219ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:35,339] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight API_VERSIONS request with correlation id 6679 due to node 1 being disconnected (elapsed time since creation: 4781ms, elapsed time since send: 4781ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:36,041] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:18:36,200] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:18:42,965] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:18:43,933] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:44,056] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:18:44,484] INFO [RaftManager id=4] Completed transition to Unattached(epoch=780, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=776, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:44,586] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=780, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=780, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:47,574] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:47,636] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:18:48,429] INFO [RaftManager id=4] Completed transition to Unattached(epoch=781, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=780, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:48,450] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:48,483] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:18:50,497] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:50,981] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27214 due to node 3 being disconnected (elapsed time since creation: 2058ms, elapsed time since send: 2049ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:51,413] INFO [RaftManager id=4] Completed transition to Unattached(epoch=786, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=781, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:52,208] INFO [RaftManager id=4] Completed transition to Unattached(epoch=787, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=786, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:18:53,873] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:18:55,483] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:55,658] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27218 due to node 1 being disconnected (elapsed time since creation: 2947ms, elapsed time since send: 2420ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:18:58,258] INFO [RaftManager id=4] Completed transition to Unattached(epoch=789, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=787, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:00,263] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:19:00,731] INFO [RaftManager id=4] Completed transition to Unattached(epoch=792, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=789, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:01,131] INFO [RaftManager id=4] Completed transition to Unattached(epoch=793, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=792, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:02,754] INFO [RaftManager id=4] Completed transition to Unattached(epoch=794, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=793, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:04,049] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:04,143] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=794, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=794, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:06,722] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:06,758] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:06,845] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:07,006] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=796, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=794, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26070, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:07,172] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:07,224] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:07,316] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:07,419] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:07,423] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:07,474] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:08,467] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:08,487] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:19:08,495] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:19:17,136] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:17,843] INFO [RaftManager id=4] Completed transition to Unattached(epoch=798, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=796, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26084, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:21,481] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:19:22,054] INFO [RaftManager id=4] Completed transition to Unattached(epoch=800, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=798, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:25,668] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:26,998] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27241 due to node 2 being disconnected (elapsed time since creation: 2085ms, elapsed time since send: 2001ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:19:34,362] INFO [RaftManager id=4] Completed transition to Unattached(epoch=802, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=800, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:36,258] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:19:39,756] INFO [RaftManager id=4] Completed transition to Unattached(epoch=807, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=802, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:42,906] INFO [RaftManager id=4] Completed transition to Unattached(epoch=809, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=807, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:19:50,372] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:20:05,116] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:20:06,537] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:06,594] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Disconnecting from node 5 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:09,251] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27247 due to node 2 being disconnected (elapsed time since creation: 3811ms, elapsed time since send: 2386ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:09,693] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 21542 due to node 5 being disconnected (elapsed time since creation: 31375ms, elapsed time since send: 31375ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:14,542] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:15,689] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=1493408237, epoch=21528) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 12:20:21,466] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:20:17,409] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:22,237] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27250 due to node 1 being disconnected (elapsed time since creation: 18093ms, elapsed time since send: 5525ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:23,501] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:23,538] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27249 due to node 3 being disconnected (elapsed time since creation: 26635ms, elapsed time since send: 5525ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:24,109] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=1493408237, epoch=21528), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2026-01-16 12:20:37,905] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:20:38,463] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:40,005] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 27253 due to node 2 being disconnected (elapsed time since creation: 13685ms, elapsed time since send: 13685ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:41,597] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:45,188] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 27257 due to node 1 being disconnected (elapsed time since creation: 10544ms, elapsed time since send: 10544ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:46,119] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:46,239] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 27258 due to node 3 being disconnected (elapsed time since creation: 10544ms, elapsed time since send: 10544ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:47,303] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:47,395] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 27263 due to node 2 being disconnected (elapsed time since creation: 5805ms, elapsed time since send: 5805ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:53,668] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:20:54,521] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:20:55,176] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27268 due to node 3 being disconnected (elapsed time since creation: 5545ms, elapsed time since send: 4858ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:01,840] INFO [GroupCoordinator 4]: Member sr-1-fdf07d79-beb7-4769-9925-953baba62875 in group schema-registry has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:21:09,209] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:11,406] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 27270 due to node 2 being disconnected (elapsed time since creation: 2554ms, elapsed time since send: 2554ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:13,594] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:21:18,851] INFO [RaftManager id=4] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:19,604] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27275 due to node 1 being disconnected (elapsed time since creation: 15645ms, elapsed time since send: 6092ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:19,613] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:19,614] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27277 due to node 3 being disconnected (elapsed time since creation: 6248ms, elapsed time since send: 5457ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:20,819] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 7 (__consumer_offsets-29) (reason: removing member sr-1-fdf07d79-beb7-4769-9925-953baba62875 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:21:28,821] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:21:31,283] INFO [GroupCoordinator 4]: Group schema-registry with generation 8 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:21:31,533] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:31,836] INFO [RaftManager id=4] Cancelled in-flight API_VERSIONS request with correlation id 27285 due to node 3 being disconnected (elapsed time since creation: 2150ms, elapsed time since send: 2150ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:35,622] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:36,421] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27287 due to node 2 being disconnected (elapsed time since creation: 3969ms, elapsed time since send: 3327ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:42,828] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:21:46,964] INFO [RaftManager id=4] Completed transition to Unattached(epoch=831, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=809, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:21:53,139] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:54,976] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27295 due to node 2 being disconnected (elapsed time since creation: 2368ms, elapsed time since send: 2288ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:21:59,023] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:21:59,126] WARN [GroupCoordinator 4]: Failed to write empty metadata for group schema-registry: The group is rebalancing, so a rejoin is needed. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:00,810] INFO [GroupCoordinator 4]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-b5be9889-7663-4f1f-a56b-d310597f0b39 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:01,475] INFO [RaftManager id=4] Completed transition to Unattached(epoch=840, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=831, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:03,947] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:22:03,949] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=840, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26084, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=840, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:04,575] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 8 (__consumer_offsets-29) (reason: Adding new member sr-1-b5be9889-7663-4f1f-a56b-d310597f0b39 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:10,701] INFO [RaftManager id=4] Completed transition to Unattached(epoch=842, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=840, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26084, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:11,585] INFO [GroupCoordinator 4]: Stabilized group schema-registry generation 9 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:11,810] INFO [RaftManager id=4] Completed transition to Unattached(epoch=844, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=842, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:14,000] INFO [RaftManager id=4] Completed transition to Unattached(epoch=845, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=844, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:14,094] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:22:14,273] INFO [RaftManager id=4] Completed transition to Unattached(epoch=846, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=845, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:14,486] INFO [GroupCoordinator 4]: Assignment received from leader sr-1-b5be9889-7663-4f1f-a56b-d310597f0b39 for group schema-registry for generation 9. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:17,466] INFO [RaftManager id=4] Completed transition to Unattached(epoch=847, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=846, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:19,096] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:22:19,108] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=848, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26084, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=847, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:24,472] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:24,892] INFO [RaftManager id=4] Completed transition to Unattached(epoch=849, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=848, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26084, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:24,913] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:22:25,024] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=851, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26084, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=849, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:27,166] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:27,348] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:22:28,025] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:28,100] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:22:28,154] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:22:28,336] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:28,348] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:22:28,340] INFO [RaftManager id=4] Completed transition to Unattached(epoch=852, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=851, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26095, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:32,016] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 9 (__consumer_offsets-29) (reason: Error REBALANCE_IN_PROGRESS when storing group assignment during SyncGroup (member: sr-1-b5be9889-7663-4f1f-a56b-d310597f0b39)) (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:32,323] INFO [GroupCoordinator 4]: Member sr-1-b5be9889-7663-4f1f-a56b-d310597f0b39 in group schema-registry has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:32,497] INFO [RaftManager id=4] Completed transition to Unattached(epoch=854, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=852, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:33,951] INFO [RaftManager id=4] Completed transition to Unattached(epoch=856, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=854, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:34,254] INFO [GroupCoordinator 4]: Group schema-registry with generation 10 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:34,420] INFO [RaftManager id=4] Completed transition to Unattached(epoch=857, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=856, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:35,059] INFO [RaftManager id=4] Completed transition to Unattached(epoch=858, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=857, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:35,941] INFO [RaftManager id=4] Completed transition to Unattached(epoch=859, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=858, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:38,953] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:39,618] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 27325 due to node 2 being disconnected (elapsed time since creation: 2076ms, elapsed time since send: 2072ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:22:40,459] INFO [GroupCoordinator 4]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-83476cc2-3aee-487a-9bd9-862827061e61 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:40,831] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 10 (__consumer_offsets-29) (reason: Adding new member sr-1-83476cc2-3aee-487a-9bd9-862827061e61 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:42,450] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:22:43,418] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=861, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26095, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=859, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:22:43,432] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:22:44,630] INFO [GroupCoordinator 4]: Stabilized group schema-registry generation 11 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:45,770] INFO [GroupCoordinator 4]: Assignment received from leader sr-1-83476cc2-3aee-487a-9bd9-862827061e61 for group schema-registry for generation 11. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:22:54,671] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:22:59,375] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2026-01-16 12:23:02,140] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,242] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:02,260] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:02,301] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 6 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:23:02,317] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:02,330] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:02,351] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 6 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:23:02,356] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:02,357] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 6 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:23:02,365] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:02,371] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:02,378] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,390] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,405] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,500] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:02,506] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:02,596] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,654] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:02,662] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:02,668] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,671] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,672] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 5 from offset 12 with partition epoch 15 and high watermark 12. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:02,680] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,692] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:02,693] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:02,694] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,698] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:02,711] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:02,716] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:02,724] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,730] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,743] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 6 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:23:02,755] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 6 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:23:02,755] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:02,768] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:02,857] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 6 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:23:02,894] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:02,985] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:03,101] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:03,126] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:03,132] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 6 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:23:03,151] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:03,168] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:03,170] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:03,204] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:03,211] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:03,237] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:03,241] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:03,354] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=11, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:23:03,449] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:03,451] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 5 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:23:05,098] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-46, __consumer_offsets-42, __consumer_offsets-10, __consumer_offsets-23, __consumer_offsets-24, __consumer_offsets-19, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 12:23:05,172] INFO [Broker id=4] Stopped fetchers as part of become-follower for 17 partitions (state.change.logger)
[2026-01-16 12:23:07,177] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-30 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-14 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-42 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-23 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-7 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-5 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-19 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 12:23:07,267] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-48 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-29 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,12), __consumer_offsets-46 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-10 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-40 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-24 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-38 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-33 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-2 -> InitialFetchState(Some(p0HtGUHKReOSVWe-VAlPVA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0)) (kafka.server.ReplicaFetcherManager)
[2026-01-16 12:23:07,278] INFO [Broker id=4] Started fetchers as part of become-follower for 17 partitions (state.change.logger)
[2026-01-16 12:23:12,672] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:13,414] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,604] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:13,631] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,674] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:13,708] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,709] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:13,710] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,722] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:13,723] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,724] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:13,724] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,724] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:13,725] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,726] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:13,727] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,728] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:13,736] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,890] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,922] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,929] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,939] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,944] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,953] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:13,954] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,382] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,429] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,291] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:14,501] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,576] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:14,577] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,583] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,625] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:14,634] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,644] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:14,626] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,644] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,644] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,644] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:14,647] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,647] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:14,647] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,647] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:14,645] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,657] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,657] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:14,657] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:15,996] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:15,997] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:15,997] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:15,998] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:15,999] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,000] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,000] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,000] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,000] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,000] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,000] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,000] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,000] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,004] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,157] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,217] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,218] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,219] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,220] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,220] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,221] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,221] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,222] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,222] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,224] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,225] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,226] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,229] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,231] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,022] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,235] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,244] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,245] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,233] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,248] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,250] INFO [GroupCoordinator 4]: Unloading group metadata for schema-registry with generation 11 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,250] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,253] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,254] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,276] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,288] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,296] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,303] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,367] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,272] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,369] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,371] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,369] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,374] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,375] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,375] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,376] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,374] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,381] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,382] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,380] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,383] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,406] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,383] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,409] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,410] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,414] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,417] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,409] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,432] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,434] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,437] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,440] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,434] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,450] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,461] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,462] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,473] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,460] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,476] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,477] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,483] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,498] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,477] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,506] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,507] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,522] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,657] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,659] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,711] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,713] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,718] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,673] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,749] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,756] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,792] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,793] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,833] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,790] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,856] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,920] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,843] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:16,951] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:17,079] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:17,081] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:16,991] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:17,085] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:17,082] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:17,094] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:48,938] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:23:49,720] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 12:23:49,917] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:50,217] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:51,014] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:51,165] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:53,471] INFO [Broker id=4] Transitioning 22 partition(s) to local followers. (state.change.logger)
[2026-01-16 12:23:53,654] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:53,658] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:53,700] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:53,823] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:53,861] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:53,878] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:53,998] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:54,010] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:54,022] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:54,475] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:54,544] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:54,560] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:54,565] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:54,576] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:54,598] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:54,612] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:54,636] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:54,650] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:54,664] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:54,665] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:54,688] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2026-01-16 12:23:54,817] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2026-01-16 12:23:55,149] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,215] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,216] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,221] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,232] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,234] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,235] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,227] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,271] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,271] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,265] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,272] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,272] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,272] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,272] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,272] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,272] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,272] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,272] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,272] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,275] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,277] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,272] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,278] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,768] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,770] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,800] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,853] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,854] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,859] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,865] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,865] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,767] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,870] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,874] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,878] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,878] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,871] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,878] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,879] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,879] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,880] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,880] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,914] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,915] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,934] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:55,935] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,964] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,007] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:55,879] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,074] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,034] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,078] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:56,076] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,093] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,113] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,120] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,125] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,084] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,140] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:56,140] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,151] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,152] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:23:56,156] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,153] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:23:56,165] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:03,055] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2026-01-16 12:24:03,173] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:03,363] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:03,400] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:03,407] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:04,779] INFO [Broker id=4] Transitioning 27 partition(s) to local followers. (state.change.logger)
[2026-01-16 12:24:04,938] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:05,430] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:05,451] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,480] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:05,511] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,512] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,512] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,512] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:05,512] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:05,513] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,513] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,514] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(UjN16IrSRlipZQeCwkGrdQ) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,520] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,522] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,530] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:05,553] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:05,556] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,617] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,632] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,642] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,681] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=16, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:05,731] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,766] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,807] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,813] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2026-01-16 12:24:05,837] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:05,839] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2026-01-16 12:24:05,861] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,865] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,876] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,892] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,893] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,894] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,894] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,894] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,900] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,903] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,904] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,905] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,906] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,907] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,908] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,909] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,909] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,913] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,914] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,914] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,917] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,919] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,919] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,920] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,920] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,921] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,922] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,922] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,923] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,925] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,925] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,926] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,927] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,928] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,929] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,903] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,930] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,932] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,932] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,932] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,933] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,933] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,934] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,934] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,935] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,936] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,936] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,941] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,939] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,944] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,948] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,948] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,951] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,950] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,952] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,954] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,952] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:05,958] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,982] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:05,998] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,007] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:06,008] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,011] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,010] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,014] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:06,012] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,020] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,019] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,028] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:24:06,028] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,028] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,052] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,106] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,129] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,217] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,221] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:06,265] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:24:45,454] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:24:45,853] INFO [RaftManager id=4] Completed transition to Unattached(epoch=862, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=861, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26409, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:46,272] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:24:46,516] INFO [RaftManager id=4] Completed transition to Unattached(epoch=867, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=862, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:46,776] INFO [RaftManager id=4] Completed transition to Unattached(epoch=868, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=867, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:46,937] INFO [RaftManager id=4] Completed transition to Unattached(epoch=869, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=868, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:47,510] INFO [RaftManager id=4] Completed transition to Unattached(epoch=870, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=869, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:48,141] INFO [RaftManager id=4] Completed transition to Unattached(epoch=871, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=870, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:51,083] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:24:54,522] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=875, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26409, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=871, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:54,576] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:24:57,967] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:24:58,043] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:24:58,146] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:24:58,155] INFO [RaftManager id=4] Completed transition to Unattached(epoch=876, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=875, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26417, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:58,496] INFO [RaftManager id=4] Completed transition to Unattached(epoch=877, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=876, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:58,566] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:24:58,779] INFO [RaftManager id=4] Completed transition to Unattached(epoch=879, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=877, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:59,589] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=879, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26417, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=879, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:24:59,602] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:25:00,347] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:25:00,363] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:25:00,424] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:30:02,786] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2026-01-16 12:30:03,916] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-15 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,140] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-15 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,143] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-48 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,163] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-48 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,170] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-46 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,175] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-46 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,177] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-10 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,189] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-10 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,192] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-40 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,205] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-40 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,223] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-24 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,228] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-24 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,233] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-38 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,236] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-38 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,253] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-29 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,273] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-29 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,276] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-33 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,280] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-33 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,290] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-2 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,300] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-2 marked as failed (kafka.server.ReplicaFetcherThread)
[2026-01-16 12:30:04,450] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-46, __consumer_offsets-14, __consumer_offsets-42, __consumer_offsets-10, __consumer_offsets-23, __consumer_offsets-24, __consumer_offsets-19, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2026-01-16 12:30:06,217] INFO [Broker id=4] Leader __consumer_offsets-15 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:30:06,334] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:30:06,362] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:30:06,392] INFO [Broker id=4] Leader __consumer_offsets-14 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:06,443] INFO [Broker id=4] Leader __consumer_offsets-42 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:06,471] INFO [Broker id=4] Leader __consumer_offsets-10 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:30:06,511] INFO [Broker id=4] Leader __consumer_offsets-23 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:06,665] INFO [Broker id=4] Leader __consumer_offsets-24 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:30:06,760] INFO [Broker id=4] Leader __consumer_offsets-19 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:06,910] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 12 with partition epoch 17, high watermark 12, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:30:07,001] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:07,062] INFO [Broker id=4] Leader __consumer_offsets-7 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:07,151] INFO [Broker id=4] Leader __consumer_offsets-40 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:30:07,226] INFO [Broker id=4] Leader __consumer_offsets-5 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2026-01-16 12:30:07,324] INFO [Broker id=4] Leader __consumer_offsets-38 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:30:07,435] INFO [Broker id=4] Leader __consumer_offsets-33 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:30:07,556] INFO [Broker id=4] Leader __consumer_offsets-2 with topic id Some(p0HtGUHKReOSVWe-VAlPVA) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2026-01-16 12:30:07,647] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 15 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,711] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,857] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,884] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,887] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,899] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,911] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 14 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,911] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,912] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 42 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,931] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,932] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 10 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,937] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:07,940] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 23 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:07,970] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,001] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 24 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:08,004] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,011] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 19 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:08,065] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,110] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:08,114] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,117] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:08,118] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,121] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 7 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:08,124] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,127] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 40 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:08,130] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,134] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 5 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:08,135] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,139] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 38 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:08,156] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,241] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 33 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:08,246] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,247] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 2 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:08,249] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,566] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-15 in 849 milliseconds for epoch 6, of which 593 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,661] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 775 milliseconds for epoch 6, of which 734 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,678] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 768 milliseconds for epoch 6, of which 756 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,714] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-14 in 801 milliseconds for epoch 7, of which 784 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,763] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-42 in 831 milliseconds for epoch 7, of which 831 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,772] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-10 in 832 milliseconds for epoch 6, of which 831 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,805] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-23 in 804 milliseconds for epoch 7, of which 773 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,808] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-24 in 798 milliseconds for epoch 6, of which 798 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:08,808] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-19 in 699 milliseconds for epoch 7, of which 699 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:11,061] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:30:11,547] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:30:12,525] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:30:12,529] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:30:12,588] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:30:12,665] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:30:12,666] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:30:12,671] INFO Loaded member MemberMetadata(memberId=sr-1-6a020778-7b37-403a-bbc3-6c4e58ef51d0, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 1. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 12:30:12,674] INFO Loaded member MemberMetadata(memberId=sr-1-e62db0b3-13b7-4d47-8090-857ecbf5befb, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 2. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 12:30:12,675] INFO Loaded member MemberMetadata(memberId=sr-1-e62db0b3-13b7-4d47-8090-857ecbf5befb, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 2. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 12:30:12,679] INFO Loaded member MemberMetadata(memberId=sr-1-e62db0b3-13b7-4d47-8090-857ecbf5befb, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 3. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 12:30:12,683] INFO Loaded member MemberMetadata(memberId=sr-1-e62db0b3-13b7-4d47-8090-857ecbf5befb, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 4. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 12:30:12,689] INFO Loaded member MemberMetadata(memberId=sr-1-fdf07d79-beb7-4769-9925-953baba62875, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 6. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 12:30:12,709] INFO Loaded member MemberMetadata(memberId=sr-1-fdf07d79-beb7-4769-9925-953baba62875, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 7. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 12:30:12,711] INFO Loaded member MemberMetadata(memberId=sr-1-b5be9889-7663-4f1f-a56b-d310597f0b39, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 9. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 12:30:12,713] INFO Loaded member MemberMetadata(memberId=sr-1-83476cc2-3aee-487a-9bd9-862827061e61, groupInstanceId=None, clientId=sr-1, clientHost=/172.24.0.19, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 11. (kafka.coordinator.group.GroupMetadata$)
[2026-01-16 12:30:12,724] INFO [RaftManager id=4] Completed transition to Unattached(epoch=880, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=879, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:30:12,778] INFO [RaftManager id=4] Completed transition to Unattached(epoch=884, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=880, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:30:12,827] INFO [GroupCoordinator 4]: Loading group metadata for schema-registry with generation 11 (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 12:30:12,860] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 4738 milliseconds for epoch 6, of which 691 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:12,897] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 4776 milliseconds for epoch 7, of which 4764 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:12,899] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-7 in 4772 milliseconds for epoch 7, of which 4771 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:12,980] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-40 in 4846 milliseconds for epoch 6, of which 4765 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:12,982] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-5 in 4842 milliseconds for epoch 7, of which 4842 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:12,982] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-38 in 4741 milliseconds for epoch 6, of which 4741 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:12,986] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-33 in 4739 milliseconds for epoch 6, of which 4737 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:13,000] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-2 in 4746 milliseconds for epoch 6, of which 4746 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2026-01-16 12:30:13,051] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:30:15,147] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:30:15,166] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=884, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27017, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=884, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:41,375] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:41,752] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:41,782] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:42,067] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 28774 due to node 3 being disconnected (elapsed time since creation: 2007ms, elapsed time since send: 2007ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:43,444] INFO [RaftManager id=4] Completed transition to Unattached(epoch=886, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=884, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27522, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:43,717] INFO [RaftManager id=4] Completed transition to Unattached(epoch=887, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=886, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:44,004] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:44,108] INFO [RaftManager id=4] Completed transition to Unattached(epoch=888, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=887, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:45,968] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:34:46,290] INFO [RaftManager id=4] Completed transition to Unattached(epoch=889, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=888, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:47,891] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=889, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27522, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=889, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:47,916] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:50,611] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:50,637] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:50,754] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:34:51,187] INFO [RaftManager id=4] Completed transition to Unattached(epoch=894, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=889, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27522, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:51,254] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:51,646] INFO [RaftManager id=4] Completed transition to Unattached(epoch=895, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=894, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:52,009] INFO [RaftManager id=4] Completed transition to Unattached(epoch=896, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=895, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:53,783] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=896, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27522, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=896, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:34:53,868] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:54,000] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 27522 (kafka.log.UnifiedLog)
[2026-01-16 12:34:54,518] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 27522 with message format version 2 (kafka.log.UnifiedLog$)
[2026-01-16 12:34:54,596] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 27522 (kafka.log.UnifiedLog$)
[2026-01-16 12:34:54,600] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=5796, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000005796.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 12:34:55,837] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:55,975] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:34:55,984] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:34:56,622] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:34:58,571] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:00,189] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:00,321] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:00,867] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:00,916] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:01,030] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:35:01,934] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 27522 with 0 producer ids in 220 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2026-01-16 12:35:02,085] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 413ms for snapshot load and 7073ms for segment recovery from offset 27522 (kafka.log.UnifiedLog$)
[2026-01-16 12:35:02,192] INFO [RaftManager id=4] Truncated to offset 27522 from Fetch response from leader 1 (org.apache.kafka.raft.KafkaRaftClient)
[2026-01-16 12:35:02,966] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:03,088] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:03,438] INFO [RaftManager id=4] Completed transition to Unattached(epoch=898, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=896, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27522, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:35:04,420] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:04,762] INFO [RaftManager id=4] Completed transition to Unattached(epoch=899, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=898, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:35:07,015] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:07,068] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 12:35:07,198] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 28799 due to node 3 being disconnected (elapsed time since creation: 2014ms, elapsed time since send: 2014ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:07,369] INFO [RaftManager id=4] Completed transition to Unattached(epoch=900, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=899, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:35:07,966] INFO [RaftManager id=4] Completed transition to Unattached(epoch=901, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=900, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:35:08,461] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=901, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=27522, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=901, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 12:35:08,473] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,096] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:10,254] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,326] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:10,328] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,450] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,536] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:10,539] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,590] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,637] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:10,648] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,706] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,772] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 12:35:10,783] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:35:10,861] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 12:45:07,633] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:14:26,886] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000032229-0000000901 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 13:14:27,044] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000032229-0000000901 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2026-01-16 13:28:50,269] INFO [GroupCoordinator 4]: Member sr-1-83476cc2-3aee-487a-9bd9-862827061e61 in group schema-registry has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 13:29:47,875] INFO [RaftManager id=4] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:29:47,922] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:29:51,529] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 8667 due to node 2 being disconnected (elapsed time since creation: 4800ms, elapsed time since send: 4800ms, throttle time: 0ms, request timeout: 4500ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:29:51,821] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 11 (__consumer_offsets-29) (reason: removing member sr-1-83476cc2-3aee-487a-9bd9-862827061e61 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 13:29:51,557] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 35341 due to node 2 being disconnected (elapsed time since creation: 27447ms, elapsed time since send: 27447ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:29:54,830] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:29:56,078] INFO [BrokerLifecycleManager id=4] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2026-01-16 13:29:56,799] INFO [GroupCoordinator 4]: Group schema-registry with generation 12 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 13:30:00,541] WARN [BrokerLifecycleManager id=4] Broker 4 sent a heartbeat request but received error REQUEST_TIMED_OUT. (kafka.server.BrokerLifecycleManager)
[2026-01-16 13:30:00,814] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:00,864] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:30:00,926] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:30:01,003] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:01,021] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:30:01,079] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:30:01,170] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:01,171] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:30:01,239] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:30:01,427] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:01,593] INFO [RaftManager id=4] Completed transition to Unattached(epoch=902, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=901, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33899, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 13:30:01,857] INFO [GroupCoordinator 4]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-ea4315e9-dffc-46d8-806f-fb2a689afd5e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 13:30:02,174] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 12 (__consumer_offsets-29) (reason: Adding new member sr-1-ea4315e9-dffc-46d8-806f-fb2a689afd5e with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 13:30:03,638] INFO [RaftManager id=4] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:03,643] INFO [RaftManager id=4] Cancelled in-flight FETCH request with correlation id 35350 due to node 3 being disconnected (elapsed time since creation: 2009ms, elapsed time since send: 2003ms, throttle time: 0ms, request timeout: 2000ms) (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:03,774] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=904, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33899, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=902, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 13:30:03,845] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:30:04,224] INFO [RaftManager id=4] Completed transition to Unattached(epoch=905, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=904, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33904, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2026-01-16 13:30:04,469] INFO [RaftManager id=4] Completed transition to Unattached(epoch=906, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=905, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 13:30:05,430] INFO [RaftManager id=4] Completed transition to Unattached(epoch=907, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=906, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 13:30:05,465] INFO [GroupCoordinator 4]: Stabilized group schema-registry generation 13 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 13:30:05,613] INFO [RaftManager id=4] Completed transition to Unattached(epoch=908, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=907, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 13:30:05,955] INFO [GroupCoordinator 4]: Assignment received from leader sr-1-ea4315e9-dffc-46d8-806f-fb2a689afd5e for group schema-registry for generation 13. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2026-01-16 13:30:05,970] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:06,206] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=908, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=33904, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=908, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2026-01-16 13:30:06,275] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:30:06,427] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:30:06,435] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:30:06,487] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2026-01-16 13:40:05,921] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 13:40:06,065] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2026-01-16 14:15:19,597] INFO [SnapshotGenerator id=4] Creating new KRaft snapshot file snapshot 00000000000000039302-0000000908 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2026-01-16 14:15:19,708] INFO [SnapshotEmitter id=4] Successfully wrote snapshot 00000000000000039302-0000000908 (org.apache.kafka.image.publisher.SnapshotEmitter)
